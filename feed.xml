<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://oweixx.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://oweixx.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-06T09:52:02+00:00</updated><id>https://oweixx.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Quaternion</title><link href="https://oweixx.github.io/blog/2025/Quaternion/" rel="alternate" type="text/html" title="Quaternion"/><published>2025-11-06T00:00:00+00:00</published><updated>2025-11-06T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/Quaternion</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/Quaternion/"><![CDATA[<h2 id="viton-an-image-based-virtual-try-on-network-ieee-2018">VITON: An Image-based Virtual Try-on Network (IEEE 2018)</h2> <h3 id="paper-github-demo"><a href="https://arxiv.org/abs/1711.08447">[Paper]</a> <a href="https://github.com/xthan/VITON">[Github]</a> <a href="https://huggingface.co/spaces/Kwai-Kolors/Kolors-Virtual-Try-On">[Demo]</a></h3> <blockquote> <p><strong>Title:</strong> VITON: An Image-based Virtual Try-on Network<br/> <strong>Journal name &amp; Publication year:</strong> Not specified, Arxiv preprint in 2018 <br/> <strong>First and Last Authors:</strong> Xintong Han, Larry S. Davis <br/> <strong>First Affiliations:</strong> University of Maryland, College Park</p> </blockquote> <p>평소 관심있었던 분야이기도 했고 현업에서 상업적으로 다양하게 이미 사용되고 있다는 것도 느꼈었기 때문에 Virtual Try-on이라는 분야에 관심을 가지게 되었다. 해당 논문은 Virtual Try-on 분야의 초기 논문이다.</p> <hr/> <blockquote> <h2 id="abstract--introduction--related-work">Abstract &amp; Introduction &amp; Related Work</h2> </blockquote> <p>지속적으로 수요가 급증하고 있는 온라인 쇼핑 산업에서 점점 더욱 더 간편하게 쉽게 소비를 할 수 있는 환경으로 발전하고 있는 요즘이다. 동시에 소비자들은 의류의 기재된 사이즈나 정보보다 실제 나에게 fit한 느낌을 알고 싶어하고 이것은 결국 불안한 소비로 이어지게 된다. 이를 해결하기 위해 제시하는 VITON은 아이템을 구매하기 전 아이템을 가상으로 착용해보며 <strong>소비자는 가상의 쇼핑 경험</strong>을 하고, <strong>소매업체는 서비스 비용 절감</strong>을 할 수 있게 된다. <img src="https://velog.velcdn.com/images/lowzxx/post/df3a37fd-99b1-48f2-b946-3111e6924505/image.png" alt=""/></p> <p>해당 논문에서는 현실적인 제약이 많고 cost가 높은 3D 정보를 전혀 사용하지 않고 2D RGB 이미지 기반에 의존하는 VITON 모델을 제시한다. VITON 모델은 의류 제품 이미지를 사람의 해당 부위에 자연스럽게 합성되어 Photorealistic한 이미지를 생성하는데 목표를 두고 있다. 해당 목표를 위해서 생성된 이미지는 다음의 조건을 충족해야 한다고 한다.</p> <ul> <li><strong>1. 사람의 신체 부위와 자세가 원본 이미지와 동일해야 한다.</strong></li> <li><strong>2. 목표 의류 아이템은 사람의 자세와 신체 형태에 따라 자연스럽게 변형되어야 한다.</strong></li> <li><strong>3. 원하는 제품의 세부적인 시각적 패턴(색감, 질감, 디테일 등등)이 명확히 드러나야 한다.</strong></li> </ul> <p>위의 조건을 충족하기 위해서는 3D 정보를 활용 하더라도 힘들 것 같은데 2D 이미지로만 구현한다는 부분에서 큰 도전과제인 것 같다.</p> <p>기존의 Virtual Try-on에 주로 사용되는 GAN에서는 목표 의류 아이템의 시각적 디테일을 모두 표현하지 못하고 기하학적 변화에 대한 결과가 좋지 않았는데 이러한 한계점을 해결하기 위해 VITON은 <strong>clothing-agnostic representation</strong>등 다양한 알고리즘들을 사용한다.</p> <hr/> <blockquote> <h2 id="viton">VITON</h2> </blockquote> <p>아래의 Model Architecture를 보았을 때 VITON은 큰 구조로는 두 개의 Network를 가지게 된다. 두 Network는 위에서 제시한 모델의 조건인 1,2번을 목표로 하는 <strong>Multi-task Encoder-decoder Generator</strong>가 있고 이를 통해 1,3번을 해결하기 위한 <strong>Refinement Network</strong>로 나뉘게 된다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/0c19e0f1-6ac0-4500-9df4-205b7fe2155d/image.png" alt=""/></p> <p><br/></p> <h3 id="1-person-representation"><strong>1. Person Representation</strong></h3> <p>VITON의 가장 큰 challenge는 1번과 2번을 충족하는 부분이다. 3D 데이터일 경우 (x,y,z)의 point값과 같은 디테일한 정보를 통해 더욱 realistic한 표현이 가능하겠지만 2D Data만으로는 어려운 표현의 한계가 있기 때문이다. 이를 해결하기 위해 Peron Representation이라는 개념을 제시한다. <img src="https://velog.velcdn.com/images/lowzxx/post/b0f815b0-a44d-47c1-9a33-5790622599e7/image.png" alt=""/></p> <ul> <li><strong>Pose heatmap.</strong> <strong><a href="https://arxiv.org/abs/1611.08050">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields (CVPR 2017)</a></strong> 먼저 제시된 의류를 인물 I에 맞추기 위해 <strong>pose estimation</strong>이 필요한데 이는 위의 논문에서 제시하는 model을 사용한다고 한다. 해당 모델을 사용하면 <strong>인물의 pose를 설명할 수 있는 주요 18개의 key-point가 11x11의 heatmap</strong>으로 제공되어 결론적으로 (18,11,11)의 정보값을 반환하게 된다.</li> </ul> <p>18개의 정확한 위치 정보뿐만이 아니라 주변의 공간적 관계를 나타내는 11x11 heatmap을 통해 2D Image지만으로 표현하는데 한계였었던 공간적인 정보를 제공한다.</p> <ul> <li> <p><strong>Human body representation.</strong> <strong><a href="https://arxiv.org/abs/1703.05446">Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing (CVPR 2017)</a></strong> 위에서 pose를 계산했다면 해당 pose에 의류가 fit하게 맞아야 한다. 이를 위해서 인물의 몸이 있는 위치 정보를 얻어내기 위해 위 논문에서 제시하는 <strong>human parser model</strong>을 사용한다. 이를 통해 1-channel로 구성된 binary mask를 얻고 의류가 충돌하는 등의 artifacts를 피하기 위해 (16x12)로 의도적으로 downsampling을 한다고 한다.</p> </li> <li> <p><strong>Face and hair segment.</strong> <strong><a href="https://arxiv.org/abs/1703.05446">Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing (CVPR 2017)</a></strong> 마지막으로 인물의 식별을 위해서 해당 인물이라고 표현할 수 있는 human parser를 위해 위에서 사용했던 모델을 사용하여 인물의 Face and hair RGB이미지를 구한다.</p> <p>왜 해당 인물이라고 표현하는 정보를 필요로 할까? 조금 찾아보니 이전의 모델들에서는 합성된 이미지에서 인물의 옷만 바뀌어야 하는데 인물의 얼굴, 머리카락 등 인물의 특징들이 변하는 문제가 있었다고 한다. 아마 이를 방지하기 위해 따로 구해놓는게 아닐까 싶다.</p> </li> </ul> <p>위의 세개의 정보를 결합하여 Person Representation을 의미하는 $p$를 만든다. $p \in \mathbb{R}^{m \times n \times k}$ 를 따르고 featuremap의 높이와 너비를 나타내는 m = 256, n = 192, 위에서 구한 정보들을 channel로 두어 18 + 1 + 3 = 22로 $p$는 $(256,192, 22)$의 shape을 가진다.</p> <p><strong>해당 $p$는 인물에 대한 풍부한 정보를 담고있어 보다 정교한 작업이 가능해진다.</strong></p> <p><br/></p> <h3 id="2-multi-task-encoder-decoder-generator"><strong>2. Multi-task Encoder-Decoder Generator</strong></h3> <p><img src="https://velog.velcdn.com/images/lowzxx/post/a25ea934-3870-402a-9b49-eb43f97b7bf3/image.png" alt=""/> U-Net 구조로 이루어진 Encoder-Decoder Generator Network에서는 입력으로 주어지는 인물 정보 p와 의류 c를 통해 <strong>c가 p의 영역으로 자연스럽게 합성되는 것을 목표</strong>로 한다. 또한 뒤의 <strong>Refinement Network에서 사용될 clothing mask</strong>도 같이 추출하게 된다.</p> <p>해당 model의 출력을 근사된 함수로 표현하면 4-channel로 구성된 $G_c(c,p) = (I’,M)$으로 표현되고 앞에 3개의 channel에는 합성된 이미지 $I’$을 의미하고 마지막 channel에는 clothing mask인 $M$을 의미한다. 인물이 보았을 때 이질감이 없이 realistic한 결과물을 위해서는 L1 loss가 아닌 인간의 실제 관측한 값과 가깝게 학습하기 위해 <strong>perceptual loss</strong>를 이용한다. ($M_0$는 human parser에서 예측한 psuedo ground truth clothing mask) 더 자세하게는 coarse image와 Ground truth의 feature map에서의 차이를 줄이기 위해 사용한다.</p> \[L_{G_c} = \sum_{i=0}^{5} \lambda_i \left\| \phi_i(I') - \phi_i(I) \right\|_1 + \left\| M - M_0 \right\|_1,\] <p>수식에서 $\phi_i$는 VGG19 Network에 ImageNet이 사전학습된 모델로 순서대로 ‘conv1_2’, ‘conv2_2’, ‘conv3_2’, ‘conv4_2’, ‘conv5_2’로 각 layer에 feature map을 이용하여 feature map에서의 차이를 줄이는 방향으로 학습이 된다. 다만 i = 0일 경우 RGB 픽셀값의 차이인 L1 loss를 사용한다고 한다.</p> <p>해당 perceptual loss를 최소화하는 학습을 반복함으로써 <strong>의상을 인물에 합성하게 되고 의류의 mask도 얻어내게 된다.</strong> 하지만 조건의 3번인 의류의 디테일을 표현하지 못하는 한계를 가진다.</p> <p><br/></p> <h3 id="3-refinement-network"><strong>3. Refinement Network</strong></h3> <p>Refinement Network는 mask에 Wraping된 의류 $c’$와 $I’$을 이용하여 Encoder-Decoder Network에서 표현하지못한 의상의 디테일 부분을 개선하여 생성하려고 한다.</p> <p><strong>Wrapped clothing item.</strong><br/> <img src="https://velog.velcdn.com/images/lowzxx/post/bb8e4abc-acba-40f6-8d06-bde3497ac30a/image.png" alt=""/> 기존의 의류 c를 Network에 그대로 사용하지 못하기 때문에 기존의 의류 c와 clothing Mask M을 이용하여 Thin Plate Spline(TPS) Transformation을 통해 의류 이미지(c)는 디테일을 유지한채로 인물의 포즈 및 체형에 맞는 $c’$으로 변환된다.</p> <p><strong>Learn to composite.</strong><br/> <img src="https://velog.velcdn.com/images/lowzxx/post/b6ec47e4-c348-4df0-9704-eb439880b13b/image.png" alt=""/></p> <p>위에서 생성한 $I’$과 $c’$을 Refinement Network $G_R$의 입력으로 넣는다. 여기서 $G_R$은 $c’$의 디테일을 활용하여 디테일이 없거나 배경처럼 덜 중요한 영역은 0(검정색), 디테일이 있는 중요한 영역은 1(흰색)로 채워진 <strong>binary composition mask $\alpha$</strong> 를 생성한다. 놀랍게도 지금까지 쓰이지 않았던 <strong>Face and hair segment.</strong> 정보가 여기서 사용된다. 의류부분과 얼굴,머리카락 부분이 겹치는 경우가 많기 때문에 이를 처리하기 위해 이때 자연스럽게 사용되어 의류 부분만 뽑을 수 있게 된다.</p> <p>그다음 $\alpha$와 $I’$을 활용하여 합성한 $\hat I$를 만들어준다. 디테일이 있는 부분은 $c’$을 따라가고 디테일이 없는 부분은 $I’$을 따라가게 만든다.</p> \[\hat I = \alpha \odot c' + (1 - \alpha) \odot I',\] <p>다음으로 원본 이미지 $I$와 synthetic image $\hat I$가 최대한 비슷한 것을 목표로 하는 perceptual loss가 한 번 더 적용이 된다.</p> \[L_{\text{perc}}(\hat{I}, I) = \sum_{i=3}^{5} \lambda_i \left\| \phi_i(\hat{I}) - \phi_i(I) \right\|_1,\] <p>그럼 마지막으로 Refinement Network가 최종 합성 이미지를 더 사실적으로 보이도록 $G_R$을 최적화 해야한다. 여기서 <strong>$L_{G_R}$은 Refinement Network의 총 손실 함수</strong>이다. 두번째 항과 세번째 항은 둘 다 규제 항으로 두번째 항은 $L_1$ 규제항으로 마스크 $\alpha$가 높은 정확도로 구분하는 항이고, 세번째 항은 TV(Total Variation) 규제항으로 이미지의 불연속성을 최소화하며 부드러운 이미지를 위해 추가하는 규제항이라고 한다.</p> <p>결론적으로 음의 <strong>$L_1$ 항을 최소화하면 의류 이미지의 디테일 정보를 더 많이 렌더링</strong> 할 수 있고, <strong>TV 규제항을 최소화하면 더 자연스러운 이미지를 생성</strong>할 수 있는 것이다.</p> \[L_{G_R} = L_{\text{perc}}(\hat{I}, I) - \lambda_{\text{warp}} \left\| \alpha \right\|_1 + \lambda_{\text{TV}} \left\| \nabla \alpha \right\|_1\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/77083bea-92f7-41e1-9290-d1caf4d47107/image.png" alt=""/></p> <p>이로써 VITON모델이 완성이 되었고 model에서 step이 진행되면서 얻어지는 결과물들을 통해 점점 자연스럽게 합성되어가는 과정을 확인할 수 있다.</p> <p>마지막으로 정리를 해보자면 <strong>Encoder-Decoder</strong> 부분에서 의류와 인물에 대한 정보가 주어지면 디테일들을 제외한 인물의 포즈, 체형, 얼굴이 보존된채로 의류가 합성된 결과물을 생성한다.</p> <p>이후 <strong>Refinement</strong> 부분에서 의류를 wraping하고 의류의 디테일 위치의 정보를 담은 Composition Mask가 생성되어 이를 통해 디테일한 부분까지 최종 합성된 이미지가 생성되게 된다.</p> <hr/> <blockquote> <h2 id="experiments">Experiments</h2> </blockquote> <h3 id="1-dataset">1. Dataset</h3> <p>데이터셋은 크롤링을 통해 16253 쌍의 정면을 바라고있는 여성 이미지와 상의 사진을 이용했다고 한다. 이중 87 ~ 88% 정도를 train, 나머지를 test data로 사용하는데 train 데이터는 인물과 의상이 pair하지만 test data에는 검증을 위해 randomly shuffle 되었다고 한다.</p> <p><br/></p> <h3 id="2-implementation-details">2. Implementation Details</h3> <p><strong>Training setup.</strong></p> <ul> <li>Adam optimizer: $\beta_1 = 0.5$, $\beta_2 = 0.999$, $lr = 0.0002$</li> <li>Encoder-decoder: 15K, Refinement: 6K, batchsize = 16</li> <li>synthetic samples size = $256 X 192$</li> </ul> <p><strong>Encoder-decoder generator.</strong></p> <ul> <li>6 convolutional layer</li> <li>Encoding layer consist 4x4 filter, stride of 2, number of filters 64, 128, 256, 512, 512, 512</li> <li>Decoding layer consist 4x4 filter, number of filters 512, 512, 256, 128, 64, 4</li> </ul> <p><strong>Refinement Network</strong></p> <ul> <li>4 fully convolutional model.</li> <li>첫 3개 layer는 3 x 3 x 64 filters and Leaky ReLU</li> <li>마지막 layer는 composition mask를 위해 1 x 1 filter와 sigmoid</li> </ul> <p><br/></p> <h3 id="3-compared-approaches">3. Compared Approaches</h3> <p><br/></p> <h3 id="4-qualitative-results">4. Qualitative Results</h3> <p><strong>Qualitative comparisons of different methods.</strong><br/> 비슷하게 사용될 수 있는 모델들과의 결과물 비교를 해보게 되면 대부분의 모델들이 detail들을 그대로 보존하지 못하고 detail을 보존했을 경우 인물의 pose에 못 따라오는 경우가 대다수이다.</p> <p>하지만 VITON 모델에서 상의 의상만 바뀌어야 하는데 바지도 같이 바뀌는 경우가 생겨버린다. 이 부분은 Face and hair segment. 때와 같이 바지도 따로 추출해준다면 충분히 보존할 수 있다고 한다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/30d588fe-fc40-4573-bb18-3a3246dfd936/image.png" alt=""/></p> <p><strong>Person Representation.</strong><br/> 다음으로 논문에서 중요한 아이디어로 제시된 person representation의 효과를 살펴보자면 pose 정보만으로 봤을 때는 확실히 인물의 pose는 잘 보존된 채로 결과물이 나오는 것을 확인할 수 있다. 다만 body shape mask image 때문에 의상과 인물이 겹쳐있는 경우 충돌로인한 artifact noise가 생기는 걸 확인할 수 있다.<br/> <img src="https://velog.velcdn.com/images/lowzxx/post/28b075e1-6a6f-4366-9fc9-11eccc4dc82f/image.png" alt=""/></p> <p><strong>Failure cases.</strong><br/> 이제부터는 모델의 한계점에 가까운데 아래의 왼쪽 예시와 같이 pose가 너무 가려진다거나 복잡할 경우 잘못된 결과물이 나오고 오른쪽과 같이 인물의 체형과 옷의 shape이 크게 맞지 않을 때 artifact가 생기는 것을 확인할 수 있다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/37064cb6-6d0c-4d84-ad49-fd8d87d35f00/image.png" alt=""/></p> <p><strong>Artifacts near Neck.</strong> <br/> 또다른 문제로는 의류 이미지에서 neck 내부 모습이 들어가 있는 경우 표현되지 말아야 하는 부분이 표현되어 버리는 문제가 생긴다. 이를 해결하기 위해 neck 내부 모습을 제거해주면 해결 되는 것을 확인할 수 있다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/b64007fe-0648-45dc-839c-c28ab1261ad2/image.png" alt=""/></p> <p><br/></p> <h3 id="5-quantitative-results">5. Quantitative Results</h3> <p><strong>Inception Score</strong>는 이미지 생성 모델의 합성 퀄리티를 평가하는 지표로 퀄리티가 높을 수록 높은 점수를 부여 받게 된다. 다만 지금까지 사용했던 Inception Score가 Virtual Try-on의 평가지표로 사용하기에는 적합하지 않다는 결론을 내리게 되었다고 한다.</p> <p>그렇게 <strong>Human evaluation metric</strong>을 따르기로 결정했고 해당 지표로 봤을 때 다른 생성형 모델보다 월등히 성능이 높은 것을 확인할 수 있었다.<br/> <img src="https://velog.velcdn.com/images/lowzxx/post/07719082-8565-45e8-878d-b96868eef008/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="conclusion">Conclusion</h2> </blockquote> <p>결론적으로 전체적으로 비용이 비싼 3D 기반 method대신 2D RGB Image를 이용한 실용적으로 사용될 수 있는 model을 만들었다고 한다.</p> <hr/> <h3 id="reference">Reference.</h3> <p><a href="https://arxiv.org/abs/1711.08447">https://arxiv.org/abs/1711.08447</a><br/> <a href="https://github.com/xthan/VITON">https://github.com/xthan/VITON</a></p>]]></content><author><name></name></author><category term="Basic"/><category term="Basic"/><summary type="html"><![CDATA[Aboujt Quaternion]]></summary></entry><entry><title type="html">[Papers] 3D Gaussian Splatting for Real-Time Radiance Field Rendering (SIGGRAPH 2023)</title><link href="https://oweixx.github.io/blog/2025/papers_3dgaussian/" rel="alternate" type="text/html" title="[Papers] 3D Gaussian Splatting for Real-Time Radiance Field Rendering (SIGGRAPH 2023)"/><published>2025-04-01T00:00:00+00:00</published><updated>2025-04-01T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_3dgaussian</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_3dgaussian/"><![CDATA[<h2 id="3d-gaussian-splatting-for-real-time-radiance-field-rendering">3D Gaussian Splatting for Real-Time Radiance Field Rendering</h2> <h3 id="papergithubdemo"><a href="https://arxiv.org/abs/2308.04079">[Paper]</a><a href="https://github.com/graphdeco-inria/gaussian-splatting">[Github]</a><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">[DEMO]</a></h3> <blockquote> <p><strong>Title:</strong> 3D Gaussian Splatting for Real-Time Radiance Field Rendering<br/> <strong>Journal name &amp; Publication Date:</strong> SIGGRAPH 2023-08-08<br/> <strong>First and Last Authors:</strong> Kerbl, Bernhard</p> </blockquote> <hr/> <blockquote> <h2 id="about-3d-gaussian">About 3D Gaussian</h2> <p>논문을 들어가기전 Gaussian 및 3D Gaussian에 대한 기본적인 개념들을 복기하고 정리하는 부분.</p> </blockquote> <p><strong>Gaussian</strong><br/> 가우시안은 확률에서 정규분포의 확률 밀도 함수(PDF)를 나타낼 때 주로 사용되는 함수이다. <strong>평균, 분산, 표준편차만으로 정의될 수 있다는 것이 특징</strong>이다.</p> \[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\] <ul> <li>$\mu$: 평균</li> <li>$\sigma^2$: 분산 (표준편차 $\sigma$의 제곱)</li> <li>$x$: 확률 변수</li> </ul> <p><strong>특징:</strong> 평균을 중심으로 대칭적으로 분포하며, 분산이 클수록 데이터가 더 넓게 퍼지며 분포의 첨도(kurtosis)가 낮아진다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/11b063d4-5180-4522-8e49-bd65f11b97f8/image.png" alt=""/></p> <p><strong>공분산(Covariance)</strong><br/> 공분산이란 두 개 이상의 변수 간의 상관 관계를 수량화하는 척도이다. 데이터의 분산이 한 변수에서 다른 변수로 어떻게 함께 변화하는지, 어떤 관계를 갖고 있는지를 나타낸다.</p> <p>**공분산 행렬$\sum$(Covariance Matrix) **<br/> 공분산 행렬은 다변량 데이터(여러 변수)에서 각 변수 간 공분산을 행렬 형태로 나타낸 행렬이다.</p> \[\Sigma = \begin{bmatrix} \text{Var}(X_1) &amp; \text{Cov}(X_1, X_2) &amp; \cdots &amp; \text{Cov}(X_1, X_n) \\ \text{Cov}(X_2, X_1) &amp; \text{Var}(X_2) &amp; \cdots &amp; \text{Cov}(X_2, X_n) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \text{Cov}(X_n, X_1) &amp; \text{Cov}(X_n, X_2) &amp; \cdots &amp; \text{Var}(X_n) \end{bmatrix}\] <ul> <li>$\text{Var}(X_i)$: 변수 $X_i$의 분산$(\sigma_i^2)$</li> <li>$\text{Cov}(X_i,X_j)$: $X_i$와$X_j$ 간의 공분산</li> </ul> <p><strong>특징</strong></p> <ul> <li>대칭 행렬: $\text{Cov}(X_i,X_j) = \text{Cov}(X_j,X_i)$</li> <li>대각 성분: 대각선 요소는 각 변수의 분산.</li> </ul> <p><strong>3D Gaussian</strong></p> \[f(x,y,z)=\frac{1}{(2\pi)^{3/2}\left|\sum\right|^{1/2}}e^{-\frac{1}{2}r^\top\sum^{-1}r}\] <ul> <li>$r$ = $\left[x-\mu_x,y-\mu_y,z-\mu_z\right]^T$: 평균($\mu$)에서의 거리 벡터</li> <li>$\sum$: 3x3 공분산 행렬 (크기와 방향성을 조정)</li> </ul> <p><strong>특징</strong></p> <ul> <li><strong>공간적 분포(위치):</strong> 3D 공간의 특정 지점(평균)을 중심으로 데이터가 퍼진다.</li> <li><strong>공분산 행렬(모양, 방향):</strong> 3D Gaussian의 분포 모양은 공분산 행렬 $\sum$에 의해 결정된다. <ul> <li>대각 성분(분산): 각 축의 분산을 통해 Gaussian의 장축, 단축 크기를 결정한다.</li> <li>비대각 성분(공분산): 비대각의 공분산 값을 통해 축 사이의 회전을 나타낸다. 즉, 타원의 방향을 결정한다.</li> </ul> </li> </ul> \[\Sigma = \begin{bmatrix} \sigma_x^2 &amp; \sigma_{xy} &amp; \sigma_{xz} \\ \sigma_{xy} &amp; \sigma_y^2 &amp; \sigma_{yz} \\ \sigma_{xz} &amp; \sigma_{yz} &amp; \sigma_z^2 \end{bmatrix}\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/d554ccf8-2c6c-41b9-932c-a94f5cb849be/image.png" alt=""/></p> <p><strong>질문</strong></p> <ul> <li>3D Gaussian은 함수형태로 point cloud를 표현하므로 <strong>implicit(암시)하게 정의</strong>되는것이 아닌가? <ul> <li>함수 형태로 표현되기 때문에 implicit한 것은 맞다. 하지만 Gaussian이라는 것도 결국 $\mu$와 $\sum$로 표현이 되기 때문에 굳이 말하자면 명시적으로 나열될 수 있는 표현이다.</li> <li>결론적으로 implicit한 표현으로 보았을 때 Gaussian함수는 연속적인 표현이 가능하며 메모리 효율성을 챙기고 부드러운 분포를 보일 수 있다는 장점을 가지게 된다.</li> </ul> </li> <li>왜 공분산 행렬을 RSS^TR^T와 같이 정의하는것일까 ? <ul> <li>Symmetric한 성질을 만들기 위하여.</li> <li>$(AA^\top)^\top = A^\top A$</li> </ul> </li> </ul> <p><strong>radiance field</strong></p> <ul> <li>3D 공간에서 빛과 색상 분포를 의미하는 함수, 개념.</li> </ul> <hr/> <blockquote> <h2 id="1-abstract--introduction">1. Abstract &amp; Introduction</h2> </blockquote> <p>3D 장면 표현 방식에서는 그동안 많은 발전이 이루어져 왔다. 대표적으로 NeRF는 MLP를 사용하여 암묵적인 특징을 최적화(implicit optimization)하며, 높은 성능을 보여주고 관련 논문들이 지속적으로 등장하고 있는 중이다.</p> <p>이 분야에서는 공통적으로 해결해야 할 문제와 도전 과제로 다음과 같은 점들이 존재한다.</p> <blockquote> <ul> <li>여러 장의 사진을 통해 장면을 <strong>효율적이고 빠르게 최적화 하고 표현</strong>하는 것.</li> <li><strong>실시간 렌더링(real-time rendering)</strong> 을 가능하게 하는 것.</li> </ul> </blockquote> <p>이번 논문에서는 기존의 SOTA 모델보다 더 빠르고 효율적인 최적화와 표현, 그리고 실시간 렌더링을 지원하는 <strong>3D Gaussian Splatting</strong>을 제안한다.</p> <p>논문에서는 다음과 같은 3가지 주요 방법론을 통해 이를 실현한다.</p> <blockquote> <ul> <li><strong>비등방성 3D Gaussian의 도입 (Anisotropic 3D Gaussians)</strong> <ul> <li>high-quality radiance field를 <strong>비구조적</strong>으로 표현하기 위한 방식.</li> <li>이르르 통해 장면의 세부 정보를 더 정확하게 표현이 가능하다.</li> </ul> </li> <li><strong>3D Gaussian 속성 최적화 (Optimization Method of 3D Gaussian Properties)</strong> <ul> <li>3D Gaussian의 속성을 최적화.</li> <li>adaptive density control을 교차적으로 활용</li> </ul> </li> <li><strong>GPU 기반 빠른 미분 가능 렌더링 (Fast, Differentiable Rendering Approach for the GPU)</strong> <ul> <li>가시성 인식(visibility-aware): 시점에서 보이는 부분만 효율적으로 계산</li> <li>비등방성 splatting(anisotropic splatting): 더 정밀한 장면 렌더링 가능</li> <li>fast backpropagaton: 효율적인 학습과 novel view synthesis 지원</li> </ul> </li> </ul> </blockquote> <hr/> <blockquote> <h2 id="2-related-work">2. Related Work</h2> </blockquote> <hr/> <blockquote> <h2 id="3-overview">3. Overview</h2> </blockquote> <ol> <li><strong>Input</strong> <ul> <li>입력 데이터는 정적인 장면의 이미지 세트와 <strong>SfM(Structure-from-Motion)</strong>을 통해 보정된 카메라 정보가 들어온다.</li> </ul> </li> <li><strong>3D Gaussian</strong> <ul> <li>SfM을 통해 들어온 point cloud를 통해 3D Gaussian 집합을 생성한다.</li> <li>3D Gaussian은 위치(mean), 모양과 방향(covariance matrix), 불투명도($\alpha$, opacity)를 통해 정의된다.</li> </ul> </li> <li><strong>Radiance Field</strong> <ul> <li>Radiance Field Color는 <strong>spherical harmonics, SH</strong>를 사용하여 표현된다.</li> </ul> </li> <li><strong>Optimization</strong> <ul> <li>3D Gaussian의 주요 파라미터인 mean, covariance matrix, $\alpha$, SH coefficients를 최적화하여 randiance field를 올바르게 표현한다.</li> <li>이 과정에서 <strong>adaptive Gaussian density control</strong>가 교차적으로(interleaved) 적용된다.</li> </ul> </li> <li><strong>Tile-based rasterizer</strong> <ul> <li>타일기반의 3D에서 2D 이미지로의 변환은 다음과 같은 효율성과 기능을 가진다. <ul> <li><strong>가시성 순서를 고려한 빠른 정렬로 $\alpha$-blending</strong> 지원.</li> <li>빠른 backward pass 구현</li> <li>가우시안의 개수 제한 없이 그래디언트 계산 가능.</li> </ul> </li> </ul> </li> </ol> <p>전체적인 procedure에 대한 도식화는 아래와 같다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/a1cc2eda-44a0-4449-a46a-e3df572235ad/image.png" width="900"/></p> <p><strong>추가적으로 알아볼 것(SfM, spherical harmonic)</strong></p> <hr/> <blockquote> <h2 id="4-differentiable-3d-gaussian-splatting">4. Differentiable 3D Gaussian Splatting</h2> </blockquote> <p>먼저 위에서의 한계점들을 극복하기 위해서는 미분가능한 volumetric 표현을 가지며 explicit한 특징으로 빠르게 렌더링이 가능한 표현법이 필요하다고 한다.<br/> 3D Gaussian은 미분가능한 표현법이며 2d splats로의 projection과 $\alpha$-blending을 통해 빠른 렌더링이 가능하기 때문에 선택했다고 한다.</p> <p><strong>3D Gaussian initialization</strong><br/> 이전의 표현법들은 normal을 통해 추정하고 optimizing하는게 굉장히 challeging하다고 한다. 그래서 해당 논문에서는 normal정보가 필요없고 <strong>covariance matrix ($\sum$),centered point ($\mu$)를 통해 정의할 수 있는 3D Gaussian</strong>을 사용한다.</p> \[G(x) = e^{-\frac{1}{2}(x)^T\sum^{-1}(x)}\] <p><strong>2D Projection Transform</strong><br/> 위에서 표현한 3D Gaussian을 projection을 위해 2D camera 좌표계인 $\Sigma’$로 변환 할 필요가 있다.</p> \[\Sigma' = J W \Sigma W^T J^T\] <blockquote> <p>$W$: <strong>viewing transformation</strong>으로 카메라 좌표계로의 변환을 의미한다.<br/> $J$: <strong>projective transformation</strong>인 Jacobian행렬로 비선형적인 투영변환을 통해 선형 근사하는 역할을 한다.<br/> <strong>Why 이런 식이 나올까 ?</strong></p> <ul> <li>쉽게 말해 $\Sigma_{WJ} = \Sigma’$으로 볼 수 있는데 <strong>Covariance matrix $\Sigma$</strong> Covariance matrix를 직접적으로 최적화 하기 위해서는 $\Sigma$가 positive semi-definite 조건을 가지고 있어야 한다. 만약 해당 조건을 충족하지 못하고 최적화를 시킨다면 잘못된 optimizing을 할 수도 있게 된다.</li> </ul> </blockquote> <p><strong>Positive semi-definite</strong> 이를 위해 <strong>scaling matrix $S$와 rotation matrix $R$을 사용하여 더 직관적이고 표현력이 높은 $\Sigma$를 구성</strong>하였다.</p> <p><strong>어떻게 이렇게 정의가 가능한 것인가 ?</strong></p> <ul> <li>이는 기존의 공분산이 가지는 의미를 통해 이해를 할 수 있다. 3D Gaussian에서 <strong>공분산은 타원체의 shape과 방향을 조정하는 역할</strong>을 하기 때문에 이는 Scale과 Rotation matrix만을 통해 표현이 가능하다는 것이다.</li> <li>공분산의 대칭성 성질을 유지하기 위해 S와 R을 통해 표현할 때 <strong>전치행렬도 같이 곱해주어 공분산의 대칭행렬 성질을 그대로 유지</strong>하게 된다.</li> </ul> \[\Sigma = RSS^TR^T\] <hr/> <blockquote> <h2 id="5-optimization-with-adaptive-density-control-of-3d-gaussians">5. Optimization with Adaptive Density Control of 3D Gaussians</h2> </blockquote> <h3 id="optimization">Optimization</h3> <p>3D Gaussian Splatting의 최적화는 렌더링 결과와 훈련 데이터셋 이미지를 비교하며 반복적으로 수행한다. 이 과정에서 발생하는 3D-2D 투영의 모호성을 해결하고 효율적인 장면 표현을 만들어내기 위한 다양한 기법이 사용된다.<br/> Optimization에는 geometry에서 생성, 제거, 이동하는 표현들이 가능해야 한다.</p> <p><strong>SGD</strong><br/> Optimizing 알고리즘으로는 GPU와 CUDA의 장점을 최대한 활용하기 위해 Stochastic Gradient Descent를 사용한다. 덕분에 빠른 rasterization이 가능했다고 한다.</p> <p><strong>Sigmoid</strong><br/> $\alpha$를 최적화하기 위해 sigmoid 함수를 사용하고 [0 - 1)로 제한하며 지수함수를 통해 공분산의 크기를 조절한다.</p> <p><strong>Inital &amp; Loss</strong><br/> 3D Gaussian의 각 축의 길이는 가장 가까운 세점까지의 평균 거리를 이용해서 초기화 해준다. 이를 통해 초반에 빠르고 안정적으로 수렴이 가능하다.<br/> 손실함수는 $\mathcal{L}_1$과 D-SSIM을 합친 함수로 구성되게 된다.</p> \[\mathcal{L} = (1-\lambda)\mathcal{L}_1 + \lambda\mathcal{L}_{\text{D-SSIM}}\] <blockquote> <p><strong>D-SSIM이란 ?</strong></p> <ul> <li><strong>D-SSIM</strong>은 Structural Similarity Index(SSIM)을 손실 함수로 사용할 수 있도록 미분 가능하게 변형한 버전이다.</li> <li>D-SSIM은 렌더링된 이미지와 캡쳐된 훈련 뷰 간의 <strong>구조적 유사성을 비교</strong>하며 모델이 더 나은 3D 표현을 하도록 학습을 돕는다. <ul> <li>노이즈 제거, 초해상도, 스타일 전이 등의 구조적 차이?….</li> </ul> </li> </ul> </blockquote> <h3 id="adaptive-control-of-gaussians">Adaptive Control of Gaussians</h3> <p>초반 SfM을 통해 얻은 sparse set에서부터 adaptively control을 통해 denser Gaussian으로 만들어 장면을 더 잘 표현하게 된다. 이후 <strong>100 iteration마다 밀도를 높이며 $\alpha(\text{transparent})$가 threshold보다 낮을 경우 제거</strong>하는 과정을 거친다.</p> <p><strong>Adaptive Control</strong><br/> Adpative Control에서 필요한 것은 <strong>빈 공간에 Gaussian을 통해 채워 넣는 것</strong>이다. 문제는 너무 적게 채워버리면 <strong>under-reconstruction</strong> 문제가 발생하고, 너무 큰 부분을 커버하면 <strong>over-reconstruction</strong> 문제가 발생한다는 것이다.</p> <p><strong>Under-Reconstruction</strong><br/> 만약 해당 Gaussain에서 채워야 할 곳이 필요하다면 <strong>동일한 size를 가진 Gaussian을 복제하고 positinal gradient쪽으로 이동하여 빈공간을 채우게된다.</strong></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/23cb3554-c6ac-482d-9ccf-400b642b35c5/image.png" alt=""/></p> <p><strong>Over-Reconstruction</strong> <br/> 만약 너무 큰 Gaussian이라면 더 작은 Gaussian으로 나눌 필요가 있다. 이때는 <strong>실험적으로 얻은 $\phi$ = 1.6의 scale factor를 통해 작은 2개의 Gaussian으로 나누고 position을 이동</strong>하게된다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/ccc20cbc-0304-41c6-9390-a620280816c2/image.png" alt=""/></p> <p><strong>Remove Gaussian</strong><br/> 위에서 제시한 방법들을 토대로 Scene의 Gaussian을 최적화하면 Gaussian의 개수는 수도 없이 늘어난다. 이를 해결하기 위해 400 iteration마다 일정 threshold보다 낮은 $\alpha$값을 가진 Gaussian은 주기적으로 제거하는 방법을 사용한다.<br/> 결국 중요한, 유의미한 Gaussian들만 남게 되는 것이다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/c673342b-2e6e-4095-9d5d-006d216ce418/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="6-fast-differentiable-rasterizer-for-gaussians">6. Fast Differentiable Rasterizer for Gaussians</h2> </blockquote> <p><strong>Goal</strong><br/> 결론적으로 목표는 빠른 rendering과 빠른 $\alpha$-blending,최적화 구조를 개선하는 것으로 볼 수 있다.</p> <p><strong>Tile-Based Rasterizer</strong><br/> 화면을 16x16 타일로 분할하여 병렬처리를 극대화한다. 각 타일은 독립적으로 처리되어 데이터 로드 및 계산 부분에서 효율화 한다. 99%의 Confidence에 해당하는 Gaussian만 유지하며 이 외의 Gaussain은 제거하여 계산의 불안정성을 방지한다.</p> <p><strong>$\alpha$-blending</strong><br/> 타일 단위의 정렬을 기반으로 블렌딩을 수행한다. 추가적인 픽셀 단위 정렬 없이도 효율적으로 $\alpha$-blending이 가능하다. 픽셀의 불투명도가 1에 도달하면 해당 픽셀의 처리를 종료하며 각 타일 내 모든 픽셀이 포화되어도 종료하게 된다.</p> <p><strong>Backward Pass</strong><br/> Backward Pass 과정에서 Foward Pass의 Blending 정보를 활용하고 계산하여 효율성을 극대화한다.</p> <p><strong>차별점</strong></p> <ul> <li><strong>픽셀 단위 정렬 제거:</strong> 성능을 크게 향상</li> <li><strong>제한 없는 기울기 계산:</strong> Gaussian 개수와 무관하게 처리 가능.</li> <li><strong>근사 $\alpha$-blending:</strong> 성능을 극대화하면서도 시각적으로 자연스러운 결과를 유지.</li> </ul> <p><img src="https://velog.velcdn.com/images/lowzxx/post/3b0b8245-0a91-4f90-b9bc-b477068dd821/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="7-implementation-results-and-evaluation">7. Implementation, Results and Evaluation</h2> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/688b6437-2783-4059-bdeb-06d94a41e1e6/image.png" alt=""/></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/3fa22134-2b86-400a-b6f9-a9d7b9199d82/image.png" width="900"/></p> <hr/> <blockquote> <h2 id="8-discussion-and-conclusions">8. Discussion and Conclusions</h2> </blockquote> <hr/> <h3 id="reference">Reference.</h3> <p><a href="https://arxiv.org/abs/2308.04079">https://arxiv.org/abs/2308.04079</a><br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a> <br/> <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/</a> <br/> <a href="https://www.researchgate.net/figure/sualization-of-a-3D-Gaussian-model-a-Uncertainty-ellipsoid-for_fig5_231212225">https://www.researchgate.net/figure/sualization-of-a-3D-Gaussian-model-a-Uncertainty-ellipsoid-for_fig5_231212225</a></p>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Paper Review]]></summary></entry><entry><title type="html">[Papers] DETR: End-to-End Object Detection with Transformers (CVPR 2020)</title><link href="https://oweixx.github.io/blog/2025/papers_detr/" rel="alternate" type="text/html" title="[Papers] DETR: End-to-End Object Detection with Transformers (CVPR 2020)"/><published>2025-03-31T00:00:00+00:00</published><updated>2025-03-31T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_detr</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_detr/"><![CDATA[<h2 id="end-to-end-object-detection-with-transformers-cvpr-2020">End-to-End Object Detection with Transformers (CVPR 2020)</h2> <h3 id="paper-github"><a href="https://arxiv.org/abs/2005.12872">[Paper]</a> <a href="https://github.com/facebookresearch/detr">[Github]</a></h3> <blockquote> <p><strong>Title:</strong> End-to-End Object Detection with Transformers<br/> <strong>Journal name &amp; Publication year:</strong> Computer Vision and Pattern Recognition 2020<br/> <strong>First and Last Authors:</strong> Nicolas Carion, Francisco Massa<br/> <strong>First Affiliations:</strong> Facebook AI</p> </blockquote> <p>저번주차 수업을 들으면서 중요하다고 판단되는 몇몇 논문중 한 논문으로 최대한 논문 원문을 보면서 이해해보려고 노력하며 정리해본다.</p> <hr/> <blockquote> <h2 id="1-abstract--introduction">1. Abstract &amp; Introduction</h2> </blockquote> <p>DETR은 <strong>Detection pipeline을 streamlines(간소화)</strong> 하며 hand-designed된 부분들을 최대한 제거하려고 노력했다고 한다. DETR의 큰 특징으로는 transformer의 encoder-decoder부분을 차용한 것과 predictions과 ground_truth의 <strong>bipartite matching(이분 매칭) loss를 적용</strong>한다는 것이다.</p> <p>Object detection 분야에서의 목표는 boding boxes와 category labels 제공하는 것이다. DETR은 마지막 부분에서 prediction과 ground truth를 직접 비교하며 loss를 계산한다는 것인데, <strong>DETR은 (non-autoregressive)parallel decoding을 사용함으로써 병렬 처리 및 출력을 하며 출력된 bounding boxes를 각 ground truth의 짝지어진 box들과 매칭</strong>하고 bipartite matching loss를 이용하여 loss 계산을 한다고 한다.</p> <p>Detection 분야에서 여러번 시험되고 많은 성능 개선이 된 Faster R-CNN과 performance적으로 비슷한 결과를 냈다고 한다. 큰 dataset에서는 더 좋은 성능을 주기도 했었지만 작은 dataset에서는 성능이 더 낮은 결과도 보였다고 한다.</p> <p>DETR은 Detection 뿐만이 아니라 다른 더 복잡한 task 활용하여 좋은 성능을 뽑아냈다고 한다. 예를 들어 segmentation or pixel-level recognition 등등…</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/19a86e1f-3fd2-493f-8266-a086cd33f2d4/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="2-related-work">2. Related Work</h2> </blockquote> <h3 id="21-set-prediction">2.1 Set Prediction</h3> <p>DETR에서는 set of box predictions를 위해 decoder구조에서 multi-task를 수행해야한다. 기존의 Detection model들에서 postprocessings + NMS(non-maximal suppression)가 수행하던 중복 bbox 제거가 set prediction에서의 걸림돌이다. direct set prediction에서 near-duplicates을 피하기 위해서 <strong>Hungarian algorithm을 기반으로 loss function을 설계</strong>한다고 한다. 이는 <strong>permutation-invariance(순열 불변성)을 적용하며 각 대상요소가 unique하게 일치</strong>하도록 해준다.</p> <p><strong>Hungarian Algorithm</strong><br/> match 해야할 두 vector $I$와 $J$가 존재할 때 I와 J에서 매칭되어 나온 cost를 최소화 또는 최대화 하는 이분 매칭 방법에 사용되는 알고리즘이다. 현재 Detection에서는 최대가 되는 cost를 원하는 것이니 최대 Hungarian Algorithm으로 예시를 들어본다.</p> <ul> <li> <p>먼저 행렬의 모든 값에서 최대가 되는 값(78)을 고르고 해당 값에서 각 원소들을 빼준다.</p> <table> <thead> <tr> <th> </th> <th>Track-1</th> <th>Track-2</th> <th>Track-3</th> <th>Track-4</th> </tr> </thead> <tbody> <tr> <td>Detection-1</td> <td>10</td> <td>70</td> <td>55</td> <td>25</td> </tr> <tr> <td>Detection-2</td> <td>62</td> <td>15</td> <td>58</td> <td>35</td> </tr> <tr> <td>Detection-3</td> <td>23</td> <td>78</td> <td>14</td> <td>63</td> </tr> <tr> <td>Detection-4</td> <td>55</td> <td>34</td> <td>47</td> <td>0</td> </tr> </tbody> </table> </li> <li> <p>다음 Detection 행 기준으로 최솟값들에 대하여 행에서 값을 빼준다. (1 = 8, 2 = 16, 3 = 0, 4 = 23)</p> <table> <thead> <tr> <th> </th> <th>Track-1</th> <th>Track-2</th> <th>Track-3</th> <th>Track-4</th> </tr> </thead> <tbody> <tr> <td>Detection-1</td> <td>68</td> <td>8</td> <td>23</td> <td>53</td> </tr> <tr> <td>Detection-2</td> <td>16</td> <td>63</td> <td>20</td> <td>43</td> </tr> <tr> <td>Detection-3</td> <td>55</td> <td>0</td> <td>64</td> <td>15</td> </tr> <tr> <td>Detection-4</td> <td>23</td> <td>44</td> <td>31</td> <td>78</td> </tr> </tbody> </table> </li> <li> <p>Track 열 기준으로 최솟값을에 대하여 해당 열에서 값을 빼준다. (1 = 0, 2 = 0, 3 = 4, 4 = 15)</p> <table> <thead> <tr> <th> </th> <th>Track-1</th> <th>Track-2</th> <th>Track-3</th> <th>Track-4</th> </tr> </thead> <tbody> <tr> <td>Detection-1</td> <td>60</td> <td>0</td> <td>15</td> <td>45</td> </tr> <tr> <td>Detection-2</td> <td>0</td> <td>47</td> <td>4</td> <td>27</td> </tr> <tr> <td>Detection-3</td> <td>55</td> <td>0</td> <td>64</td> <td>15</td> </tr> <tr> <td>Detection-4</td> <td>0</td> <td>21</td> <td>8</td> <td>55</td> </tr> </tbody> </table> </li> <li> <p>그렇게 완성된 행렬을 확인해보았을 때 행렬에 있는 모든 0들을 vector의 개수에 맞게 덮을 수 있다면 최대 값을 구할 수 있게 된다. (ex. 0들을 열 기준으로 1,2,3,4를 선으로 덮을 수 있음.)</p> <table> <thead> <tr> <th> </th> <th>Track-1</th> <th>Track-2</th> <th>Track-3</th> <th>Track-4</th> </tr> </thead> <tbody> <tr> <td>Detection-1</td> <td>60</td> <td>0</td> <td>11</td> <td>30</td> </tr> <tr> <td>Detection-2</td> <td>0</td> <td>47</td> <td>0</td> <td>12</td> </tr> <tr> <td>Detection-3</td> <td>55</td> <td>0</td> <td>60</td> <td>0</td> </tr> <tr> <td>Detection-4</td> <td>0</td> <td>21</td> <td>4</td> <td>40</td> </tr> </tbody> </table> </li> <li> <p>해당 0에 대한 값들을 matching을 시켜보면 (1,2) (2,3) (3,4) (4,1)의 값들이 되고 해당 값들을 모두 더하면 Hungarian Algorithm과 bipartite matching을 통한 최댓값을 구할 수 있게 된다. Final Assignment (Optimal Matching)</p> <ul> <li><strong>Detection-1</strong> to <strong>Track-2</strong>: 70</li> <li><strong>Detection-2</strong> to <strong>Track-3</strong>: 58</li> <li><strong>Detection-3</strong> to <strong>Track-1</strong>: 55</li> <li><strong>Detection-4</strong> to <strong>Track-4</strong>: 63 <strong>Maximum Total Value:</strong> 70 + 58 + 55 + 63 = <strong>246</strong></li> </ul> </li> </ul> <p><br/></p> <h3 id="22-transformers-and-parallel-decoding">2.2 Transformers and Parallel Decoding</h3> <p>DETR에서 핵심적으로 중요한 부분이 Transformer 구조인데 Transformer는 처음에 NLP쪽에서 쓰이던 모델 구조였지만 memory 구조적으로나 long squences를 다루는 부분에서 기존의 RNN보다 낫다는 판단이었고 이를 Vision에서도 사용했었던 여러 논문을 토대로 Transformer 구조를 채택했다고 한다.</p> <p>기존의 Transformer는 Sequence-to-Sequence구조로 출력이 하나씩 나오는 구조라 costly한 단점이 있었다. 이 부분을 해결하기 효율적으로 해결하기 위해 <strong>주어진 위치에서 객체의 위치와 클래스를 한꺼번에 예측하는 즉, 병렬적인 Decoding 문제로 변환하였다는 부분이 특징</strong>이다. 이는 기존의 Transformer대로 사용했을 때 순차적으로 예측하지 않고 병렬적으로 예측하여 inference 속도가 월등히 빨라지게 된다.</p> <p><br/></p> <h3 id="23-object-detection">2.3 Object Detection</h3> <p>기존의 Detection 분야에서의 model들은 One-stage detector나 Two-staege detector나 모두 초반에 설정되는 추측 설정들에 따라 성능이 크게 좌우되는 경향이 있었다. 기존의 이런 불편함들을 모두 간소화시켜 end-to-end detection하는 방법을 보여준다. 기존의 Detection 마지막 부분에서 사용됐던 NMS 대신 direct set losses를 사용하여 이러한 post-processing 부분도 줄일 수 있었다.</p> <hr/> <blockquote> <h2 id="3-the-detr-model">3. The DETR model</h2> </blockquote> <h3 id="31-object-detection-set-prediction-loss">3.1 Object detection set prediction loss</h3> <p>먼저 볼 부분은 set prediction loss로 해당 loss에서는 ground truth와 unique한 matching이 되어야 한다. 기본적으로 처음에 fixed-size로 지정된 N은 $\varnothing$(no object)로도 표현이 될 수 있기 때문에 이미지 내에서 detection 할 객체보다 더 많은 개수로 지정이 되어야 한다.</p> <p>${\sigma \in S_N}$를 따라 N개의 예측값들을 permutation을 통해 $y_i$와 $\hat y_{\sigma(i)}$의 bipartite matching값들의 합이 최소가 되는 <strong>$\sigma$(permutation)를 Hungarian algorithm을 이용해서 찾는게 목표</strong>이다. 해당 match에서는 class의 일치와 boxe의 일치 모두 고려한다고 한다. <strong>$y_i$는 $(c_i,b_i)$로 $c_i$는 class, $b_i$는 bbox</strong>에 대한 4개의 숫자로 이루어진 vector정보로 구성되어있다.</p> \[\sigma = \underset{\sigma \in S_N}{\arg\min} \sum_{i}^{N} \mathcal{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)})\] <p>이렇게 되었을 때 <strong>$\hat y_{\sigma(i)}$는 $\hat p_{\sigma_(i)}(c_i)$로 표현이 되는데, 이는 $\sigma_(i)$일 때 $c_i$일 확률을 의미</strong>하게 된다. 그렇게 $\mathcal{L}<em>{\text{match}}(y_i, \hat y</em>{\sigma_(i)})$는 아래와 같은 식으로 표현 될 수 있다. 왼쪽은 class 분류 손실, 오른쪽은 bbox 손실로 표현이 된다. 왼쪽식은 해당 정답 클래스 확률이 큰게 목표이니 커지면서 -가 붙어 loss가 작아지는 쪽이 되고, 오른쪽은 두 bbox가 같아서 0으로 수렴하게 되는게 목표가 되어 총 손실함수는 작아지는 쪽으로 표현이 된다.</p> \[-1_{\{\hat{c}_{i} \neq \emptyset\}} \, \hat{p}_{\sigma(i)}(c_{i}) + 1_{\{c_{i} \neq \emptyset\}} \, \mathcal{L}_{\text{box}}(b_{i}, \hat{b}_{\sigma(i)}).\] <p>해당 부분이 기존의 Detector model들에서 사용된 match proposal, anchors를 맞추는 부분을 대체 한다고 볼 수 있다. <strong>가장 큰 차이점은 direct set prediction을 이용하기 때문에 중복 제거가 되며 one-to-one matching이 된다는 부분</strong>이다.</p> <p>위에서 구한 표현법을 이용해서 Hungarian algorithm의 loss를 구하게 되면 아래와 같은 수식으로 정의된다. 여기서 $\hat \sigma$는 처음에 구했던 최적의 $\sigma$이다. 만약 $c_i = \varnothing$인 경우 가중치를 10배 낮춰 클래스 불균형을 해소한다고 한다. 그렇게 $\varnothing$ matching cost는 예측에 의존하지 않고 cost는 일정하다.</p> \[\mathcal{L}_{\text{Hungarian}}(y, \hat{y}) = \sum_{i=1}^{N} \left( -\log \hat{p}_{\hat \sigma(i)}(c_{i}) + 1_{\{c_{i} \neq \emptyset\}} \, \mathcal{L}_{\text{box}}(b_{i}, \hat{b}_{\hat \sigma(i)}) \right),\] <h3 id="bounding-box-loss"><strong>Bounding box loss.</strong></h3> <p>일반적으로 많이 사용되는 $\ell_1$ loss는 상대적인 오차가 비슷하더라도 작은 박스와 큰 박스에 대해 서로 다른 갖는 문제점이 있엇기 때문에 이를 완화하기 위해 <strong>스케일 불변인 $\ell_1$ loss와 $\mathcal{L}_\text{iou}$를 결합하여 bounding box loss가 표현</strong>이 된다.</p> \[\lambda_{\text{iou}} \, \mathcal{L}_{\text{iou}}(b_{i}, \hat{b}_{\sigma(i)}) + \lambda_{\text{L1}} \, \|b_{i} - \hat{b}_{\sigma(i)}\|_{1}, \quad \text{where } \lambda_{\text{iou}}, \lambda_{\text{L1}} \in \mathbb{R}\] <p><br/></p> <h3 id="32-detr-architecture">3.2 DETR architecture</h3> <p><img src="https://velog.velcdn.com/images/lowzxx/post/84620733-10de-4673-9158-0bb46541c22e/image.png" alt=""/> DETR은 크게 CNN, encoder-decoder Transformer, feed forward network (FFN)로 simple하게 구성되어 있다.</p> <h3 id="backbone"><strong>Backbone.</strong></h3> <p>입력으로 들어오는 image $x_{\text{img}} \in \mathbb{R}^{3 \times H_0 \times W_0}$를 <strong>compact feature representation으로 표현하기 위해 CNN backbone</strong>에 들어가게 되고 $C=2048$, $H,W = \frac{H_0}{32}, \frac{W_0}{32}$로 정의된 $\mathbb{R}^{C \times H \times W}$차원을 가진 형태로 출력이 된다.</p> <h3 id="transformer-encoder"><strong>Transformer encoder.</strong></h3> <p>먼저 channel dimension을 줄이기 위해 1x1 convolution을 이용하여 $\mathbb{R}^{d \times H \times W}$ 차원으로 줄여주고 이를 $z_0$라고 표현한다. 또 sequence 형태로 표현하기 위해 $d \times HW$형태로 표현해준다. encoder 구조는 Multi-Head Self-Attention과 FFN구조로 이루어져 있고 <strong>Transformer는 입력 시퀀스의 순서를 인식하지 못하는 순열 불변성(permutation-invariant)이므로 순서 정보를 보존하기 위해 positional encoding을 추가</strong>해준다.</p> <h3 id="transformer-decoder"><strong>Transformer decoder.</strong></h3> <p>decoder는 기존의 transformer의 standard한 architecture를 따르며 $d$크기의 $N$개의 embedding으로 변환하는 Multi-Head Attention 구조를 가진다. <strong>기존의 decoder 다른 부분은 N개의 object를 병렬적으로 decoding한다는 것이다.</strong> <br/> decoder역시 permutation-invariant 특성을 가지므로 learned positinal encodings인 Object Query를 디코더의 입력으로 사용된다. 신기하게도 <strong>object query는 positinal encoding의 역할과 encoder의 출력값들에 대한 정보를 학습하는 query의 역할</strong>을 동시에 하고 있다. 그렇게 추가된 N개의 object query는 decoder의 단계를 거쳐 class와 bbox를 최종 예측할 수 있게 된다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/5166db6c-5bfe-4897-b408-a2dc9a3bd7a3/image.png" alt=""/></p> <p><strong>Prediction feed-forward network (FFNs).</strong><br/> 마지막 FFN은 ReLU함수를 사용하는 3-layer perceptron과 linear projection으로 구성되어 있다. Bounding Box는 중심좌표와 h,w로 구성되어있고, Class는 softmax function을 통해 예측을 한다. $\varnothing$로 검출이 되기도 하는데 이는 background class로 이해시킨다고 한다.</p> <p><strong>Auxiliary decoding losses.</strong><br/> decoder label뒤에 예측 FFN과 Hungarian loss를 추가하여 보조 loss로 사용하면 학습에 도움된다고 한다.</p> <hr/> <blockquote> <h2 id="4-experiments">4. Experiments</h2> </blockquote> <h3 id="41-comparison-with-faster-r-cnn">4.1 Comparison with Faster R-CNN</h3> <p>기존의 SOTA 모델이였던 Faster R-CNN과 성능 비교를 보여주는 정량적 지표이다.<br/> <img src="https://velog.velcdn.com/images/lowzxx/post/3f448739-a35c-4091-8693-0e6a3ed0837f/image.png" alt=""/></p> <p><br/></p> <h3 id="42-ablations">4.2 Ablations</h3> <p>row에서 encoder layers에 따른 성능 변화를 확인할 수 있다. <img src="https://velog.velcdn.com/images/lowzxx/post/66bdcf18-4860-45fb-bcc2-7eec420c547b/image.png" alt=""/></p> <p>마지막 encoder layer에서의 attention maps를 visualize한 모습이다. <img src="https://velog.velcdn.com/images/lowzxx/post/9444f3a8-de81-4f81-b9fa-5ee2d91a3ae8/image.png" alt=""/></p> <p>다음은 rare classes의 distribution generalization을 보여주는 모습이다.<br/> <img src="https://velog.velcdn.com/images/lowzxx/post/f5a5089e-d081-4e99-86d3-0e3b2ccfdce7/image.png" alt=""/></p> <p>출력 결과물과 decoder의 attention maps를 visualize한 모습이다.<br/> <img src="https://velog.velcdn.com/images/lowzxx/post/6e3b689c-e06e-4ae7-9f72-3d3aab6ea6e9/image.png" alt=""/></p> <p><br/></p> <h3 id="43-analysis">4.3 Analysis</h3> <p><strong>Decoder output slot analysis</strong><br/> <img src="https://velog.velcdn.com/images/lowzxx/post/d803c3ca-0b4d-4c95-9429-d3e88d42758e/image.png" alt=""/></p> <p><strong>Generalization to unseen numbers of instances.</strong><br/> <img src="https://velog.velcdn.com/images/lowzxx/post/93165fc6-513a-402c-a00c-a073bdaf0bc2/image.png" alt=""/></p> <p><br/></p> <h3 id="44-detr-for-panoptic-segmentation">4.4 DETR for panoptic segmentation</h3> <p>DETR의 decoder outputs단에 adding mask를 통하여 panoptic segmentation task를 수행하는 것을 도식화로 보여준다.<br/> <img src="https://velog.velcdn.com/images/lowzxx/post/5e41b16e-8f99-41ff-a50c-12f533099bb4/image.png" alt=""/></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/39131794-4594-4db3-ab47-d38ac8354878/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="5-conclusion">5. Conclusion</h2> </blockquote> <p>DETR(Detection Transformer)은 새로운 객체 탐지 접근 방식으로, Transformer를 활용하여 객체 탐지 문제를 End-to-End로 해결하는 모델이다. <strong>전통적인 Detection과 달리 anchor와 비최대 억제(non-maximum suppression) 같은 후처리 과정을 필요로 하지 않으며</strong>, transformer 구조를 이용하여 detection task를 성공적으로 수행한다.</p> <hr/> <h3 id="reference">Reference.</h3> <p><a href="https://arxiv.org/abs/2005.12872">https://arxiv.org/abs/2005.12872</a></p>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Paper Review]]></summary></entry><entry><title type="html">[Papers] Multiply: Reconstruction of Multiple People from Monocular Video in the Wild (CVPR 2024)</title><link href="https://oweixx.github.io/blog/2025/papers_multiply/" rel="alternate" type="text/html" title="[Papers] Multiply: Reconstruction of Multiple People from Monocular Video in the Wild (CVPR 2024)"/><published>2025-03-31T00:00:00+00:00</published><updated>2025-03-31T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_multiply</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_multiply/"><![CDATA[<h2 id="multiply-reconstruction-of-multiple-people-from-monocular-video-in-the-wild">MultiPly: Reconstruction of Multiple People from Monocular Video in the Wild</h2> <h3 id="paper-github-demo"><a href="https://arxiv.org/abs/2406.01595">[Paper]</a> <a href="https://github.com/eth-ait/MultiPly">[Github]</a> <a href="https://eth-ait.github.io/MultiPly/">[Demo]</a></h3> <blockquote> <p><strong>Title:</strong> MultiPly: Reconstruction of Multiple People from Monocular Video in the Wild<br/> <strong>Journal name &amp; Publication year:</strong> CVPR 2024<br/> <strong>First and Last Authors:</strong> Jiang, Zeren<br/> <strong>First Affiliations:</strong> ETH Z ̈urich</p> </blockquote> <hr/> <blockquote> <h2 id="abstract--introduction">Abstract &amp; Introduction</h2> </blockquote> <p><strong>Limitation</strong> 기존 3D Shape estimating과 3D Reconstruction 분야는 빠르게 발전해왔지만 monocular(단일 카메라) video에서 <strong>인물들간의 interaction이 있을 때 즉, occlusion이 있을 때 잘못된 예측과 생성을 하는 한계점</strong>들이 존재하였다.</p> <p><strong>Expected Outcomes</strong> 만약 이를 통해 해결이 되고 더욱 발전이 된 Model과 방법론들이 등장한다면 장비가 비싼 Virtual이나 증강, 가상현실 산업에서 카메라 하나를 가지고 인물을 온전히 Reconsturction 할 수 있는 AI 기술도 나올 수 있지 않을까 싶다.</p> <p>결론적으로 MultiPly는 위의 multi-person reconstruction의 limitation을 해결하고 3D, AR, XR, 4D등 다양한 산업에서도 사용할 수 있는 방법론을 제시한다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/86af97b8-71de-4e59-a2b6-2c498e00dbc0/image.png" width="900"/></p> <hr/> <blockquote> <h2 id="method-1-layered-neural-representation">Method 1: Layered Neural Representation</h2> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/cd306b4f-2423-4f7b-80a9-0b76e43893a7/image.png" width="900"/></p> <h3 id="neural-avatars">Neural Avatars.</h3> <p>기본적으로 3D Reconstruction을 위해 인물을 표현하는 3D shape 정보와 인물의 외관을 표현하기 위한 Appearance 정보가 필요하게 된다.</p> <blockquote> <p><strong>Implicit Representation</strong> 지속적으로 나오는 <strong>Implicit Representation</strong>이라는 표현은 단순히 3D 형태로 표현하기 위한 mesh나 point cloud가 아닌 수학적 표현으로 함축하겠다는 것을 의미하게 된다. 이는 계산과 자원의 효율성을 충족하기 위해서 사용한다고 봐야할 것이다.</p> </blockquote> <p><strong>3D Shape</strong> 먼저 3D shape의 기하학적인 구조를 표현하기 위해 <strong>signed-distance field(SDF)</strong> 를 사용하게 된다. <strong>SDF는 3D 공간의 각 점에 대해 표면까지의 거리와 방향을 계산해 저장하는 함수</strong>이다. 이를 통해 3D shape정보를 mesh나 point cloud 없이 컴팩트하게 저장할 수 있게 된다.</p> <blockquote> <p><strong>Signed Distance Field (SDF)</strong> SDF는 3D 공간 내의 각 점에서 특정 표면까지의 <strong>부호 있는 거리(signed distance)</strong> 를 정의하는 스칼라 필드이다. 이 값은 기하학적 정보를 컴팩트하게 표현할 수 있어 다양한 3D 표현 방식에서 사용된다.</p> <p><strong>SDF의 구성 원리</strong></p> <ul> <li> <p><strong>정의:</strong> SDF(p)는 3D 공간의 점 p에서 특정 표면까지의 최소 유클리드 거리와 부호를 반환하는 함수입니다.</p> </li> <li><strong>양수 값 (+):</strong> 점이 표면의 외부에 위치함을 의미.</li> <li><strong>음수 값 (-):</strong> 점이 표면의 내부에 위치함을 의미.</li> <li><strong>값이 0일 때:</strong> 점이 표면 위에 정확히 위치함을 의미.</li> </ul> </blockquote> <p><strong>Appearance</strong> 다음으로 인물의 외관 Appearance을 Texture Field로 표현한다. 이는 특정 점에서의 색상이나 방사량(빛의 강도)을 표현하는 함수로 사람의 외형(옷, 피부색)을 나타낼 수 있게 된다. <strong>SDF로 출력된 값들 중 값이 0인 것들은 물체의 표면</strong>을 의미하기 때문에 이에 집중하여 3D 좌표와 SDF값에 따라서 $c^p$(RGB, 광학 방사량)을 계산하게 되는 것이다.</p> <p><strong>Layerd Representation</strong> 해당 논문에서는 multiple people들을 분리하여 3D Representation하는 것이 목표이므로 이를 위해 Layer를 기반으로 각 인물에 대한 정보들을 표현하며 최종적으로 모든 계층을 종합하여 구성하는 방식을 따르고 있다. 결론적으로 <strong>하나의 모델로 학습</strong>을 하지만 <strong>독립적인 계층으로 분리</strong>하여 각 인물들의 정보를 표현한다는 것이다.</p> <p>최종적으로 수식적으로 살펴보면 <strong>사람 $p$를 표현하기 위한 신경망 $f^p$로 표현</strong>이된다. \(c^p, s^p = f^p(x_c^p, \theta^p)\)</p> <blockquote> <p>$f^p$: 사람 p를 표현하는 신경망 $x_c^p$: 특정 점의 좌표 $\theta^p$: 인물의 pose paramter $c^p$: texture field 정보 $s^p$: SDF 정보</p> </blockquote> <h3 id="deformation-module">Deformation Module.</h3> <p>현재 우리가 표현하는 <strong>canonical space의 해당하는 $x_c^p$좌표들은 인물의 포즈 정보가 담겨있지 않은 T-Pose의 정보</strong>로 구성되어있다. 이를 <strong>포즈 정보를 포함한 Deformed space의 $x_c^p$로 표현하기 위해 대중적으로 사용하는 SMPL을 사용</strong>한다. 추가적으로 자연스러운 skin 표현을 위해 LBS도 사용한다고 한다.</p> <p>반대로 T-Pose를 표현하기 위해 $x_d^p$와 $\theta^p$를 SMPL의 역함수를 이용하여 $x_c^p$로 표현할 수도 있다고 한다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/ae8c105f-9773-40c6-8661-d5e18cd679eb/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="method-2-layer-wise-volume-rendering">Method 2: Layer-Wise Volume Rendering</h2> <p>해당 부분은 Layer별 인물들을 Volume Rendering 하기 위한 방법들을 설명한다.</p> </blockquote> <h3 id="volume-rendering-for-human-layers">Volume Rendering for Human Layers.</h3> <p>기존의 Vanila Volume Rendering과 조금 다르게 dynamic한 장면들과 인물별로 layered된 표현을 Volume Rendering하고 싶어하는 것이 목표이며 차이점이다.</p> <p><strong>opacity &amp; density</strong><br/> 먼저 Volume Rendering을 위한 각 점에 대한 opacity와 density를 구하게 된다. 밀도 $\sigma$는 deformed 좌표와 SMPL의 역함수를 이용한 값에 Laplace distribution’s Cumulative Distribution Function(CDF)를 이용하여 계산하고 해당 밀도를 이용하여 sampling된 점들의 좌표값 차이와 함께 해당 sampling의 opacity값을 계산하게 된다.</p> \[o_i^p = 1 - \exp(-\sigma_i^p \Delta x_i)\] \[\sigma_i^p = \sigma \left( f_s^p \left( T_{\text{SMPL}}^{-1}(x_{d,i}^p, \theta^p), \theta^p \right) \right)\] <p><strong>Radiance Accumulation</strong><br/> 위에서 구한 opacity값과 color값을 이용하여 인물별 Volume Rendering을 진행하게 된다.</p> <p>핵심적으로 multi-people에서의 Volume Rendering 문제를 해결하기 위해 $Z_i^{q,p}$를 이용하여 샘플 i점보다 앞에 있는 점들의 집합을 이용하여 가려지는(occlusion)문제를 해결했다고 한다.</p> \[\hat{C}_H = \sum_{i=1}^N \sum_{p=1}^P \left[o_i^p c_i^p \prod_{q=1}^P \prod_{j \in Z_i^{q,p}} \left( 1 - o_j^q \right) \right]\] \[Z_i^{q,p} = \{j \in [1, N] \mid z(x_{d,j}^q) &lt; z(x_{d,i}^p) \}\] <h3 id="scene-composition">Scene Composition</h3> <p>위에서 구한 인물에 대한 Volume Rendering Color값 $\hat C_H$값과 NeRF++에서 사용되는 Background Color값 구하는 $f^b$를 이용하여 나온 $\hat C^b$을 Composition 하여 최종 Color값 $\hat C$를 계산하게 된다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/8a4006f8-9a81-41ab-8e9d-e72b1ad4b516/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="method-3-progressive-prompt-for-sam">Method 3: Progressive Prompt for SAM</h2> <p>SAM을 이용하여 더 정확한 instance segmentation mask를 업데이트하고 생성하는 방법론들을 설명한다.</p> </blockquote> <p>아직도 occlusion에 대한 문제가 있기 때문에 이를 해결하기 위해 <strong>promptable segmentation model인 SAM을 이용하여 더 정확한 인물별 instance segmentation이 가능하게 된다.</strong></p> <p><strong>Deformed Mesh</strong><br/> SDF에서 매쉬를 효율적으로 추출할 수 있는 Multiresolution IsoSurface Extraction(MISE) 알고리즘을 이용하여 해당 p에 대한 mesh 값들을 구하게 된다.</p> \[S_d^p = &lt;V_d^p, F^p&gt; = \text{MISE}(f_s^p,\theta^p)\] <p><strong>Instance Mask</strong><br/> 이후 변형된 mesh를 differentiable rasterizer $R$을 이용하여 instance mask $\mathcal{M}$을 만들어준다. $\mathcal{M}$ = 1 : 메쉬 내부 영역 $\mathcal{M}$ = 0 : 메쉬 외부 또는 가려진 영역</p> \[\mathcal{M}_{\text{mesh}}^p = R(S_d^p).\] <p><strong>Point Prompt</strong><br/> 추가적으로 SAM에 전달할 point prompt를 2D keypoint기반으로 생성한다. 해당 값들은 SMPL을 통해 나온 파라미터들로 구할 수 있다.</p> \[\mathcal{K}_{2d}^p = \{ \Pi (\mathcal{J}(\theta^p, \beta^p)) \},\] <p><strong>Progressive Update</strong><br/> 위에서 구한 prompt들을 이용하여 instance mask $\mathcal{M}$을 지속적으로 업데이트하여 보다 정확한 segmentation 정보를 얻게 된다.</p> \[\mathcal{M}_\text{sam}^p = \text{SAM}(\mathcal{M}_\text{mesh}^p, \mathcal{P}_\text{+}^p,\mathcal{P}_\text{-}^p)\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/281e6fdc-e7b2-4cbd-abdb-e6c2f6d0d7de/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="method-4-confidence-guided-alternating-optimization">Method 4: Confidence-Guided Alternating Optimization</h2> </blockquote> <p>인물간의 가려짐으로 인한 부정확한 pose나 잘못된 depth를 예측하는 경우가 생기게 된다. 기존의 pose와 shape을 동시에 optimize하는 것보다 좋은 방법으로 <strong>pose와 shape을 번갈아 최적화하는 confidence-guided optimization(신뢰도 기반 최적화)을 제시</strong>한다.</p> <p><strong>Confidence-Guided Optimization</strong><br/> mesh기반 mask $\mathcal{M}<em>{\text mesh}^{p,i}$ 와 SAM을 통해 정제된 $\mathcal{M}</em>{\text sam}^{p,i}$ 를 통해 IoU 계산을 함으로써 해당 mask에 대한 신뢰도를 측정한다.</p> <p>한 frame의 scene에 있는 mask에 대해 모두 계산한 IoU의 평균값이 $\alpha (\text{threshold})$ 이상일 경우 해당 frame을 reliable한 frame이라고 정의한다.</p> \[\mathcal{L}_r = \{ I_i \in \mathcal{L} \mid \frac{1}{P} \sum_{p=1}^P \text{IoU}(M_{\text mesh}^{p,i}, M_{sam}^{p,i}) \geq \alpha \},\] <p><strong>Alternating Optimization</strong></p> <blockquote> <p>“To avoid damaging shape updates that are due to wrong poses, we only optimize pose parameters for unreliable frames and jointly optimize pose and shape parameters for reliable frames.”</p> </blockquote> <p>위의 표현에서 볼 수 있듯이 wrong poses로 인해 shape이 손상되지 않도록 unreliable frame에서는 pose만, reliable frame에서는 pose와 shape을 동시에 optimization하는 방법을 제안한다.</p> <hr/> <blockquote> <h2 id="method-5-objectives">Method 5: Objectives</h2> </blockquote> <h3 id="reconstruction-loss">Reconstruction Loss.</h3> <p>예측된 Color와 GT의 Color의 $L_1-\text{distance}$ 값으로 Loss를 구성한다. $\mathcal{R}$은 샘플링된 ray를 의미한다.</p> \[L_{rgb} = \frac{1}{\left|\mathcal{R}\right|} \sum_{r \in \mathcal{R}} \left| C(r) - \hat {C}(r) \right|\] <h3 id="instance-mask-loss">Instance Mask Loss.</h3> <p>Volume Rendering에서 사용하는 opacity $o^p$ 값을 최적화 하는 Loss이다.</p> \[\hat{O}^p(r) = \sum_{i=1}^N \left[ o_i^p \prod_{q=1}^P \prod_{j \in Z_i^{q,p}} (1 - o_j^q) \right].\] <p>아래는 sam의 refined mask와 인물 $p$에 대한 투명도 값의 $L_1-\text{distance}$를 계산한다. mask와 mask가 아닌것이 의아했지만, 생각해보면 <strong>두 값 모두 해당 pixel에 해당 인물이 있냐 없냐를 표현한다는 동일한 목표</strong>를 가지고 있기 때문에 충분히 계산이 가능하고 최적화가 가능한 부분이다.</p> \[L_{\text{mask}} = \frac{1}{|\mathcal{R}|} \sum_{r \in \mathcal{R}} \sum_{p=1}^P \left| \mathcal{M}_{\text{sam}}^p(r) - \hat{O}^p(r) \right|.\] <h3 id="eikonal-loss">Eikonal Loss.</h3> <p>SDF 함수의 gradient 값을 1로 제약을 두는 loss를 구성한다. 만약 gradient 크기가 1을 유지하지 못하면 SDF의 성질을 갖지 못하는 것이므로 신뢰하기 어렵게 된다.</p> \[L_e = \sum_{p=1}^P \mathbb{E}_{x_c} \left( \left\| \nabla f_s^p(x_c^p, \theta^p) \right\| - 1 \right)^2.\] <h3 id="depth-order-loss">Depth Order Loss.</h3> <p>결국 3D Reconstruction이기 때문에 중요한 depth order에 대한 loss를 구현한다.</p> \[L_\text{depth} = \sum_{(u,p,q) \in \mathcal{D}} log(1+exp(D_p(u)-D_q(u))),\] <h3 id="interpenetration-loss">Interpenetration Loss.</h3> <p>해당 Loss는 물리적으로 불가능한 사람들의 겹치는 현상을 방지하기 위해 쓰이는 Loss이다. mesh기반의 3D 분야에서 많이 보이는 문제로 한 사람의 팔이 다른 사람의 팔을 침투하는 것이 예시가 된다.</p> <p>이는 복잡한 다중 사람 장면에서 메쉬 복원의 정확도를 높이고, 물리적으로 타당한 결과를 생성하는 데 도움을 준다.</p> \[L_\text{inter} = \sum_{p=1}^{P} \sum_{q=1,q\neq p}^{P} \left|| \mathcal{V}_{in}^{p,q} - NN(\mathcal{V}_{in}^{p,q},S_d^q) \right||\] <hr/> <blockquote> <h2 id="experiments">Experiments</h2> </blockquote> <p>보다 높은 성능을 보여주기 위해 사용된 방법론들이 추가적으로 많은 디테일들을 끌어 올려주는 것을 확인할 수 있다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/624cb23e-47dd-43e3-8de3-2007d39ee0ed/image.png" alt=""/></p> <p>multi-people Reconstruction에서 겹침이 있음에도 인물간의 복원과 분리가 완벽히 되는 것을 확실하게 보여주는 사진이다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/fad61999-66a1-447a-8808-81207287e011/image.png" width="900"/></p> <hr/> <h3 id="reference">Reference.</h3> <p>https://arxiv.org/abs/2406.01595 https://github.com/eth-ait/MultiPly https://eth-ait.github.io/MultiPly/ https://www.youtube.com/watch?v=r9giQPUp1Gw</p>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Paper Review]]></summary></entry><entry><title type="html">[Papers] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020)</title><link href="https://oweixx.github.io/blog/2025/papers_nerf/" rel="alternate" type="text/html" title="[Papers] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020)"/><published>2025-03-31T00:00:00+00:00</published><updated>2025-03-31T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_nerf</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_nerf/"><![CDATA[<h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h3 id="paper-github-demo"><a href="https://arxiv.org/abs/2003.08934">[Paper]</a> <a href="https://github.com/bmild/nerf">[Github]</a> <a href="https://www.matthewtancik.com/nerf">[Demo]</a></h3> <blockquote> <p><strong>Title:</strong> NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis <br/> <strong>Journal name &amp; Publication year:</strong> ECCV 2020<br/> <strong>First and Last Authors:</strong> Ben Mildenhall <br/> <strong>First Affiliations:</strong> UC Berkeley, Google Research, UC San Diego</p> </blockquote> <hr/> <blockquote> <h2 id="abstract--introduction">Abstract &amp; Introduction</h2> </blockquote> <p>NeRF는 <strong>Novel View Synthesis(NVS)</strong> 계열의 기술이며 입력으로 들어오는 이미지들을 통해 특정 위치에서 해당 물체를 바라보는 synthetic image를 생성하는 기술이다. “for View Synthesis”라는 표현을 새로운 시점의 생성이라는 뜻으로 이해할 수 있다.</p> <p><em><strong>한마디로 지금까지 관측한 이미지들로부터 관측하지 못한 시점에서의 image를 생성하는 기술이다.</strong></em></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/6ebdd725-00bb-44b8-acb6-3f3a23cca597/image.gif" width="900"/></p> <p>NeRF의 PipeLine은 크게 2단계의 과정으로 나눌 수 있다.</p> <ul> <li><strong>Neural Network(MLP)를 통한 3D 공간 특징 추출</strong></li> <li><strong>Volume Rendering을 통한 2D 이미지 생성</strong></li> </ul> <p><img src="https://velog.velcdn.com/images/lowzxx/post/83b84661-d3ad-4f6e-9cf2-12a528795e86/image.png" width="900"/></p> <p>이 외에도 High-Resolution과 High-Frequency를 위한 <strong>Positional Encoding</strong>, <strong>Sampling</strong> 등에 대한 부분은 Optimizing 부분에서 확인해볼 예정이다.</p> <hr/> <blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> </blockquote> <p>가장 먼저 살펴볼 부분은 <strong>밀도 및 색상을 예측 하는 MLP</strong>부분이다. 입력으로는 3D 좌표인 $x = (x, y, z)$와 시점을 나타내는 $d(θ, φ)$ 값이 들어가서 해당 좌표의 RGB 값 $c = (R,G,B)$와 density 값인 $σ$가 출력이 된다.</p> \[FΘ : (x, d) → (c, σ) \quad{} FΘ : (x,y,z,θ, φ) → (R,G,B,σ)\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/f91c69ed-38a8-4649-9bb6-6bdfc0d8ddb7/image.png" width="900"/></p> <p>MLP는 아래와 같이 구성되어 있다. 검은색 화살표는 Linear + ReLU을 거치게 되고 노란색 화살표는 Linear로만 이루어져있으며, 마지막 점선 화살표는 Linear + Sigmoid로 이루어져 있다.중간중간의 <strong>+</strong> 는 Concatenate를 의미한다.</p> <p>처음으로 들어오는 입력값 position x가 3차원이 아닌 60차원으로 들어오게 된다. 이는 Positional Encoding과정을 거치기 때문인데 이는 쉽게 말해서 <strong>3차원 값으로는 표현하지 못하는 영역을 60차원으로 표현하여 디테일을 높여주기 위함</strong>이다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/2b0b8e15-cb5f-436f-9f0f-6214b1f89e96/image.png" width="900"/></p> <blockquote> <p><strong>Positional Encoding의 목적</strong></p> <p><strong>원인</strong> : 일반적으로, NeRF의 MLP는 10개의 층과 뉴런으로 이루어진 단순한 구조이다. 이는 저주파(low-frequency) 정보를 학습하는 데 적합하며, 고주파(high-frequency) 정보를 학습하는 데 한계가 있다.</p> <ul> <li><strong>저주파 정보</strong>: 부드럽고 점진적인 변화 (배경 색상)</li> <li><strong>고주파 정보</strong>: 날카로운 경계나 세부적인 구조 (물체의 윤곽, 텍스쳐 등)</li> </ul> <p>따라서, <strong>단순히 3D 좌표를 입력하면 고주파 정보를 제대로 학습할 수 없고, 결과적으로 부드럽고 디테일이 부족한 장면을 생성하게 된다.</strong></p> <p><strong>그럼 왜 60차원인가?</strong> 3D 좌표의 각 차원을 2L개의 주파수 성분으로 확장(L=10)하며, 이는 총 $3 \times 2L = 60$이 된다.</p> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/812a8098-88a4-460b-b19e-8d5f36a9ab95/image.png" width="900"/></p> <p>그리고 중간 5번째 layer에서 입력으로 들어온 60차원의 좌표값과 똑같은 값이 concatenate되는 부분은 일종의 <strong>skip connection</strong>의 역할로 모델 학습의 안정성과 효율성을 높이기 위함이다.</p> <p>8번째 레이어에서 <strong>밀도(density)</strong> 값이 출력되는데, 이는 해당 좌표를 바라보는 direction 값과는 무관하다는 것을 알 수 있다. <strong>밀도란 개념은 바라보는 시점(viewpoint)에 따라 달라지는 값이 아니라, 특정 좌표 그 자체에서 고정되는 값</strong>이기 때문이다. 따라서 NeRF의 MLP 내부에서 밀도 값은 <strong>Positional Encoding을 통해 확장된 좌표 값</strong>만을 입력으로 받아 계산된다.</p> <p>이후, direction 값 $d$가 concatenate되어 MLP는 해당 좌표와 방향 정보를 조합해 픽셀의 $(R,G,B)$ 값을 출력하게 된다. 이는 색상 값이 시점에 따라 달라지는 <strong>view-dependent</strong> 특성을 학습하기 위해 설계된 과정이다. 예를 들어 빛 반사나 굴절은 시점에 따라 달라질 수 있기 때문이다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/2b0b8e15-cb5f-436f-9f0f-6214b1f89e96/image.png" width="900"/></p> <blockquote> <p><strong>밀도(density)와 direction의 관계</strong></p> <ul> <li>밀도는 특정 좌표가 얼마나 “물질”이 있는지 나타내는 값으로, 시점(view)에 독립적이다. 이는 density field가 3D 공간의 고유한 물리적 특성을 나타낸다고 할 수 있다.</li> <li>반면, 색상 정보는 바라보는 방향에 따라 빛의 반사나 굴절이 달라 질 수 있기 때문에 view-dependent한 특성을 가지게 된다.</li> </ul> <p><strong>오케이 알겠는데 그럼 왜 이렇게 설계 했을까 ?</strong></p> <ul> <li>이는 view-independent한 정보와 view-dependent한 정보를 분리하여 모델이 더 효율적으로 학습할 수 있도록 설계했기 때문이다. 즉, 밀도와 색상 예측 과정을 하나의 MLP내부에서 분리하여 보다 정교한 3D 표현을 학습할 수 있게 설계 한 것이다.</li> </ul> <p><strong>Lambertian effects</strong></p> <ul> <li>논문에서 나오는 표현으로 람베르트 반사라는 용어이다. 이는 관찰자가 바라보는 각도와 관계없이 같은 겉보기 밝기를 갖는 성질을 의미한다.</li> <li>하지만 <strong>NeRF는 direction값을 input으로 사용하기 때문에 각도에 따라 휘도가 달라지는 non-Labertian effects성질을 갖게 되는 것</strong>이다.</li> </ul> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/987984b5-e931-4cdf-93f9-d0f3f7e6b1ad/image.png" width="900"/></p> <hr/> <blockquote> <h2 id="volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</h2> </blockquote> <p>5D를 통해 나온 $c$(color)와 $\sigma$(density)를 통해서 <strong>2D Image를 생성하기 위해 Computer Graphics의 고전적인 방법론인 volume rendering을 사용</strong>한다.</p> <p>수식을 간단하게 살펴보자면 결과 값인 $C(\mathbf{r})$은 <strong>하나의 ray(pixel)에서 기대할 수 있는 Color값(expected color)</strong>을 의미하게 된다.</p> <blockquote> <ul> <li><strong>$t$:</strong> t는 ray의 깊이(depth)를 의미하는 parameter로, <strong>카메라에서 시작된 광선이 3D 공간에서 특정 지점에 도달하기까지의 거리(depth)를 의미</strong>한다. $t_n$은 광선이 시작되는 지점, $t_f$는 광선이 끝나는 지점을 의미한다.</li> <li><strong>$\sigma(\mathbf{r}(t))$:</strong> 해당 시점에서의 density값으로 볼 수 있으며 값이 커질수록 Weight가 커지게 된다.</li> <li><strong>$T(t)$:</strong> <strong>Transmittance(빛의 투과도)</strong>를 의미하며 수식적으로 보았을 때 <strong>density값이 커질 수록 작아진다</strong>는 것을 알 수 있다. 이를 해석해보자면 우리가 보려고 하는 물체 앞에 밀도를 가지는 물체가 있을 때 <strong>우리가 보고자 하는 물체가 가려지게 되는 것</strong>을 수식적으로 표현했다고 볼 수 있다. <strong>pixel은 해당 값이 클 수록 투명하고 작을수록 불투명하게 된다.</strong></li> <li><strong>$c(\mathbf r(t),d)$:</strong> 해당 ray와 시점에 대한 물체의 색을 나타내는 부분이다.</li> </ul> </blockquote> \[C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) c(\mathbf{r}(t), \mathbf{d}) \, dt, \quad \text{where} \quad T(t) = \exp \left( - \int_{t_n}^t \sigma(\mathbf{r}(s)) \, ds \right).\] <p>정리하면, 한 픽셀의 색상은 광선(ray)의 모든 지점에서 <strong>(전달된 투과도) × (밀도) × (색상)</strong> 을 누적하여 적분한 값과 같다. 이 적분은 광선 상의 작은 간격($dt$)에 대해 수행된다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/e795fdd8-aed6-47cb-94e0-067ec9c8791e/image.png" width="900"/></p> <p><strong>continuous(연속적인) 한 적분식을 실제로 프로그래밍에 사용할 수 있게 하기 위해 discrete(이산적인)하게 변환해야 한다.</strong> 그래서 수치적 방법으로 아래와 같이 근사하게 된다. 여기서 사용되는게 <strong>Stratified sampling</strong>으로, 고정된 간격의 샘플링을 하는 것이 아니라 <strong>각 구간에 대해서 무작위 샘플링을 하게 되어 적분의 정확성을 향상</strong> 시키게 되었다고 설명한다. <strong>결론적으로 무작위 샘플링을 통해 적분을 근사하여 연속적인 장면을 표현</strong>할 수 있는 것이다.</p> \[t_i \sim \mathcal{U} \left[ t_n + \frac{i-1}{N}(t_f - t_n), \, t_n + \frac{i}{N}(t_f - t_n) \right]\] <p>그렇게 위에서 샘플링된 $c_i$와 $\sigma_i$값들을 기반으로 $\hat{C}$를 계산하게 된다.</p> <blockquote> <ul> <li>$T_i$: 남아있는 빛의 양 (투과도)</li> <li>$(1-\exp(\sigma_i\delta_i))$: 해당 지점에서 흡수된 빛의 양 (불투명도)</li> <li>$c_i$: 해당 지점의 색상</li> <li>$\hat{C}$: 각 샘플링 지점의 색상 값을 가중합한 결과로 최종 픽셀 색상</li> </ul> </blockquote> \[\hat{C}(\mathbf{r}) = \sum_{i=1}^N T_i \left(1 - \exp(-\sigma_i \delta_i)\right) c_i, \quad \text{where} \quad T_i = \exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)\] <hr/> <blockquote> <h2 id="optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</h2> </blockquote> <h3 id="positional-encoding">Positional encoding</h3> <p>이전에 MLP 부분에서 3차원 좌표를 60차원 입력으로 변환할 때 사용되는 Positional Encoding 기법에 대한 설명이다. 다시 한번 복기하자면 더 높은 고차원으로 표현을 하여 고주파 정보 즉, 물체의 윤곽과 텍스쳐에 대한 detail 정보들을 출력할 수 있게 된다.</p> <blockquote> <ul> <li><strong>저주파 정보</strong>: 부드럽고 점진적인 변화 (배경 색상)</li> <li><strong>고주파 정보</strong>: 날카로운 경계나 세부적인 구조 (물체의 윤곽, 텍스쳐 등)</li> </ul> </blockquote> \[\gamma(p) = \left( \sin(2^0 \pi p), \cos(2^0 \pi p), \cdots, \sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p) \right).\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/88878216-bfa4-4ca6-bbc9-b5ed9147b5a7/image.png" width="900"/></p> <h3 id="hierarchical-volume-sampling">Hierarchical volume sampling</h3> <p>기존의 방식들은 빈 공간(free space) 이나 가려진 영역(occluded regions)과 같이 <strong>렌더링에 기여하지 않는 부분도 반복적으로 샘플링하였기 때문에 매우 비효율적</strong> 이었다.</p> <p>NeRF의 Hierarchical Sampling은 장면의 중요 영역에 샘플링을 집중하여 렌더링 효율과 품질을 높이는 전략이다. Coarse Network와 Fine Network를 동시에 최적화 하게 된다. <strong>Coarse Network에서는 전체적인 이미지</strong>, <strong>Fine Network에서는 중요한 영역</strong>에 대해 집중하게 된다.</p> <blockquote> <ol> <li><strong>Coarse Sampling:</strong> Stratified Sampling을 사용해 고르게 샘플링.</li> <li><strong>PDF 생성:</strong> Coarse Network의 출력을 바탕으로 확률 밀도 함수(PDF)를 생성.</li> <li><strong>Fine Sampling:</strong> PDF를 기반으로 Inverse Transform Samplingdmf 사용하여 중요한 영역에서 추가 샘플링.</li> <li><strong>최종 렌더링:</strong> Coarse와 Fine 샘플을 결합하여 최종 이미지를 생성.</li> </ol> <p>이 과정은 <strong>샘플링 효율성을 극대화</strong>하고, <strong>빈 공간에 낭비되는 계산을 줄이는 동시에 중요한 영역의 디테일을 더 잘 포착</strong>하도록 설계하였다</p> </blockquote> \[\hat{C}_c(\mathbf{r}) = \sum_{i=1}^{N_c} w_i c_i, \quad w_i = T_i \left(1 - \exp(-\sigma_i \delta_i) \right).\] <h3 id="loss">Loss</h3> <p>그렇게 Coarse Network와 Fine Network를 통해 나온 output을 통해 실제 Ground Truth와 L2 Norm을 이용하여 Loss를 간단하게 구성된다.</p> \[L = \sum_{r \in R} \left[ \| \hat{C}_c(r) - C(r) \|_2^2 + \| \hat{C}_f(r) - C(r) \|_2^2 \right]\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/01a206e4-e484-4041-a57b-89906c50886b/image.png" width="900"/></p> <hr/> <blockquote> <h2 id="results">Results</h2> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/0e4d7181-446e-4833-aeb6-b1a9d7cca4d9/image.png" width="900"/></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/1e680de8-5ba0-45b1-906a-0ff78b5530ef/image.png" width="900"/></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/6e158864-22bb-4d00-93fa-5152800da902/image.png" width="900"/></p> <hr/> <h3 id="reference">Reference.</h3> <p><a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a><br/> <a href="https://github.com/bmild/nerf">https://github.com/bmild/nerf</a><br/> <a href="https://csm-kr.tistory.com/64">https://csm-kr.tistory.com/64</a> <br/> <a href="https://jaehoon-daddy.tistory.com/26">https://jaehoon-daddy.tistory.com/26</a><br/> <a href="https://www.youtube.com/watch?v=Mk0y1L8TvKE">https://www.youtube.com/watch?v=Mk0y1L8TvKE</a> <br/> <a href="https://an067.pages.mi.hdm-stuttgart.de/or-jupyterbook/05_NeRF_improvements/05_NeRF_improvements">https://an067.pages.mi.hdm-stuttgart.de/or-jupyterbook/05_NeRF_improvements/05_NeRF_improvements</a></p>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Paper Review]]></summary></entry><entry><title type="html">[Papers] VITON: An Image-based Virtual Try-on Network (IEEE 2018)</title><link href="https://oweixx.github.io/blog/2025/papers_viton/" rel="alternate" type="text/html" title="[Papers] VITON: An Image-based Virtual Try-on Network (IEEE 2018)"/><published>2025-03-31T00:00:00+00:00</published><updated>2025-03-31T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_viton</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_viton/"><![CDATA[<h2 id="viton-an-image-based-virtual-try-on-network-ieee-2018">VITON: An Image-based Virtual Try-on Network (IEEE 2018)</h2> <h3 id="paper-github-demo"><a href="https://arxiv.org/abs/1711.08447">[Paper]</a> <a href="https://github.com/xthan/VITON">[Github]</a> <a href="https://huggingface.co/spaces/Kwai-Kolors/Kolors-Virtual-Try-On">[Demo]</a></h3> <blockquote> <p><strong>Title:</strong> VITON: An Image-based Virtual Try-on Network<br/> <strong>Journal name &amp; Publication year:</strong> Not specified, Arxiv preprint in 2018 <br/> <strong>First and Last Authors:</strong> Xintong Han, Larry S. Davis <br/> <strong>First Affiliations:</strong> University of Maryland, College Park</p> </blockquote> <p>평소 관심있었던 분야이기도 했고 현업에서 상업적으로 다양하게 이미 사용되고 있다는 것도 느꼈었기 때문에 Virtual Try-on이라는 분야에 관심을 가지게 되었다. 해당 논문은 Virtual Try-on 분야의 초기 논문이다.</p> <hr/> <blockquote> <h2 id="abstract--introduction--related-work">Abstract &amp; Introduction &amp; Related Work</h2> </blockquote> <p>지속적으로 수요가 급증하고 있는 온라인 쇼핑 산업에서 점점 더욱 더 간편하게 쉽게 소비를 할 수 있는 환경으로 발전하고 있는 요즘이다. 동시에 소비자들은 의류의 기재된 사이즈나 정보보다 실제 나에게 fit한 느낌을 알고 싶어하고 이것은 결국 불안한 소비로 이어지게 된다. 이를 해결하기 위해 제시하는 VITON은 아이템을 구매하기 전 아이템을 가상으로 착용해보며 <strong>소비자는 가상의 쇼핑 경험</strong>을 하고, <strong>소매업체는 서비스 비용 절감</strong>을 할 수 있게 된다. <img src="https://velog.velcdn.com/images/lowzxx/post/df3a37fd-99b1-48f2-b946-3111e6924505/image.png" alt=""/></p> <p>해당 논문에서는 현실적인 제약이 많고 cost가 높은 3D 정보를 전혀 사용하지 않고 2D RGB 이미지 기반에 의존하는 VITON 모델을 제시한다. VITON 모델은 의류 제품 이미지를 사람의 해당 부위에 자연스럽게 합성되어 Photorealistic한 이미지를 생성하는데 목표를 두고 있다. 해당 목표를 위해서 생성된 이미지는 다음의 조건을 충족해야 한다고 한다.</p> <ul> <li><strong>1. 사람의 신체 부위와 자세가 원본 이미지와 동일해야 한다.</strong></li> <li><strong>2. 목표 의류 아이템은 사람의 자세와 신체 형태에 따라 자연스럽게 변형되어야 한다.</strong></li> <li><strong>3. 원하는 제품의 세부적인 시각적 패턴(색감, 질감, 디테일 등등)이 명확히 드러나야 한다.</strong></li> </ul> <p>위의 조건을 충족하기 위해서는 3D 정보를 활용 하더라도 힘들 것 같은데 2D 이미지로만 구현한다는 부분에서 큰 도전과제인 것 같다.</p> <p>기존의 Virtual Try-on에 주로 사용되는 GAN에서는 목표 의류 아이템의 시각적 디테일을 모두 표현하지 못하고 기하학적 변화에 대한 결과가 좋지 않았는데 이러한 한계점을 해결하기 위해 VITON은 <strong>clothing-agnostic representation</strong>등 다양한 알고리즘들을 사용한다.</p> <hr/> <blockquote> <h2 id="viton">VITON</h2> </blockquote> <p>아래의 Model Architecture를 보았을 때 VITON은 큰 구조로는 두 개의 Network를 가지게 된다. 두 Network는 위에서 제시한 모델의 조건인 1,2번을 목표로 하는 <strong>Multi-task Encoder-decoder Generator</strong>가 있고 이를 통해 1,3번을 해결하기 위한 <strong>Refinement Network</strong>로 나뉘게 된다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/0c19e0f1-6ac0-4500-9df4-205b7fe2155d/image.png" alt=""/></p> <p><br/></p> <h3 id="1-person-representation"><strong>1. Person Representation</strong></h3> <p>VITON의 가장 큰 challenge는 1번과 2번을 충족하는 부분이다. 3D 데이터일 경우 (x,y,z)의 point값과 같은 디테일한 정보를 통해 더욱 realistic한 표현이 가능하겠지만 2D Data만으로는 어려운 표현의 한계가 있기 때문이다. 이를 해결하기 위해 Peron Representation이라는 개념을 제시한다. <img src="https://velog.velcdn.com/images/lowzxx/post/b0f815b0-a44d-47c1-9a33-5790622599e7/image.png" alt=""/></p> <ul> <li><strong>Pose heatmap.</strong> <strong><a href="https://arxiv.org/abs/1611.08050">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields (CVPR 2017)</a></strong> 먼저 제시된 의류를 인물 I에 맞추기 위해 <strong>pose estimation</strong>이 필요한데 이는 위의 논문에서 제시하는 model을 사용한다고 한다. 해당 모델을 사용하면 <strong>인물의 pose를 설명할 수 있는 주요 18개의 key-point가 11x11의 heatmap</strong>으로 제공되어 결론적으로 (18,11,11)의 정보값을 반환하게 된다.</li> </ul> <p>18개의 정확한 위치 정보뿐만이 아니라 주변의 공간적 관계를 나타내는 11x11 heatmap을 통해 2D Image지만으로 표현하는데 한계였었던 공간적인 정보를 제공한다.</p> <ul> <li> <p><strong>Human body representation.</strong> <strong><a href="https://arxiv.org/abs/1703.05446">Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing (CVPR 2017)</a></strong> 위에서 pose를 계산했다면 해당 pose에 의류가 fit하게 맞아야 한다. 이를 위해서 인물의 몸이 있는 위치 정보를 얻어내기 위해 위 논문에서 제시하는 <strong>human parser model</strong>을 사용한다. 이를 통해 1-channel로 구성된 binary mask를 얻고 의류가 충돌하는 등의 artifacts를 피하기 위해 (16x12)로 의도적으로 downsampling을 한다고 한다.</p> </li> <li> <p><strong>Face and hair segment.</strong> <strong><a href="https://arxiv.org/abs/1703.05446">Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing (CVPR 2017)</a></strong> 마지막으로 인물의 식별을 위해서 해당 인물이라고 표현할 수 있는 human parser를 위해 위에서 사용했던 모델을 사용하여 인물의 Face and hair RGB이미지를 구한다.</p> <p>왜 해당 인물이라고 표현하는 정보를 필요로 할까? 조금 찾아보니 이전의 모델들에서는 합성된 이미지에서 인물의 옷만 바뀌어야 하는데 인물의 얼굴, 머리카락 등 인물의 특징들이 변하는 문제가 있었다고 한다. 아마 이를 방지하기 위해 따로 구해놓는게 아닐까 싶다.</p> </li> </ul> <p>위의 세개의 정보를 결합하여 Person Representation을 의미하는 $p$를 만든다. $p \in \mathbb{R}^{m \times n \times k}$ 를 따르고 featuremap의 높이와 너비를 나타내는 m = 256, n = 192, 위에서 구한 정보들을 channel로 두어 18 + 1 + 3 = 22로 $p$는 $(256,192, 22)$의 shape을 가진다.</p> <p><strong>해당 $p$는 인물에 대한 풍부한 정보를 담고있어 보다 정교한 작업이 가능해진다.</strong></p> <p><br/></p> <h3 id="2-multi-task-encoder-decoder-generator"><strong>2. Multi-task Encoder-Decoder Generator</strong></h3> <p><img src="https://velog.velcdn.com/images/lowzxx/post/a25ea934-3870-402a-9b49-eb43f97b7bf3/image.png" alt=""/> U-Net 구조로 이루어진 Encoder-Decoder Generator Network에서는 입력으로 주어지는 인물 정보 p와 의류 c를 통해 <strong>c가 p의 영역으로 자연스럽게 합성되는 것을 목표</strong>로 한다. 또한 뒤의 <strong>Refinement Network에서 사용될 clothing mask</strong>도 같이 추출하게 된다.</p> <p>해당 model의 출력을 근사된 함수로 표현하면 4-channel로 구성된 $G_c(c,p) = (I’,M)$으로 표현되고 앞에 3개의 channel에는 합성된 이미지 $I’$을 의미하고 마지막 channel에는 clothing mask인 $M$을 의미한다. 인물이 보았을 때 이질감이 없이 realistic한 결과물을 위해서는 L1 loss가 아닌 인간의 실제 관측한 값과 가깝게 학습하기 위해 <strong>perceptual loss</strong>를 이용한다. ($M_0$는 human parser에서 예측한 psuedo ground truth clothing mask) 더 자세하게는 coarse image와 Ground truth의 feature map에서의 차이를 줄이기 위해 사용한다.</p> \[L_{G_c} = \sum_{i=0}^{5} \lambda_i \left\| \phi_i(I') - \phi_i(I) \right\|_1 + \left\| M - M_0 \right\|_1,\] <p>수식에서 $\phi_i$는 VGG19 Network에 ImageNet이 사전학습된 모델로 순서대로 ‘conv1_2’, ‘conv2_2’, ‘conv3_2’, ‘conv4_2’, ‘conv5_2’로 각 layer에 feature map을 이용하여 feature map에서의 차이를 줄이는 방향으로 학습이 된다. 다만 i = 0일 경우 RGB 픽셀값의 차이인 L1 loss를 사용한다고 한다.</p> <p>해당 perceptual loss를 최소화하는 학습을 반복함으로써 <strong>의상을 인물에 합성하게 되고 의류의 mask도 얻어내게 된다.</strong> 하지만 조건의 3번인 의류의 디테일을 표현하지 못하는 한계를 가진다.</p> <p><br/></p> <h3 id="3-refinement-network"><strong>3. Refinement Network</strong></h3> <p>Refinement Network는 mask에 Wraping된 의류 $c’$와 $I’$을 이용하여 Encoder-Decoder Network에서 표현하지못한 의상의 디테일 부분을 개선하여 생성하려고 한다.</p> <p><strong>Wrapped clothing item.</strong><br/> <img src="https://velog.velcdn.com/images/lowzxx/post/bb8e4abc-acba-40f6-8d06-bde3497ac30a/image.png" alt=""/> 기존의 의류 c를 Network에 그대로 사용하지 못하기 때문에 기존의 의류 c와 clothing Mask M을 이용하여 Thin Plate Spline(TPS) Transformation을 통해 의류 이미지(c)는 디테일을 유지한채로 인물의 포즈 및 체형에 맞는 $c’$으로 변환된다.</p> <p><strong>Learn to composite.</strong><br/> <img src="https://velog.velcdn.com/images/lowzxx/post/b6ec47e4-c348-4df0-9704-eb439880b13b/image.png" alt=""/></p> <p>위에서 생성한 $I’$과 $c’$을 Refinement Network $G_R$의 입력으로 넣는다. 여기서 $G_R$은 $c’$의 디테일을 활용하여 디테일이 없거나 배경처럼 덜 중요한 영역은 0(검정색), 디테일이 있는 중요한 영역은 1(흰색)로 채워진 <strong>binary composition mask $\alpha$</strong> 를 생성한다. 놀랍게도 지금까지 쓰이지 않았던 <strong>Face and hair segment.</strong> 정보가 여기서 사용된다. 의류부분과 얼굴,머리카락 부분이 겹치는 경우가 많기 때문에 이를 처리하기 위해 이때 자연스럽게 사용되어 의류 부분만 뽑을 수 있게 된다.</p> <p>그다음 $\alpha$와 $I’$을 활용하여 합성한 $\hat I$를 만들어준다. 디테일이 있는 부분은 $c’$을 따라가고 디테일이 없는 부분은 $I’$을 따라가게 만든다.</p> \[\hat I = \alpha \odot c' + (1 - \alpha) \odot I',\] <p>다음으로 원본 이미지 $I$와 synthetic image $\hat I$가 최대한 비슷한 것을 목표로 하는 perceptual loss가 한 번 더 적용이 된다.</p> \[L_{\text{perc}}(\hat{I}, I) = \sum_{i=3}^{5} \lambda_i \left\| \phi_i(\hat{I}) - \phi_i(I) \right\|_1,\] <p>그럼 마지막으로 Refinement Network가 최종 합성 이미지를 더 사실적으로 보이도록 $G_R$을 최적화 해야한다. 여기서 <strong>$L_{G_R}$은 Refinement Network의 총 손실 함수</strong>이다. 두번째 항과 세번째 항은 둘 다 규제 항으로 두번째 항은 $L_1$ 규제항으로 마스크 $\alpha$가 높은 정확도로 구분하는 항이고, 세번째 항은 TV(Total Variation) 규제항으로 이미지의 불연속성을 최소화하며 부드러운 이미지를 위해 추가하는 규제항이라고 한다.</p> <p>결론적으로 음의 <strong>$L_1$ 항을 최소화하면 의류 이미지의 디테일 정보를 더 많이 렌더링</strong> 할 수 있고, <strong>TV 규제항을 최소화하면 더 자연스러운 이미지를 생성</strong>할 수 있는 것이다.</p> \[L_{G_R} = L_{\text{perc}}(\hat{I}, I) - \lambda_{\text{warp}} \left\| \alpha \right\|_1 + \lambda_{\text{TV}} \left\| \nabla \alpha \right\|_1\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/77083bea-92f7-41e1-9290-d1caf4d47107/image.png" alt=""/></p> <p>이로써 VITON모델이 완성이 되었고 model에서 step이 진행되면서 얻어지는 결과물들을 통해 점점 자연스럽게 합성되어가는 과정을 확인할 수 있다.</p> <p>마지막으로 정리를 해보자면 <strong>Encoder-Decoder</strong> 부분에서 의류와 인물에 대한 정보가 주어지면 디테일들을 제외한 인물의 포즈, 체형, 얼굴이 보존된채로 의류가 합성된 결과물을 생성한다.</p> <p>이후 <strong>Refinement</strong> 부분에서 의류를 wraping하고 의류의 디테일 위치의 정보를 담은 Composition Mask가 생성되어 이를 통해 디테일한 부분까지 최종 합성된 이미지가 생성되게 된다.</p> <hr/> <blockquote> <h2 id="experiments">Experiments</h2> </blockquote> <h3 id="1-dataset">1. Dataset</h3> <p>데이터셋은 크롤링을 통해 16253 쌍의 정면을 바라고있는 여성 이미지와 상의 사진을 이용했다고 한다. 이중 87 ~ 88% 정도를 train, 나머지를 test data로 사용하는데 train 데이터는 인물과 의상이 pair하지만 test data에는 검증을 위해 randomly shuffle 되었다고 한다.</p> <p><br/></p> <h3 id="2-implementation-details">2. Implementation Details</h3> <p><strong>Training setup.</strong></p> <ul> <li>Adam optimizer: $\beta_1 = 0.5$, $\beta_2 = 0.999$, $lr = 0.0002$</li> <li>Encoder-decoder: 15K, Refinement: 6K, batchsize = 16</li> <li>synthetic samples size = $256 X 192$</li> </ul> <p><strong>Encoder-decoder generator.</strong></p> <ul> <li>6 convolutional layer</li> <li>Encoding layer consist 4x4 filter, stride of 2, number of filters 64, 128, 256, 512, 512, 512</li> <li>Decoding layer consist 4x4 filter, number of filters 512, 512, 256, 128, 64, 4</li> </ul> <p><strong>Refinement Network</strong></p> <ul> <li>4 fully convolutional model.</li> <li>첫 3개 layer는 3 x 3 x 64 filters and Leaky ReLU</li> <li>마지막 layer는 composition mask를 위해 1 x 1 filter와 sigmoid</li> </ul> <p><br/></p> <h3 id="3-compared-approaches">3. Compared Approaches</h3> <p><br/></p> <h3 id="4-qualitative-results">4. Qualitative Results</h3> <p><strong>Qualitative comparisons of different methods.</strong><br/> 비슷하게 사용될 수 있는 모델들과의 결과물 비교를 해보게 되면 대부분의 모델들이 detail들을 그대로 보존하지 못하고 detail을 보존했을 경우 인물의 pose에 못 따라오는 경우가 대다수이다.</p> <p>하지만 VITON 모델에서 상의 의상만 바뀌어야 하는데 바지도 같이 바뀌는 경우가 생겨버린다. 이 부분은 Face and hair segment. 때와 같이 바지도 따로 추출해준다면 충분히 보존할 수 있다고 한다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/30d588fe-fc40-4573-bb18-3a3246dfd936/image.png" alt=""/></p> <p><strong>Person Representation.</strong><br/> 다음으로 논문에서 중요한 아이디어로 제시된 person representation의 효과를 살펴보자면 pose 정보만으로 봤을 때는 확실히 인물의 pose는 잘 보존된 채로 결과물이 나오는 것을 확인할 수 있다. 다만 body shape mask image 때문에 의상과 인물이 겹쳐있는 경우 충돌로인한 artifact noise가 생기는 걸 확인할 수 있다.<br/> <img src="https://velog.velcdn.com/images/lowzxx/post/28b075e1-6a6f-4366-9fc9-11eccc4dc82f/image.png" alt=""/></p> <p><strong>Failure cases.</strong><br/> 이제부터는 모델의 한계점에 가까운데 아래의 왼쪽 예시와 같이 pose가 너무 가려진다거나 복잡할 경우 잘못된 결과물이 나오고 오른쪽과 같이 인물의 체형과 옷의 shape이 크게 맞지 않을 때 artifact가 생기는 것을 확인할 수 있다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/37064cb6-6d0c-4d84-ad49-fd8d87d35f00/image.png" alt=""/></p> <p><strong>Artifacts near Neck.</strong> <br/> 또다른 문제로는 의류 이미지에서 neck 내부 모습이 들어가 있는 경우 표현되지 말아야 하는 부분이 표현되어 버리는 문제가 생긴다. 이를 해결하기 위해 neck 내부 모습을 제거해주면 해결 되는 것을 확인할 수 있다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/b64007fe-0648-45dc-839c-c28ab1261ad2/image.png" alt=""/></p> <p><br/></p> <h3 id="5-quantitative-results">5. Quantitative Results</h3> <p><strong>Inception Score</strong>는 이미지 생성 모델의 합성 퀄리티를 평가하는 지표로 퀄리티가 높을 수록 높은 점수를 부여 받게 된다. 다만 지금까지 사용했던 Inception Score가 Virtual Try-on의 평가지표로 사용하기에는 적합하지 않다는 결론을 내리게 되었다고 한다.</p> <p>그렇게 <strong>Human evaluation metric</strong>을 따르기로 결정했고 해당 지표로 봤을 때 다른 생성형 모델보다 월등히 성능이 높은 것을 확인할 수 있었다.<br/> <img src="https://velog.velcdn.com/images/lowzxx/post/07719082-8565-45e8-878d-b96868eef008/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="conclusion">Conclusion</h2> </blockquote> <p>결론적으로 전체적으로 비용이 비싼 3D 기반 method대신 2D RGB Image를 이용한 실용적으로 사용될 수 있는 model을 만들었다고 한다.</p> <hr/> <h3 id="reference">Reference.</h3> <p><a href="https://arxiv.org/abs/1711.08447">https://arxiv.org/abs/1711.08447</a><br/> <a href="https://github.com/xthan/VITON">https://github.com/xthan/VITON</a></p>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Paper Review]]></summary></entry><entry><title type="html">랜덤 마라톤 코스(41,42)</title><link href="https://oweixx.github.io/blog/2025/marathon_42_41/" rel="alternate" type="text/html" title="랜덤 마라톤 코스(41,42)"/><published>2025-03-30T00:00:00+00:00</published><updated>2025-03-30T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/marathon_42_41</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/marathon_42_41/"><![CDATA[<p>지금까지 풀지 못했었던 랜덤 마라톤 문제들을 조금씩 풀어보려고 한다.</p> <p>이전에 풀지 못했었던 코스 41,42의 골드 문제 4문제를 풀어보았다.</p> <p>3/30 (일) 카페에서 약 2시간 이상 풀이를 하였다.</p> <p><br/></p> <h4 id="22862-가장-긴-짝수-연속한-부분-수열-large"><a href="https://www.acmicpc.net/problem/22862">22862 가장 긴 짝수 연속한 부분 수열 (large)</a></h4> <p>Gold 5 난이도의 가장 긴 짝수 연속한 부분 수열을 구하는 문제이다.</p> <p>문제를 보았을 때 유형 중 하나인 증가하는 부분 수열 결의 문제라고 생각하고 DP로 접근했다가 실패하고 문제 유형을 통해 <code class="language-plaintext highlighter-rouge">투 포인터</code>유형의 문제라는 것을 알고 투 포인터로 접근하였다.</p> <p>해당 문제를 투포인터로 접근하는 방법을 생각해보자.</p> <p>$s$와 $e$를 어떻게 이용할 것이냐가 투포인터의 정수라고 생각한다. 해당 문제의 성질을 보면 결국 가장 긴 길이를 구해야하는 것이 목표이며 이를 위해선 $e$는 끝까지 진행을 해야하며 $s$는 조건에 맞게 따라오는 Logic으로 구성해야 할 것이다.</p> <p>만약 e가 앞으로 쭉쭉 진행을 하면서 해당 부분이 홀수라면 $cnt$를 1 올려주고 $cnt$가 조건에 맞는 K개 이상이 되었을 경우 해당 조건에 맞을 수 있을 때 까지 s를 증가시켜준다.</p> <p>결국 구해야하는 <strong>가장 긴 짝수 연속한 부분 수열</strong>은 현재 길이 $e-s+1$에서 홀수의 개수 $cnt$를 빼준</p> \[result = (e-s+1-cnt)\] <p>로 정의된다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">sys</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span>

<span class="c1"># 길이가 N인 수열 S
# 짝수로 이루어진 연속한 부분 수열 중 가장 긴 길이
</span>
<span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span><span class="nf">input</span><span class="p">().</span><span class="nf">split</span><span class="p">())</span>
<span class="n">arr</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span><span class="nf">input</span><span class="p">().</span><span class="nf">split</span><span class="p">()))</span>

<span class="n">s</span><span class="p">,</span> <span class="n">e</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span><span class="mi">0</span>
<span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">ml</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">s</span> <span class="o">&lt;=</span> <span class="n">e</span> <span class="ow">and</span> <span class="n">e</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="p">:</span>
    <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span> <span class="p">:</span>
        <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="k">while</span> <span class="n">cnt</span> <span class="o">&gt;</span> <span class="n">k</span> <span class="p">:</span>
        <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span> <span class="p">:</span>
            <span class="n">cnt</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="n">ml</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">ml</span><span class="p">,</span> <span class="n">e</span><span class="o">-</span><span class="n">s</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">cnt</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nf">print</span><span class="p">(</span><span class="n">ml</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><br/></p> <h4 id="2240-자두나무"><a href="https://www.acmicpc.net/problem/2240">2240 자두나무</a></h4> <p>Gold 5난이도의 T초동안 최대 W번 움직이며 받을 수 있는 자두의 최대 개수를 출력하는 문제이다.</p> <p>문제 의도 자체는 DP라는게 명확히 보이는 문제였지만 점화식을 처음에 명확히 파악하는게 어려웠었다.</p> <p>dp를 설정할 때 i,j,k를 다음과 같이 설정하였다.</p> <ul> <li>i : N초</li> <li>j : W번 자리 이동</li> <li>k : 현재 자리 (1 or 2)</li> </ul> <p>만약 현재 자리와 떨어지는 자두의 자리와 같다면 다음 점화식과 같다.</p> \[dp[i][j][k] = dp[i-1][j][k] + 1\] <p>만약 현재 자리와 떨어지는 자두의 자리가 다르다면 점화식은 다음과 같다.</p> \[dp[i][j][k] = max(dp[i-1][j][k], dp[i-1][j-1][1-k] + 1)\] <p>추가로 처음 자두는 1번 자두나무 아래에 위치해 있다는 것을 주의해야 한다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">sys</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span>

<span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span><span class="nf">input</span><span class="p">().</span><span class="nf">split</span><span class="p">())</span>
<span class="n">arr</span> <span class="o">=</span> <span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
<span class="n">cur</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">dp</span> <span class="o">=</span> <span class="p">[[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">w</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>


<span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="p">:</span>
    <span class="n">dp</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">else</span> <span class="p">:</span>
    <span class="n">dp</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="p">:</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">w</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="p">:</span>
            <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">else</span> <span class="p">:</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="p">:</span>
                    <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">],</span> <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="o">-</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span> <span class="p">:</span>
                    <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>

<span class="n">_max</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">:</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">w</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="p">:</span>
            <span class="n">_max</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">_max</span><span class="p">,</span> <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">_max</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><br/></p> <h4 id="5980-corn-maze"><a href="https://www.acmicpc.net/problem/5980">5980 Corn Maze</a></h4> <p>Gold 4 난이도의 Grpah 탐색 문제이다. 딱봐도 BFS Search 문제이지만 주의 할 부분은 Slide라고 하는 경로 이동 Trigger가 추가 되었다는 것이다.</p> <p>전체적으로 기본적인 BFS와 같이 시작점에서 이동할 수 있는 방향으로 이동한다. Slide Trigger를 만날 경우 해당 지점을 방문배열에 Check하고 도착하여 연결된 부분에서는 방문배열을 Check하지 않는다.</p> <p>위와 같은 방법으로 해야하는 경우가 발생하게 되는데, 예를 들어 S -&gt; S’로 이동하고 S’ -&gt; S로도 이동이 가능해야 하는 경우가 있기 때문이다. 해당 반례는 아래와 같다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>6 6
###=##
#.WV##
#K####
#V.W##
#.K.@#
######
</code></pre></div></div> <p>재미있는 문제인 것 같다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">sys</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">deque</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span>

<span class="k">def</span> <span class="nf">bfs</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="p">:</span>
    <span class="n">q</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">()</span>
    <span class="n">q</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">check</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="k">while</span> <span class="n">q</span> <span class="p">:</span>
        <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">cnt</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">popleft</span><span class="p">()</span>
        <span class="c1">#print(x,y)
</span>        <span class="nf">if </span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="o">==</span> <span class="n">end</span> <span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
            <span class="nf">exit</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">dx</span><span class="p">,</span><span class="n">dy</span> <span class="ow">in</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)]</span> <span class="p">:</span>
            <span class="n">nx</span><span class="p">,</span><span class="n">ny</span> <span class="o">=</span> <span class="n">x</span><span class="o">+</span><span class="n">dx</span><span class="p">,</span><span class="n">y</span><span class="o">+</span><span class="n">dy</span>
            <span class="k">if</span> <span class="mi">0</span><span class="o">&lt;=</span><span class="n">nx</span><span class="o">&lt;</span><span class="n">n</span> <span class="ow">and</span> <span class="mi">0</span><span class="o">&lt;=</span><span class="n">ny</span><span class="o">&lt;</span><span class="n">m</span> <span class="ow">and</span> <span class="n">maps</span><span class="p">[</span><span class="n">nx</span><span class="p">][</span><span class="n">ny</span><span class="p">]</span> <span class="o">!=</span> <span class="sh">'</span><span class="s">#</span><span class="sh">'</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">check</span><span class="p">[</span><span class="n">nx</span><span class="p">][</span><span class="n">ny</span><span class="p">]</span> <span class="p">:</span>
                <span class="k">if</span> <span class="n">maps</span><span class="p">[</span><span class="n">nx</span><span class="p">][</span><span class="n">ny</span><span class="p">].</span><span class="nf">isalpha</span><span class="p">()</span> <span class="p">:</span>
                    <span class="n">lst</span> <span class="o">=</span> <span class="n">tp</span><span class="p">[</span><span class="n">maps</span><span class="p">[</span><span class="n">nx</span><span class="p">][</span><span class="n">ny</span><span class="p">]]</span>
                    <span class="n">another</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">px</span><span class="p">,</span><span class="n">py</span> <span class="ow">in</span> <span class="n">lst</span> <span class="p">:</span>
                        <span class="nf">if </span><span class="p">(</span><span class="n">px</span><span class="p">,</span><span class="n">py</span><span class="p">)</span> <span class="o">!=</span> <span class="p">(</span><span class="n">nx</span><span class="p">,</span><span class="n">ny</span><span class="p">)</span> <span class="p">:</span>
                            <span class="n">another</span> <span class="o">=</span> <span class="p">(</span><span class="n">px</span><span class="p">,</span><span class="n">py</span><span class="p">)</span>
                    <span class="n">check</span><span class="p">[</span><span class="n">nx</span><span class="p">][</span><span class="n">ny</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
                    <span class="n">nx</span><span class="p">,</span><span class="n">ny</span> <span class="o">=</span> <span class="n">another</span>
                    <span class="c1">#check[nx][ny] = True
</span>                    <span class="n">q</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">nx</span><span class="p">,</span><span class="n">ny</span><span class="p">,</span> <span class="n">cnt</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

                <span class="k">else</span> <span class="p">:</span>
                    <span class="n">check</span><span class="p">[</span><span class="n">nx</span><span class="p">][</span><span class="n">ny</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
                    <span class="n">q</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">nx</span><span class="p">,</span><span class="n">ny</span><span class="p">,</span><span class="n">cnt</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>


<span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span><span class="nf">input</span><span class="p">().</span><span class="nf">split</span><span class="p">())</span>
<span class="n">maps</span> <span class="o">=</span> <span class="p">[</span><span class="nf">list</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span><span class="nf">input</span><span class="p">().</span><span class="nf">strip</span><span class="p">()))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
<span class="n">check</span> <span class="o">=</span> <span class="p">[[</span><span class="bp">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">m</span><span class="p">)]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>

<span class="n">tp</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">:</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="p">:</span>
        <span class="k">if</span> <span class="n">maps</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">].</span><span class="nf">isalpha</span><span class="p">()</span> <span class="p">:</span>
            <span class="n">tp</span><span class="p">[</span><span class="n">maps</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]].</span><span class="nf">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">maps</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="sh">'</span><span class="s">@</span><span class="sh">'</span> <span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">maps</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="sh">'</span><span class="s">=</span><span class="sh">'</span> <span class="p">:</span>
            <span class="n">end</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span>
<span class="nf">bfs</span><span class="p">(</span><span class="n">start</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">start</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><br/></p> <h4 id="29616-인기-투표"><a href="https://www.acmicpc.net/problem/29616">29616 인기 투표</a></h4> <p>잘 몰겠다.. 어렵다… 접근법이… 어렵다…</p> <p><br/></p>]]></content><author><name></name></author><category term="PS"/><category term="PS"/><summary type="html"><![CDATA[랜덤 마라톤 코스(41,42)]]></summary></entry><entry><title type="html">3월 월간 회고</title><link href="https://oweixx.github.io/blog/2025/monthly_review_2503/" rel="alternate" type="text/html" title="3월 월간 회고"/><published>2025-03-30T00:00:00+00:00</published><updated>2025-03-30T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/monthly_review_2503</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/monthly_review_2503/"><![CDATA[<p>이번 3월 달도 오늘 주말까지 해서 2일 남았다.</p> <p>생각보다 나는 이번 달이 꽤나 길었다고 느껴졌었는데 왜인지는 모르겠다.</p> <p>3월의 목표를 글로 정리한적이 없었기에 이번 달을 정리함과 동시에 다음달의 Self-Motivate를 다시 한 번 상기시키기위해 글로 정리해본다.</p> <hr/> <h3 id="monthly-review"><strong>Monthly Review</strong></h3> <p>먼저 이번달을 간단하게 글로 정리해보고자 한다.</p> <p>이번달의 가장 큰 이벤트라고 한다면 당연히 미루고 미루었던(?) 복학이다. 이번 25-1학기에 3학년에 복학하면서 보통에 비해 2년 늦은 학업을 다니고 있다.</p> <p>사실 2년이 늦건 몇년이 늦건 크게 개의치 않다. 그간 2년동안 경험하고 이루어냈던 것들이 지금의 나를 이루어내는데 지대한 영향을 주었기 때문이다. 어린시절의 영향 70, 이번 2년의 20, 특정하기 어려운 10…. 이렇게 합쳐져 100의 내가 되었다고 생각한다.</p> <p>하여간, 정리해서 이번달을 Keyword로 살펴보도록 하자.</p> <h4 id="학점gpa"><strong>학점(GPA)</strong></h4> <p>이번 복학을 하면서 가장 크게 신경쓰는 것 중 하나로 대학원을 들어가기 위한 좋은 학점 챙기기 였다. 나에게 학점이라하면…. 사실은 지금 하던대로 하면 충분히 잘 챙길 수 있지 않을까라는 안일한 생각으로 정의가 되어있다. 비교적 널널하기도 했었던 1,2 학년을 생각해보면 지금만큼 공부를 하지 않았음에도 ‘왜 이렇게 학점이 나오지?…‘라고 할 정도로 꽤나 순방했었기 때문이다.</p> <p>하지만 이번 학기 본전공 5개 타전공(수학과) 1개해서 전공수업 6개를 수업을 들어보니 ‘아 이거 안일한 정신상태로 공부하면 큰일나겠다.’라는 경각심이 순간 들었었다. 현재 위기감을 느끼고 있는 수업으로는 운영체제, AI 기초수학, 컴퓨터비전 이렇게 3개이다. 사실 어려워서 경각심이 들기보다는 흥미가 별로 없기 때문에 자연스럽게 소훌해지면서 나중에 경각심이 든 친구들이다.</p> <p>현재 학점이 막 나쁜편은 아니지만 사실 내 기준에서는 크게 만족스럽지 않은 학점과 석차이다. 이전과 다르게 목표의식이 있기 때문에 지금처럼 열심히 하면 좋은 학점을 받을 수 있지 않을까 싶다. 그냥 다시 한 번 스스로에게 말하고 싶은 말은 <code class="language-plaintext highlighter-rouge">안일하지 말자.</code> 이다.</p> <h4 id="algorithm"><strong>Algorithm</strong></h4> <p>학교 수업이 크게 재미있지 않아서 그런지 초반에 꽤나 빠르게 열심히 알고리즘을 달렸다.</p> <p>solved.ac 기준으로 3월 초 1843점으로 P3 중반이었지만 현재는 2112로 P1 하위 구간에 도달하였다. 사실 최대유량(Network Flow)유형 문제가 너무 재밌어서 관련 문제들을 너무 재밌게 풀다보니 여기에 도달한 것 같다.(Graph가 진짜 재밌다…..)</p> <p>현재로써 조금 느끼는 부분은 이전만큼 PS에 시간을 투자하기는 어렵지 않을까 라는 생각이다. 기존 목표였던 대학원을 목표로 두고있는 만큼 연구적인 부분에서의 활동을 더 해야하지 않을까 라는 생각이 최근에서 조금씩 들기 시작했다. 현실을 직시했달까?…</p> <p>그래서 현재는 방향성을 점수를 올리기보다는 골드 수준의 문제들을 꾸준히 풀면서 전체적인 알고리즘의 감을 높이려고 한다. 주 컨텐츠로는 solved.ac의 마라톤을 할 것 같고 추가적으로 수학적인 문제에 손을 대지 않을까 라는 생각이다. 더 자세한 계획으로는 이후에 작성해야겠다.</p> <h4 id="college-life"><strong>College Life</strong></h4> <p>종합적인 학교 생활이다. 사실 지금 매우 개인적인 학교 생활을 보내고 있기 때문에 너무나도 편하고 자유롭다고 총평 할 수 있다. 내가 이상적으로 생각했던 학교 생활이다.</p> <p>다만 아쉬운건, 비슷한 마인드를 가진 친구가 한명 있으면 좋지 않을까 라는 부분이었는데 아직 많은 사람들을 만나보지 않았기도 하고 이런 부분을 알기 위해서는 deep하게 들어가야 하기 때문에 아직 발견하지 못해서 아쉬운 것 같다.</p> <p>큰 이변이 없다면 현재와 같은 Routine으로 학교를 쭉 다닐 것 같다. 사실 Routine에 짜잘한 변수들에 큰 스트레스를 받는 나로써 지금 굉장히 만족스럽기 때문에 이상한 사건이나 사람이 안 꼬였으면 좋겠다는 바람 뿐이다. 물론 그게 나일수도 있긴하다….</p> <p><br/></p> <hr/> <h2 id="future"><strong>Future</strong></h2> <h3 id="goal"><strong>Goal</strong></h3> <ul> <li>4월은 중간고사가 있는 달이다. 0순위로 학점을 잘 받아야 한다.</li> <li>4월부터 시험체제 Routine을 꾸리고 행동하자.</li> <li>운영체제, 컴퓨터비전, AI 기초수학에 시간을 조금 할애해보자.</li> <li>학부연구생 참여를 해보자.</li> <li>주 1논문 읽고 Review하는 Routine을 추가하자.</li> </ul> <h3 id="try"><strong>Try</strong></h3> <ul> <li>학부연구생 contac 꾸준히 하기.</li> <li>틈틈이 좋은 인재(?)를 탐색해보자.</li> <li>담당 지도교수님께 연구 활동 의지에 대한 어필을 해보자.</li> <li>과제는 최대한 빠르게 쳐내고 해결하는 과정을 오래가져가기.</li> </ul> <h3 id="mindset"><strong>Mindset</strong></h3> <ul> <li>오늘 느꼈음. 잃을 것 없는 사람마냥 생각해야 그제서야 용감하게 행동한다.</li> <li>실패를 두려워하지말자.</li> <li>문제건 과제건 어떤 어려움이건 내 힘으로 온전히 해결하는 과정을 소중히 여겨보자.</li> <li>스스로에게 의지하는 습관을 들여보자(?)</li> <li>항상 안일하지말고, 어제보다 더 열심히 사는 것을 오늘 하루의 목표로 두자.</li> </ul> <h3 id="conclusion"><strong>Conclusion</strong></h3> <p>이번 달을 정리해보니 꽤나 열심히 살았던 것 같다.</p> <p>다음달도 늘 그렇듯이 잘해보자.</p>]]></content><author><name></name></author><category term="회고"/><category term="회고"/><summary type="html"><![CDATA[월간 회고]]></summary></entry><entry><title type="html">랜덤 마라톤 코스(43)</title><link href="https://oweixx.github.io/blog/2025/marathon_43/" rel="alternate" type="text/html" title="랜덤 마라톤 코스(43)"/><published>2025-03-29T00:00:00+00:00</published><updated>2025-03-29T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/marathon_43</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/marathon_43/"><![CDATA[<p>이번 주 solved.ac 랜덤 마라톤 문제들의 간단한 풀이이다. 25.03.29 카페에서 본공부 전 간단하게 진행하였다.</p> <p><br/></p> <h4 id="9947-coin-tossing">9947 Coin tossing</h4> <p>Bronze 2 난이도의 문제로 매우 간단한 문제였다.</p> <p>입력으로 들어오는 n개의 H,T에 대하여 같은 경우 앞 사람이 점수를 먹고, 다르면 뒷 사람이 점수를 먹는 형식이다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">sys</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span>

<span class="k">while</span> <span class="bp">True</span> <span class="p">:</span>
    <span class="n">_in</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span><span class="nf">input</span><span class="p">().</span><span class="nf">split</span><span class="p">()))</span>
    <span class="k">if</span> <span class="n">_in</span> <span class="o">==</span> <span class="p">[</span><span class="sh">'</span><span class="s">#</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">#</span><span class="sh">'</span><span class="p">]:</span>
        <span class="nf">exit</span><span class="p">()</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span>
    <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">:</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span><span class="nf">input</span><span class="p">().</span><span class="nf">split</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">b</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span> <span class="p">:</span>
            <span class="n">y</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">_in</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">_in</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><br/></p> <h4 id="31738-매우-어려운-문제">31738 매우 어려운 문제</h4> <p>정수 N,M에 대하여 N! % M을 구하는 문제이다.</p> <p>처음에 정말 단순히 <code class="language-plaintext highlighter-rouge">math.factorial(n) % m</code>을 통해 풀었다가 너무 당연하게도 시간초과를 받았다.</p> <p>문제의 제한을 보면 N이 $10^{18}$, M이 $10^{17}$인 것을 확인할 수 있다. 간단한 DP를 통해 나머지를 관리해주자.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">sys</span>
<span class="kn">import</span> <span class="n">math</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span>

<span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span><span class="nf">input</span><span class="p">().</span><span class="nf">split</span><span class="p">())</span>
<span class="k">if</span> <span class="n">n</span> <span class="o">&gt;=</span> <span class="n">m</span> <span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="nf">exit</span><span class="p">()</span>
<span class="k">else</span> <span class="p">:</span>
    <span class="n">dp</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="p">:</span>
        <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span><span class="o">%</span><span class="n">m</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">dp</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><br/></p> <h4 id="5078-shirts">5078 Shirts</h4> <p>셔츠들을 입력을 받아서 사이즈, 색깔 별로 정렬하여 출력하는 간단한 문제였다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">sys</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span>

<span class="k">while</span> <span class="bp">True</span> <span class="p">:</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">n</span> <span class="p">:</span>
        <span class="c1"># work
</span>        <span class="nf">exit</span><span class="p">()</span>
    
    <span class="n">shirts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="nf">input</span><span class="p">().</span><span class="nf">strip</span><span class="p">()</span>
        <span class="n">shirts</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="nf">input</span><span class="p">().</span><span class="nf">strip</span><span class="p">()</span>
        <span class="n">shirts</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">shirts</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="nf">ord</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nf">ord</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
    <span class="nf">print</span><span class="p">(</span><span class="o">*</span><span class="n">shirts</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><br/></p> <h4 id="1448-삼각형-만들기">1448 삼각형 만들기</h4> <p>괜찮은 함정(?) 문제 였다.</p> <p>괜찮다고 생각했던 부분중 하나는 <code class="language-plaintext highlighter-rouge">세 변의 길이의 합이 최댓값을 구하고 싶다.</code> 라는 부분과 삼각형을 이룰 수 있는 성질이 합쳐지면 매우 Greedy한 문제로 변한다는 부분이었다.</p> <p>결국</p> \[a + b &gt; c, a + c &gt; b, b + c &gt; a\] <p>를 만족해야 하는 것이며 단순히 정렬하여 맨 위 index부터 연속된 3개의 숫자가 삼각형을 이룰 수 있는 조건인지 찾으면 되는 매우 간단한 문제로 변한다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">sys</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span>

<span class="n">n</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span>
<span class="n">arr</span> <span class="o">=</span> <span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
<span class="n">arr</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="p">:</span>
    <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">&gt;</span> <span class="n">c</span> <span class="ow">and</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">&gt;</span> <span class="n">a</span> <span class="ow">and</span> <span class="n">a</span> <span class="o">+</span> <span class="n">c</span> <span class="o">&gt;</span> <span class="n">b</span> <span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">+</span><span class="n">c</span><span class="p">)</span>
        <span class="nf">exit</span><span class="p">()</span>
    <span class="k">else</span> <span class="p">:</span>
        <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nf">print</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><br/></p> <h4 id="18126-너구리-구구">18126 너구리 구구</h4> <p>무려 7번이나 틀린 문제이다…</p> <p>총 1부터 N개로 N개의 방으로 이루어진 곳에서 N-1개의 길로 서로 오갈 수 있다고 한다. 이 때 입구에서 가장 먼 방에 아이스크림을 숨기려고 한다.</p> <p>당연히 가장 멀다길래 N번방 까지의 거리일 줄 알았지만 길들의 거리를 모두 계산하고 1번부터 가장 멀리 떨어진 방까지의 거리를 출력하는 것이 문제였다.</p> <p>왜 틀렸지? 라는 생각이 든다면 문제를 다시 한 번 확인해보자.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">sys</span><span class="p">,</span> <span class="n">heapq</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span>

<span class="k">def</span> <span class="nf">bfs</span><span class="p">(</span><span class="n">start</span><span class="p">)</span> <span class="p">:</span>
    <span class="n">heap</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">heapq</span><span class="p">.</span><span class="nf">heappush</span><span class="p">(</span><span class="n">heap</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">start</span><span class="p">))</span>

    <span class="k">while</span> <span class="n">heap</span> <span class="p">:</span>
        <span class="n">cost</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">heapq</span><span class="p">.</span><span class="nf">heappop</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">cost</span> <span class="o">&gt;</span> <span class="n">dist</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="p">:</span>
            <span class="k">continue</span>

        <span class="k">for</span> <span class="n">nx</span><span class="p">,</span> <span class="n">ncost</span> <span class="ow">in</span> <span class="n">edge</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="p">:</span>
            <span class="n">next_cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">+</span> <span class="n">ncost</span>
            <span class="k">if</span> <span class="n">dist</span><span class="p">[</span><span class="n">nx</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">next_cost</span> <span class="p">:</span>
                <span class="n">dist</span><span class="p">[</span><span class="n">nx</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_cost</span>
                <span class="n">heapq</span><span class="p">.</span><span class="nf">heappush</span><span class="p">(</span><span class="n">heap</span><span class="p">,</span> <span class="p">(</span><span class="n">next_cost</span><span class="p">,</span> <span class="n">nx</span><span class="p">))</span>
<span class="n">edge</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
<span class="n">dist</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e20</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
<span class="n">dist</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">dist</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">:</span>
    <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span><span class="nf">input</span><span class="p">().</span><span class="nf">split</span><span class="p">())</span>
    <span class="n">edge</span><span class="p">[</span><span class="n">a</span><span class="p">].</span><span class="nf">append</span><span class="p">((</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">))</span>
    <span class="n">edge</span><span class="p">[</span><span class="n">b</span><span class="p">].</span><span class="nf">append</span><span class="p">((</span><span class="n">a</span><span class="p">,</span><span class="n">c</span><span class="p">))</span>
<span class="nf">bfs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="n">dist</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><br/></p> <h4 id="2784-가로-세로-퍼즐">2784 가로 세로 퍼즐</h4> <p>실버2 같은 느낌은 아니었지만 이번에도 꽤 괜찮은 문제였다.</p> <p>6개의 단어가 주어지면 해당 6개의 단어로 $3X3$ 가로 세로 퍼즐을 만들 수 있냐라는 문제이다.</p> <p>입력으로 주어지는 단어 수가 6개로 고정되어 있고 6개중 3개로 단어의 경우의 수를 뽑는 것은 시간적으로 매우 여유있기 때문에 permutations을 이용해 뽑아주었다.</p> <p>괜찮다고 생각했던 부분은 그냥 재밌었기 때문이다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">sys</span>
<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">permutations</span> <span class="k">as</span> <span class="n">pm</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span>

<span class="n">word</span> <span class="o">=</span> <span class="p">[</span><span class="nf">input</span><span class="p">().</span><span class="nf">rstrip</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)]</span>
<span class="n">lst</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">pm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">lst</span> <span class="p">:</span>
    <span class="n">another</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">org</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">i</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">j</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">i</span> <span class="p">:</span>
            <span class="n">another</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
    <span class="n">vertical</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="p">:</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="sh">''</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="p">:</span>
            <span class="n">tmp</span> <span class="o">+=</span> <span class="n">org</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
        <span class="n">vertical</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
    <span class="n">vertical</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span>
    <span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="p">:</span>
        <span class="k">if</span> <span class="n">vertical</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">another</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="p">:</span>
            <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">cnt</span> <span class="o">==</span> <span class="mi">3</span> <span class="p">:</span>
        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">i</span> <span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">row</span><span class="p">])</span>
        <span class="nf">exit</span><span class="p">()</span>
    
<span class="nf">print</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><br/></p> <h4 id="14426-접두사-찾기">14426 접두사 찾기</h4> <p>실버1 트라이 문제이지만 단순히 접두사로 올 수 있는 모든 경우의 수를 set형에 넣어두고 check하는 것으로도 풀이가 가능하였다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">sys</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span>

<span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span><span class="nf">input</span><span class="p">().</span><span class="nf">split</span><span class="p">())</span>

<span class="n">c</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">input</span><span class="p">().</span><span class="nf">rstrip</span><span class="p">()</span>
    <span class="n">t</span> <span class="o">=</span> <span class="sh">''</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">s</span> <span class="p">:</span>
        <span class="n">t</span> <span class="o">+=</span> <span class="n">j</span>
        <span class="n">c</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">input</span><span class="p">().</span><span class="nf">rstrip</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">c</span> <span class="p">:</span>
        <span class="n">a</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><br/></p> <h4 id="25343-최장-최장-증가-부분-수열">25343 최장 최장 증가 부분 수열</h4> <p>Gold 5 난이도의 LIS(Longest Increasing Subsequence) 문제이다.</p> <p>최장 부분 수열의 문제를 2차원 version인데 다행히도 난이도를 낮추기 위한 최단거리라는 제한이 주어져 있다. (사실 최단거리가 아니면 어떻게 될지 난 모르긴함.)</p> <p>기본 LIS의 틀인 DP를 통해 이전의 값들 보다 크다면 해당 DP를 업데이트 하는 방식 이었다. 점화식으로 보자면</p> \[dp[i] = max(dp[i], dp[j] + 1) \quad if \quad (arr[i] &gt; arr[j] \quad and \quad i &gt; j)\] <p>약간 헷갈릴 수 있는 부분으로 원래 1차원 LIS의 공식은 $i = range(1,n)$, $j = range(i)$ 였었다. 하지만 2차원 LIS에서 바뀐 이유는 같은 열 같은 행이면서 이전의 값이 있을 수 있기 때문에 범위를 아래 코드와 같이 수정해줘야 한다는 부분이다. 이부분은 재미있으면서도 좋은 부분인 것 같다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">sys</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span>

<span class="n">n</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">input</span><span class="p">())</span>
<span class="n">arr</span> <span class="o">=</span> <span class="p">[</span><span class="nf">list</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span><span class="nf">input</span><span class="p">().</span><span class="nf">split</span><span class="p">()))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>

<span class="n">dp</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">:</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="p">:</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="p">:</span>
                <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">arr</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">t</span><span class="p">]</span> <span class="p">:</span>
                    <span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">dp</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">ans</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">:</span>
    <span class="n">ans</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">ans</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">dp</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="PS"/><category term="PS"/><summary type="html"><![CDATA[랜덤 마라톤 코스(43)]]></summary></entry><entry><title type="html">뤼카의 정리</title><link href="https://oweixx.github.io/blog/2025/Lucas_theorem/" rel="alternate" type="text/html" title="뤼카의 정리"/><published>2025-03-27T00:00:00+00:00</published><updated>2025-03-27T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/Lucas_theorem</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/Lucas_theorem/"><![CDATA[<p>더 어려운 수학 category를 들어가기에 앞서 뤼카의 정리를 한 번 살펴보고자 한다.</p> <p>뤼카의 정리는 어떤 조합의 수를 소수 p에 대해 법 p 상에서 구할 때 간편한 계산 방식을 제공한다. 다시 말해 <code class="language-plaintext highlighter-rouge">작은 값들의 조합을 통해 해를 빠르게 계산</code>할 수 있다.</p> <p><br/></p> <h4 id="공식화">공식화</h4> <p>임의의 음이 아닌 정수 m과 n, 소수 p에 대하여 뤼카의 정리는 다음과 같이 합동식으로 표현할 수 있다.</p> \[\binom {m}{n} \equiv \prod_{i=0}^{k}{\binom{m_i}{n_i} \pmod p}\] <p>여기서 첨자가 붙은 수들은 m과 n을 소수 p에 대해 다음과 같이 p진법으로 전개했을 때 얻어지는 것들이다.</p> <ol> <li>$m=m_kp^k+m_{k-1}p^{k-1}+\cdots+m_1p+m_0,$</li> <li>$n=n_kp^k+n_{k-1}p^{k-1}+\cdots+n_1p+n_0$</li> </ol> <p>이상과 같은 뤼카의 정리는 임의의 자연수 q에 대해 법 p의 q제곱 형태로 일반화가 가능하다.</p> <p><br/></p> <h4 id="증명">증명</h4> <h5 id="1-다항식-증명">1. 다항식 증명</h5> \[(1+x)^p\] <p>해당 식에 대하여</p> \[(1+x)^p = \binom p 0 + \binom p 1 x + \binom p 2 x^2 + \cdots + \binom p p x^p\] <p>위의 식으로 표현이 가능하고 이는 $\binom p 1$부터 $\binom p {p-1}$까지 모두 p를 인수로 가진다는 것을 알 수 있다. 이는 $\pmod p$연산을 적용하면 초항과 마지막 항을 제외하고는 모든 항이 제거된다.</p> <p>정리하면</p> \[(1+x)^p \equiv 1 + x^p \pmod p\] <p>위 합동으로 해당 명제가 성립한다.</p> \[(1+x)^{p^{n+1}} = ((1+x)^p)^{p^n} \equiv 1 + x^p \pmod p\] <p>위 식 역시 귀납적으로 정의 될 수 있으며 해당 명제가 성립한다.</p> <p><br/></p> <h5 id="2-뤼카의-정리-증명">2. 뤼카의 정리 증명</h5> <p>이를 이용해서 다음과 같이 전개가 가능하다.</p> \[\sum_{n=0}^{m} \binom m n x^n \equiv (1+x)^m \equiv \prod_{i=0}^{k} \left[(1+x)^{p^i} \right]^{m_i} \equiv \prod_{i=0}^{k} \left[ 1+x^{p^i} \right]^{m_i} \pmod p\] <p>다시 이항 정리를 써서 안쪽의 식을 풀어내면,</p> \[\equiv \prod_{i=0}^k \left[ \sum_{{n_i}=0}^{m_i} \binom {m_i} {n_i} {x^{n_ip^i}} \right] \equiv \sum_{n=0}^m \left[ \prod_{i=0}^k \binom {m_i}{n_i} \right] x^n \pmod p\] <p>이 된다. 모든 차수마다 계수는 같으므로 위 뤼카의 정리가 성립하게 된다.</p>]]></content><author><name></name></author><category term="theorem"/><category term="theorem"/><summary type="html"><![CDATA[뤼카의 정리]]></summary></entry></feed>