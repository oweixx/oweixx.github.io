<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://oweixx.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://oweixx.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-21T05:32:38+00:00</updated><id>https://oweixx.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">[Papers] One-Step Image Translation with Text-to-Image Models (Preprint)</title><link href="https://oweixx.github.io/blog/2025/papers_img2img_turbo/" rel="alternate" type="text/html" title="[Papers] One-Step Image Translation with Text-to-Image Models (Preprint)"/><published>2025-11-20T00:00:00+00:00</published><updated>2025-11-20T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_img2img_turbo</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_img2img_turbo/"><![CDATA[<h2 id="one-step-image-translation-with-text-to-image-models">One-Step Image Translation with Text-to-Image Models</h2> <h3 id="papergithub"><a href="https://arxiv.org/abs/2403.12036">[Paper]</a><a href="https://github.com/GaParmar/img2img-turbo">[Github]</a></h3> <blockquote> <p><strong>Title:</strong> One-Step Image Translation with Text-to-Image Models <br/> <strong>Journal name &amp; Publication Date:</strong> Preprint 2024-03-18<br/> <strong>Affiliation:</strong> Carnegie Mellon University, Adobe Research</p> </blockquote> <hr/> <blockquote> <h2 id="0-paired-image-translation-pix2pix-turbo">0. Paired Image Translation (pix2pix-turbo)</h2> </blockquote> <p>Paired Image Translation은 애초에 train과 target 쌍이 있는 task의 translation을 의미한다.</p> <p>image의 edge와 prompt를 주면 해당 pormpt에 맞게 egde의 structure를 따라 image가 생성이 되는 형식이다.</p> <p>sketch to image도 비슷하게 “찰떡같이 알아듣는다.”를 모델로 표현한 느낌이다. input으로 들어오는 입력으로 들어오는 sketch image에 대하여 prompt로 직접적으로 바로 표현해준다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre><span class="c1"># edge to image
</span><span class="n">python</span> <span class="n">src</span><span class="o">/</span><span class="n">inference_paired</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_name</span> <span class="sh">"</span><span class="s">edge_to_image</span><span class="sh">"</span> \
    <span class="o">--</span><span class="n">input_image</span> <span class="sh">"</span><span class="s">assets/examples/bird.png</span><span class="sh">"</span> \
    <span class="o">--</span><span class="n">prompt</span> <span class="sh">"</span><span class="s">a blue bird</span><span class="sh">"</span> \
    <span class="o">--</span><span class="n">output_dir</span> <span class="sh">"</span><span class="s">outputs</span><span class="sh">"</span>

<span class="c1"># sketch to image
</span><span class="n">python</span> <span class="n">src</span><span class="o">/</span><span class="n">inference_paired</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_name</span> <span class="sh">"</span><span class="s">sketch_to_image_stochastic</span><span class="sh">"</span> \
<span class="o">--</span><span class="n">input_image</span> <span class="sh">"</span><span class="s">assets/examples/sketch_input.png</span><span class="sh">"</span> <span class="o">--</span><span class="n">gamma</span> <span class="mf">0.4</span> \
<span class="o">--</span><span class="n">prompt</span> <span class="sh">"</span><span class="s">ethereal fantasy concept art of an asteroid. magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy</span><span class="sh">"</span> \
<span class="o">--</span><span class="n">output_dir</span> <span class="sh">"</span><span class="s">outputs</span><span class="sh">"</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>먼저 공통적으로 사용하는 <code class="language-plaintext highlighter-rouge">Pix2Pix_Turbo</code>Model을 확인해보자.</p> <p>init을 살펴보면, 공통적으로 Condition image에 대한 vae encoder, decoder module들이 있는 것이 보이고, skip connection conv와 unet이 초기의 <code class="language-plaintext highlighter-rouge">sd-turbo</code> 깡통으로 초기화 되어있는 것 같다.</p> <p>이후에 두 task에 맞게 달라지는 부분은 각 task에 맞게 학습이 된 sd-turbo unet구조를 가중치를 불러와서 갖다 붙이는 부분이다. 그 외에 둘의 차이점이 있는 부분은 없다. 공통적으로 task에 맞게 model을 불러온 뒤 <code class="language-plaintext highlighter-rouge">LoRA Adapter</code>형태로 vae.add_adapter하는 부분이 보인다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">Pix2Pix_Turbo</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">pretrained_name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">pretrained_path</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">ckpt_folder</span><span class="o">=</span><span class="sh">"</span><span class="s">checkpoints</span><span class="sh">"</span><span class="p">,</span> <span class="n">lora_rank_unet</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">lora_rank_vae</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">stabilityai/sd-turbo</span><span class="sh">"</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">tokenizer</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">text_encoder</span> <span class="o">=</span> <span class="n">CLIPTextModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">stabilityai/sd-turbo</span><span class="sh">"</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">text_encoder</span><span class="sh">"</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sched</span> <span class="o">=</span> <span class="nf">make_1step_sched</span><span class="p">()</span>

        <span class="n">vae</span> <span class="o">=</span> <span class="n">AutoencoderKL</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">stabilityai/sd-turbo</span><span class="sh">"</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">vae</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">vae</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">my_vae_encoder_fwd</span><span class="p">.</span><span class="nf">__get__</span><span class="p">(</span><span class="n">vae</span><span class="p">.</span><span class="n">encoder</span><span class="p">,</span> <span class="n">vae</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">__class__</span><span class="p">)</span>
        <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">my_vae_decoder_fwd</span><span class="p">.</span><span class="nf">__get__</span><span class="p">(</span><span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">,</span> <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">__class__</span><span class="p">)</span>
        <span class="c1"># add the skip connection convs
</span>        <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">skip_conv_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
        <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">skip_conv_2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
        <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">skip_conv_3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
        <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">skip_conv_4</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
        <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">ignore_skip</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">unet</span> <span class="o">=</span> <span class="n">UNet2DConditionModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">stabilityai/sd-turbo</span><span class="sh">"</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">unet</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">pretrained_name</span> <span class="o">==</span> <span class="sh">"</span><span class="s">edge_to_image</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://www.cs.cmu.edu/~img2img-turbo/models/edge_to_image_loras.pkl</span><span class="sh">"</span>
            <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">ckpt_folder</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">outf</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">ckpt_folder</span><span class="p">,</span> <span class="sh">"</span><span class="s">edge_to_image_loras.pkl</span><span class="sh">"</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">outf</span><span class="p">):</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Downloading checkpoint to </span><span class="si">{</span><span class="n">outf</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                <span class="n">total_size_in_bytes</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">headers</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">content-length</span><span class="sh">'</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
                <span class="n">block_size</span> <span class="o">=</span> <span class="mi">1024</span>  <span class="c1"># 1 Kibibyte
</span>                <span class="n">progress_bar</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_size_in_bytes</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="sh">'</span><span class="s">iB</span><span class="sh">'</span><span class="p">,</span> <span class="n">unit_scale</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">outf</span><span class="p">,</span> <span class="sh">'</span><span class="s">wb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="nf">iter_content</span><span class="p">(</span><span class="n">block_size</span><span class="p">):</span>
                        <span class="n">progress_bar</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
                        <span class="nb">file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="n">progress_bar</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">total_size_in_bytes</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">progress_bar</span><span class="p">.</span><span class="n">n</span> <span class="o">!=</span> <span class="n">total_size_in_bytes</span><span class="p">:</span>
                    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">ERROR, something went wrong</span><span class="sh">"</span><span class="p">)</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Downloaded successfully to </span><span class="si">{</span><span class="n">outf</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">p_ckpt</span> <span class="o">=</span> <span class="n">outf</span>
            <span class="n">sd</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">p_ckpt</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">unet_lora_config</span> <span class="o">=</span> <span class="nc">LoraConfig</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">sd</span><span class="p">[</span><span class="sh">"</span><span class="s">rank_unet</span><span class="sh">"</span><span class="p">],</span> <span class="n">init_lora_weights</span><span class="o">=</span><span class="sh">"</span><span class="s">gaussian</span><span class="sh">"</span><span class="p">,</span> <span class="n">target_modules</span><span class="o">=</span><span class="n">sd</span><span class="p">[</span><span class="sh">"</span><span class="s">unet_lora_target_modules</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">vae_lora_config</span> <span class="o">=</span> <span class="nc">LoraConfig</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">sd</span><span class="p">[</span><span class="sh">"</span><span class="s">rank_vae</span><span class="sh">"</span><span class="p">],</span> <span class="n">init_lora_weights</span><span class="o">=</span><span class="sh">"</span><span class="s">gaussian</span><span class="sh">"</span><span class="p">,</span> <span class="n">target_modules</span><span class="o">=</span><span class="n">sd</span><span class="p">[</span><span class="sh">"</span><span class="s">vae_lora_target_modules</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">vae</span><span class="p">.</span><span class="nf">add_adapter</span><span class="p">(</span><span class="n">vae_lora_config</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="sh">"</span><span class="s">vae_skip</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">_sd_vae</span> <span class="o">=</span> <span class="n">vae</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sd</span><span class="p">[</span><span class="sh">"</span><span class="s">state_dict_vae</span><span class="sh">"</span><span class="p">]:</span>
                <span class="n">_sd_vae</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="sh">"</span><span class="s">state_dict_vae</span><span class="sh">"</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
            <span class="n">vae</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">_sd_vae</span><span class="p">)</span>
            <span class="n">unet</span><span class="p">.</span><span class="nf">add_adapter</span><span class="p">(</span><span class="n">unet_lora_config</span><span class="p">)</span>
            <span class="n">_sd_unet</span> <span class="o">=</span> <span class="n">unet</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sd</span><span class="p">[</span><span class="sh">"</span><span class="s">state_dict_unet</span><span class="sh">"</span><span class="p">]:</span>
                <span class="n">_sd_unet</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="sh">"</span><span class="s">state_dict_unet</span><span class="sh">"</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
            <span class="n">unet</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">_sd_unet</span><span class="p">)</span>
		<span class="c1"># elif pretrained_name == "sketch_to_image_stochastic" :
</span>        	<span class="c1"># ...</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>이제 해당 model에 forward형식으로 입력만 넣어주면 vae와 unet구조를 이용해서 image를 생성해 낼 수 있다.</p> <p>먼저 <code class="language-plaintext highlighter-rouge">edge_to_image</code>인 경우를 살펴보자. 이때는 본문의 forward parameter에서도 알 수 있지만, deterministic이 기본 default값 True로 고정되어 있다. 그렇게 <code class="language-plaintext highlighter-rouge">if deterministic:</code> 부분으로 호출이 된다. model 구조는 논문의 figure에서 잘 설명되어 있듯이, <code class="language-plaintext highlighter-rouge">vae.encoder -&gt; unet_encoder -&gt; unet_decoder -&gt; vae.decoder</code> 순서로 image가 생성이 되게 된다.</p> <p>다음으로 <code class="language-plaintext highlighter-rouge">sketch_to_image_stochastic</code> 같은 경우는 edge to image와 다르게 deterministic parameter가 False로 고정 되어 있어 else 문으로 들어가게 된다. pipeline 구조의 형태는 동일하지만, <code class="language-plaintext highlighter-rouge">weight = r</code> 형태를 이용해서 각 부분 적으로 weight로 기능을 조정해주는 부분이 존재한다. 해당 부분의 특이점 말고는 위와 동일한 형식이다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
</pre></td><td class="code"><pre>     
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">c_t</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">prompt_tokens</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">noise_map</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># either the prompt or the prompt_tokens should be provided
</span>        <span class="nf">assert </span><span class="p">(</span><span class="n">prompt</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">)</span> <span class="o">!=</span> <span class="p">(</span><span class="n">prompt_tokens</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">),</span> <span class="sh">"</span><span class="s">Either prompt or prompt_tokens should be provided</span><span class="sh">"</span>

        <span class="k">if</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># encode the text prompt
</span>            <span class="n">caption_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">model_max_length</span><span class="p">,</span>
                                            <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">max_length</span><span class="sh">"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="n">input_ids</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>
            <span class="n">caption_enc</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">text_encoder</span><span class="p">(</span><span class="n">caption_tokens</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">caption_enc</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">text_encoder</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">deterministic</span><span class="p">:</span>
            <span class="n">encoded_control</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">c_t</span><span class="p">).</span><span class="n">latent_dist</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">scaling_factor</span>
            <span class="n">model_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">unet</span><span class="p">(</span><span class="n">encoded_control</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">caption_enc</span><span class="p">,).</span><span class="n">sample</span>
            <span class="n">x_denoised</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">sched</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">model_pred</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">encoded_control</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">prev_sample</span>
            <span class="n">x_denoised</span> <span class="o">=</span> <span class="n">x_denoised</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">model_pred</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">incoming_skip_acts</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">current_down_blocks</span>
            <span class="n">output_image</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">x_denoised</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">scaling_factor</span><span class="p">).</span><span class="n">sample</span><span class="p">).</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># scale the lora weights based on the r value
</span>            <span class="n">self</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="nf">set_adapters</span><span class="p">([</span><span class="sh">"</span><span class="s">default</span><span class="sh">"</span><span class="p">],</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">r</span><span class="p">])</span>
            <span class="nf">set_weights_and_activate_adapters</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">vae_skip</span><span class="sh">"</span><span class="p">],</span> <span class="p">[</span><span class="n">r</span><span class="p">])</span>
            <span class="n">encoded_control</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">c_t</span><span class="p">).</span><span class="n">latent_dist</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">scaling_factor</span>
            <span class="c1"># combine the input and noise
</span>            <span class="n">unet_input</span> <span class="o">=</span> <span class="n">encoded_control</span> <span class="o">*</span> <span class="n">r</span> <span class="o">+</span> <span class="n">noise_map</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">conv_in</span><span class="p">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">r</span>
            <span class="n">unet_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">unet</span><span class="p">(</span><span class="n">unet_input</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">caption_enc</span><span class="p">,).</span><span class="n">sample</span>
            <span class="n">self</span><span class="p">.</span><span class="n">unet</span><span class="p">.</span><span class="n">conv_in</span><span class="p">.</span><span class="n">r</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="n">x_denoised</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">sched</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">unet_output</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">unet_input</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">prev_sample</span>
            <span class="n">x_denoised</span> <span class="o">=</span> <span class="n">x_denoised</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">unet_output</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">incoming_skip_acts</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">current_down_blocks</span>
            <span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">r</span>
            <span class="n">output_image</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">x_denoised</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">scaling_factor</span><span class="p">).</span><span class="n">sample</span><span class="p">).</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output_image</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>그렇게 위에 형식에 맞게 제작이 된 model을 이용해서 prompt와 preprocessing된 image를 model에 넣어주면 output_image를 생성해낼 수 있다.</p> <p><code class="language-plaintext highlighter-rouge">sketch_to_iamge_stochastic</code>같은 경우는 deterministic하게 생성할 것인지에 대한 diverse 조절이 가능하다는 이야기가 논문의 본문 3.4절 Extension에 등장하게 된다. 해당 부분을 기능으로 구현했다는 부분이 눈에 띄는 부분이다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="code"><pre>    
<span class="c1"># translate the image
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">model_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">edge_to_image</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">canny</span> <span class="o">=</span> <span class="nf">canny_from_pil</span><span class="p">(</span><span class="n">input_image</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">low_threshold</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">high_threshold</span><span class="p">)</span>
        <span class="n">canny_viz_inv</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">fromarray</span><span class="p">(</span><span class="mi">255</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">canny</span><span class="p">))</span>
        <span class="n">canny_viz_inv</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">bname</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">.png</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">_canny.png</span><span class="sh">'</span><span class="p">)))</span>
        <span class="n">c_t</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">to_tensor</span><span class="p">(</span><span class="n">canny</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">use_fp16</span><span class="p">:</span>
            <span class="n">c_t</span> <span class="o">=</span> <span class="n">c_t</span><span class="p">.</span><span class="nf">half</span><span class="p">()</span>
        <span class="n">output_image</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">c_t</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">prompt</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">args</span><span class="p">.</span><span class="n">model_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">sketch_to_image_stochastic</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">image_t</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">to_tensor</span><span class="p">(</span><span class="n">input_image</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.5</span>
        <span class="n">c_t</span> <span class="o">=</span> <span class="n">image_t</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">cuda</span><span class="p">().</span><span class="nf">float</span><span class="p">()</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">c_t</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">H</span> <span class="o">//</span> <span class="mi">8</span><span class="p">,</span> <span class="n">W</span> <span class="o">//</span> <span class="mi">8</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">c_t</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">use_fp16</span><span class="p">:</span>
            <span class="n">c_t</span> <span class="o">=</span> <span class="n">c_t</span><span class="p">.</span><span class="nf">half</span><span class="p">()</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">noise</span><span class="p">.</span><span class="nf">half</span><span class="p">()</span>
        <span class="n">output_image</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">c_t</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">prompt</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">gamma</span><span class="p">,</span> <span class="n">noise_map</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <hr/> <blockquote> <h2 id="1-unpaired-image-translation-cyclegan-turbo">1. Unpaired Image Translation (CycleGAN-Turbo)</h2> </blockquote> <p>위의 paried image translation과는 다른 inference.py를 사용하고 있다. 해당 부분에서 Model의 구조가 다를 것이라는 것을 어느정도 예측해볼 수 있다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre><span class="c1"># day to night
</span><span class="n">python</span> <span class="n">src</span><span class="o">/</span><span class="n">inference_unpaired</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_name</span> <span class="sh">"</span><span class="s">day_to_night</span><span class="sh">"</span> \
    <span class="o">--</span><span class="n">input_image</span> <span class="sh">"</span><span class="s">assets/examples/day2night_input.png</span><span class="sh">"</span> <span class="o">--</span><span class="n">output_dir</span> <span class="sh">"</span><span class="s">outputs</span><span class="sh">"</span>

<span class="c1"># night to day
</span><span class="n">python</span> <span class="n">src</span><span class="o">/</span><span class="n">inference_unpaired</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_name</span> <span class="sh">"</span><span class="s">night_to_day</span><span class="sh">"</span> \
    <span class="o">--</span><span class="n">input_image</span> <span class="sh">"</span><span class="s">assets/examples/night2day_input.png</span><span class="sh">"</span> <span class="o">--</span><span class="n">output_dir</span> <span class="sh">"</span><span class="s">outputs</span><span class="sh">"</span>

<span class="c1"># clear to rainy
</span><span class="n">python</span> <span class="n">src</span><span class="o">/</span><span class="n">inference_unpaired</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_name</span> <span class="sh">"</span><span class="s">clear_to_rainy</span><span class="sh">"</span> \
    <span class="o">--</span><span class="n">input_image</span> <span class="sh">"</span><span class="s">assets/examples/clear2rainy_input.png</span><span class="sh">"</span> <span class="o">--</span><span class="n">output_dir</span> <span class="sh">"</span><span class="s">outputs</span><span class="sh">"</span>

<span class="c1"># rainy to clear
</span><span class="n">python</span> <span class="n">src</span><span class="o">/</span><span class="n">inference_unpaired</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_name</span> <span class="sh">"</span><span class="s">rainy_to_clear</span><span class="sh">"</span> \
    <span class="o">--</span><span class="n">input_image</span> <span class="sh">"</span><span class="s">assets/examples/rainy2clear_input.png</span><span class="sh">"</span> <span class="o">--</span><span class="n">output_dir</span> <span class="sh">"</span><span class="s">outputs</span><span class="sh">"</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>사실 논문의 본문에서 알 수 있듯이 unpaired image translation에서는 CycleGAN-Turbo 형태로 사용한다고 나와 있다. 밑의 model의 형태를 한 번 간단하게 살펴보자</p> <p>이 부분은 위에서 본 pix2pix-turbo의 init과 아예 똑같은 형식이다. vae와 unet을 기본 형태로 정의해놓고 pretrained_name에 맞게 model을 불러온다. 재미 있는건 <code class="language-plaintext highlighter-rouge">self.direction</code>이라는 부분이 있는데, 여기서 <code class="language-plaintext highlighter-rouge">a2b, b2a</code> 형식으로 있다. 이 부분은 본문에서도 나와있듯이 GAN의 형태를 위해 <code class="language-plaintext highlighter-rouge">f(b2a(a2b(x)))</code> 형태로 다시 복원할 때 사용하기 위한 부분인 것 같다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">CycleGAN_Turbo</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">pretrained_name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">pretrained_path</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">ckpt_folder</span><span class="o">=</span><span class="sh">"</span><span class="s">checkpoints</span><span class="sh">"</span><span class="p">,</span> <span class="n">lora_rank_unet</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">lora_rank_vae</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">stabilityai/sd-turbo</span><span class="sh">"</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">tokenizer</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">text_encoder</span> <span class="o">=</span> <span class="n">CLIPTextModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">stabilityai/sd-turbo</span><span class="sh">"</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">text_encoder</span><span class="sh">"</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sched</span> <span class="o">=</span> <span class="nf">make_1step_sched</span><span class="p">()</span>
        <span class="n">vae</span> <span class="o">=</span> <span class="n">AutoencoderKL</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">stabilityai/sd-turbo</span><span class="sh">"</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">vae</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">unet</span> <span class="o">=</span> <span class="n">UNet2DConditionModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">stabilityai/sd-turbo</span><span class="sh">"</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="sh">"</span><span class="s">unet</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">vae</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">my_vae_encoder_fwd</span><span class="p">.</span><span class="nf">__get__</span><span class="p">(</span><span class="n">vae</span><span class="p">.</span><span class="n">encoder</span><span class="p">,</span> <span class="n">vae</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">__class__</span><span class="p">)</span>
        <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">my_vae_decoder_fwd</span><span class="p">.</span><span class="nf">__get__</span><span class="p">(</span><span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">,</span> <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">__class__</span><span class="p">)</span>
        <span class="c1"># add the skip connection convs
</span>        <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">skip_conv_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
        <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">skip_conv_2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
        <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">skip_conv_3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
        <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">skip_conv_4</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>
        <span class="n">vae</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">ignore_skip</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">self</span><span class="p">.</span><span class="n">unet</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">vae</span> <span class="o">=</span> <span class="n">unet</span><span class="p">,</span> <span class="n">vae</span>
        <span class="k">if</span> <span class="n">pretrained_name</span> <span class="o">==</span> <span class="sh">"</span><span class="s">day_to_night</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://www.cs.cmu.edu/~img2img-turbo/models/day2night.pkl</span><span class="sh">"</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">load_ckpt_from_url</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">ckpt_folder</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">999</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">caption</span> <span class="o">=</span> <span class="sh">"</span><span class="s">driving in the night</span><span class="sh">"</span>
            <span class="n">self</span><span class="p">.</span><span class="n">direction</span> <span class="o">=</span> <span class="sh">"</span><span class="s">a2b</span><span class="sh">"</span>
        <span class="k">elif</span> <span class="n">pretrained_name</span> <span class="o">==</span> <span class="sh">"</span><span class="s">night_to_day</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://www.cs.cmu.edu/~img2img-turbo/models/night2day.pkl</span><span class="sh">"</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">load_ckpt_from_url</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">ckpt_folder</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">999</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">caption</span> <span class="o">=</span> <span class="sh">"</span><span class="s">driving in the day</span><span class="sh">"</span>
            <span class="n">self</span><span class="p">.</span><span class="n">direction</span> <span class="o">=</span> <span class="sh">"</span><span class="s">b2a</span><span class="sh">"</span>
        <span class="k">elif</span> <span class="n">pretrained_name</span> <span class="o">==</span> <span class="sh">"</span><span class="s">clear_to_rainy</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://www.cs.cmu.edu/~img2img-turbo/models/clear2rainy.pkl</span><span class="sh">"</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">load_ckpt_from_url</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">ckpt_folder</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">999</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">caption</span> <span class="o">=</span> <span class="sh">"</span><span class="s">driving in heavy rain</span><span class="sh">"</span>
            <span class="n">self</span><span class="p">.</span><span class="n">direction</span> <span class="o">=</span> <span class="sh">"</span><span class="s">a2b</span><span class="sh">"</span>
        <span class="k">elif</span> <span class="n">pretrained_name</span> <span class="o">==</span> <span class="sh">"</span><span class="s">rainy_to_clear</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://www.cs.cmu.edu/~img2img-turbo/models/rainy2clear.pkl</span><span class="sh">"</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">load_ckpt_from_url</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">ckpt_folder</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">999</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">caption</span> <span class="o">=</span> <span class="sh">"</span><span class="s">driving in the day</span><span class="sh">"</span>
            <span class="n">self</span><span class="p">.</span><span class="n">direction</span> <span class="o">=</span> <span class="sh">"</span><span class="s">b2a</span><span class="sh">"</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>이후에는 간단하게 아래와 같이 foward를 진행할 수 있다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="code"><pre><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">forward_with_networks</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">direction</span><span class="p">,</span> <span class="n">vae_enc</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">vae_dec</span><span class="p">,</span> <span class="n">sched</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">text_emb</span><span class="p">):</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">direction</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">a2b</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">b2a</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">x_enc</span> <span class="o">=</span> <span class="nf">vae_enc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="n">direction</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">model_pred</span> <span class="o">=</span> <span class="nf">unet</span><span class="p">(</span><span class="n">x_enc</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">text_emb</span><span class="p">,).</span><span class="n">sample</span>
    <span class="n">x_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">sched</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">model_pred</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">timesteps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_enc</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">return_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">prev_sample</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">B</span><span class="p">)])</span>
    <span class="n">x_out_decoded</span> <span class="o">=</span> <span class="nf">vae_dec</span><span class="p">(</span><span class="n">x_out</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="n">direction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_out_decoded</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">caption</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">caption_emb</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">direction</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">direction</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="n">direction</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">direction</span>
    <span class="k">if</span> <span class="n">caption</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">caption_emb</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">caption</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="n">caption</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">caption</span>
    <span class="k">if</span> <span class="n">caption_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">caption_enc</span> <span class="o">=</span> <span class="n">caption_emb</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">caption_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tokenizer</span><span class="p">(</span><span class="n">caption</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">model_max_length</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">max_length</span><span class="sh">"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="n">input_ids</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">caption_enc</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">text_encoder</span><span class="p">(</span><span class="n">caption_tokens</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">detach</span><span class="p">().</span><span class="nf">clone</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">forward_with_networks</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">direction</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">vae_enc</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">unet</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">vae_dec</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">sched</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">caption_enc</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Code Review]]></summary></entry><entry><title type="html">[Papers] FaceLift: Learning Generalizable Single Image 3D Face Reconstruction from Synthetic Heads (ICCV 2025)</title><link href="https://oweixx.github.io/blog/2025/papers_FaceLift/" rel="alternate" type="text/html" title="[Papers] FaceLift: Learning Generalizable Single Image 3D Face Reconstruction from Synthetic Heads (ICCV 2025)"/><published>2025-11-19T00:00:00+00:00</published><updated>2025-11-19T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_FaceLift</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_FaceLift/"><![CDATA[<h2 id="facelift-learning-generalizable-single-image-3d-face-reconstruction-from-synthetic-heads">FaceLift: Learning Generalizable Single Image 3D Face Reconstruction from Synthetic Heads</h2> <h3 id="papergithubproject"><a href="https://arxiv.org/pdf/2412.17812">[Paper]</a><a href="https://github.com/weijielyu/FaceLift">[Github]</a><a href="https://www.wlyu.me/FaceLift/">[Project]</a></h3> <blockquote> <p><strong>Title:</strong> FaceLift: Learning Generalizable Single Image 3D Face Reconstruction from Synthetic Heads <br/> <strong>Journal name &amp; Publication Date:</strong> ICCV 2023-12-23<br/> <strong>Affiliation:</strong> University of California, Merced, Adobe Research</p> </blockquote> <hr/> <blockquote> <h2 id="0-multi-view-diffusion-based-generated-images">0. Multi-View Diffusion based Generated images</h2> </blockquote> <p>논문에서 제시된 <code class="language-plaintext highlighter-rouge">View Geneartion</code> 부분에 대한 코드 부분으로, Training된 Multi-view Diffusion을 이용하여 Single Image로 부터 각기 다른 Viewing image를 생성하는 부분이다.</p> <p><code class="language-plaintext highlighter-rouge">inference.py</code>에서 main함수에서 model들을 모두 init한 이후에 <code class="language-plaintext highlighter-rouge">process_single_image</code>로 넘어가면 본격적으로 진행이 된다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre><span class="n">mv_imgs</span> <span class="o">=</span> <span class="nf">unclip_pipeline</span><span class="p">(</span>
    <span class="n">input_image</span><span class="p">,</span> 
    <span class="bp">None</span><span class="p">,</span>
    <span class="n">prompt_embeds</span><span class="o">=</span><span class="n">color_prompt_embedding</span><span class="p">,</span>
    <span class="n">guidance_scale</span><span class="o">=</span><span class="n">guidance_scale_2D</span><span class="p">,</span>
    <span class="n">num_images_per_prompt</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">num_inference_steps</span><span class="o">=</span><span class="n">step_2D</span><span class="p">,</span>
    <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span>
    <span class="n">eta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="p">).</span><span class="n">images</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>타고타고 들어가다 보면 <code class="language-plaintext highlighter-rouge">mvdiffusion/pipelines/pipeline_mvdiffusion_unclip.py</code>에 DiffusionPipeline을 상속받는 <code class="language-plaintext highlighter-rouge">StableUnCLIPImg2ImgPipeline</code>이 있다. 해당 부분은 “pipeline for text-guided image to image generation using stable uinCLIP”이라고 설명이 적혀 있다.</p> <p>해당 부분은 논문에서와 같이 text embedding으로 view generation을 하기 때문에 해당 pipeline을 사용하는 것 같다.</p> <p>결국 pipeline을 다시 재수정한 부분이니 실제로 실행되는 <code class="language-plaintext highlighter-rouge">__call__</code>에서의 동작과정에 집중해서 확인해보자.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">StableUnCLIPImg2ImgPipeline</span><span class="p">(</span><span class="n">DiffusionPipeline</span><span class="p">):</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>input으로 들어오는 prompt에 대하여 embedding시키고 <code class="language-plaintext highlighter-rouge">prompte_embeds</code>형태의 출력으로 받는다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre><span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_encode_prompt</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="n">num_images_per_prompt</span><span class="o">=</span><span class="n">num_images_per_prompt</span><span class="p">,</span>
    <span class="n">do_classifier_free_guidance</span><span class="o">=</span><span class="n">do_classifier_free_guidance</span><span class="p">,</span>
    <span class="n">negative_prompt</span><span class="o">=</span><span class="n">negative_prompt</span><span class="p">,</span>
    <span class="n">prompt_embeds</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="p">,</span>
    <span class="n">negative_prompt_embeds</span><span class="o">=</span><span class="n">negative_prompt_embeds</span><span class="p">,</span>
    <span class="n">lora_scale</span><span class="o">=</span><span class="n">text_encoder_lora_scale</span><span class="p">,</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>input image를 encoder에 넣고 embedding하고 latent형태로 변환시킨다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre><span class="n">image_embeds</span><span class="p">,</span> <span class="n">image_latents</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_encode_image</span><span class="p">(</span>
    <span class="n">image_pil</span><span class="o">=</span><span class="n">image_pil</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="n">num_images_per_prompt</span><span class="o">=</span><span class="n">num_images_per_prompt</span><span class="p">,</span>
    <span class="n">do_classifier_free_guidance</span><span class="o">=</span><span class="n">do_classifier_free_guidance</span><span class="p">,</span>
    <span class="n">noise_level</span><span class="o">=</span><span class="n">noise_level</span><span class="p">,</span>
    <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>위에서 처리된 prompt와 image ebedding 변수들과 latent를 이용하여 Denosing Loop에서 처리한다. diffusion에 대한 지식이 아직은 많이 부족해서 어림짐작해서 일단은 해석해보겠다….</p> <p>먼저 <code class="language-plaintext highlighter-rouge">torch.cat([latent_model_input, image_latents], dim=1)</code> 부분에서 생성해야 하는 latent와 conditioning으로 들어가는 image latents가 concat되어 input으로 들어간다.</p> <p>이후에 unet에 직접적으로 들어갈 때는 encoder_hidden_states의 input으로 prompt_embeds가 들어가게 되어 <code class="language-plaintext highlighter-rouge">predict the noise residual</code>을 수행하게 된다.</p> <p>이후에 noisy sample을 step해주어 x_t -&gt; x_t-1 latent를 계산해준다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="code"><pre><span class="c1"># 8. Denoising loop
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">progress_bar</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">do_classifier_free_guidance</span><span class="p">:</span>
        <span class="n">latent_model_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">latents</span><span class="p">,</span> <span class="n">latents</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">latent_model_input</span> <span class="o">=</span> <span class="n">latents</span>
    <span class="n">latent_model_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span>
            <span class="n">latent_model_input</span><span class="p">,</span> <span class="n">image_latents</span>
        <span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">latent_model_input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">scheduler</span><span class="p">.</span><span class="nf">scale_model_input</span><span class="p">(</span><span class="n">latent_model_input</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

    <span class="c1"># predict the noise residual
</span>    <span class="n">unet_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">unet</span><span class="p">(</span>
        <span class="n">latent_model_input</span><span class="p">,</span>
        <span class="n">t</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="p">,</span>
        <span class="n">class_labels</span><span class="o">=</span><span class="n">image_embeds</span><span class="p">,</span>
        <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="n">cross_attention_kwargs</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="n">noise_pred</span> <span class="o">=</span> <span class="n">unet_out</span>
        
    <span class="c1"># perform guidance
</span>    <span class="k">if</span> <span class="n">do_classifier_free_guidance</span><span class="p">:</span>
        <span class="n">noise_pred_uncond</span><span class="p">,</span> <span class="n">noise_pred_text</span> <span class="o">=</span> <span class="n">noise_pred</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">noise_pred</span> <span class="o">=</span> <span class="n">noise_pred_uncond</span> <span class="o">+</span> <span class="n">guidance_scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">noise_pred_text</span> <span class="o">-</span> <span class="n">noise_pred_uncond</span><span class="p">)</span>

    <span class="c1"># compute the previous noisy sample x_t -&gt; x_t-1
</span>    <span class="n">latents</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">noise_pred</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">latents</span><span class="p">,</span> <span class="o">**</span><span class="n">extra_step_kwargs</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="n">callback_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">callback</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">latents</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <hr/> <blockquote> <h2 id="1-gs-lrm">1. GS-LRM</h2> </blockquote> <p>GSLRM Class는 <code class="language-plaintext highlighter-rouge">gslrm/model/gslrm.py</code>에 숨어 있다. 해당 부분도 방대한 코드 양으로 모두 이해하기는 힘들 것 같다. 많은 부분을 생략하고 forward 부분의 코드 구성을 한 번 확인해보자. 사실 forward 부분도 150줄이 넘는다.</p> <p>args부터 간단하게 살펴보면 batch형태로 들어오는 data는 논문에서와 같이 생성된 Multi-view images와 Camera intrinsics 정보 등이 있고, 출력값으로는 Dictionary형태의 model output이고 여기서 Gaussian정보들을 반환해주고 있으니 해당 정보들로 바로 GS Reconstruction이 가능하다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span> 
    <span class="n">batch_data</span><span class="p">:</span> <span class="n">edict</span><span class="p">,</span> 
    <span class="n">create_visual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> 
    <span class="n">split_data</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">edict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Forward pass of the GSLRM model.
    
    Args:
        batch_data: Input batch containing:
            - image: Multi-view images [batch, views, channels, height, width]
            - fxfycxcy: Camera intrinsics [batch, views, 4]
            - c2w: Camera-to-world matrices [batch, views, 4, 4]
        create_visual: Whether to create visualization outputs
        split_data: Whether to split input/target data
        
    Returns:
        Dictionary containing model outputs including Gaussians, renders, and losses
    </span><span class="sh">"""</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>눈에 확 띄는 주요한 부분만 모아서 한꺼번에 봐보자. 사실 이 밑의 process들이 다 논문에 나와있는 부분이긴한데 중요한 것 같다.</p> <p>대략적으로는 <code class="language-plaintext highlighter-rouge">Patchify &amp; Linear</code> 하는 부분이 있고 <code class="language-plaintext highlighter-rouge">transformer process</code>를 통과하고 Linear &amp; Unpatchify하여 gaussian_tokens과 image_patch_tokens으로 나뉘고 이를 통해 gaussian parameter를 생성하게 된다. 이후에 pixel-aligned를 하여 gaussian parameter를 예측(?) 하게 되는 일련의 과정인 것 같다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
</pre></td><td class="code"><pre><span class="c1"># Prepare posed images with Plucker coordinates [batch, views, channels, height, width]
</span><span class="n">posed_images</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_create_posed_images_with_plucker</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="c1"># Tokenize images into patches
</span><span class="n">image_patch_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">patch_embedder</span><span class="p">(</span><span class="n">posed_images</span><span class="p">)</span>  <span class="c1"># [batch*views, num_patches, hidden_dim]
</span><span class="n">_</span><span class="p">,</span> <span class="n">num_patches</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">image_patch_tokens</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
<span class="n">image_patch_tokens</span> <span class="o">=</span> <span class="n">image_patch_tokens</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_views</span> <span class="o">*</span> <span class="n">num_patches</span><span class="p">,</span> <span class="n">hidden_dim</span>
<span class="p">)</span>  <span class="c1"># [batch, views*patches, hidden_dim]
</span>
<span class="c1"># Prepare Gaussian tokens with positional embeddings
</span><span class="n">gaussian_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gaussian_position_embeddings</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Process through transformer with gradient checkpointing
</span><span class="n">combined_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_process_through_transformer</span><span class="p">(</span>
    <span class="n">gaussian_tokens</span><span class="p">,</span> <span class="n">image_patch_tokens</span>
<span class="p">)</span>

<span class="c1"># Split back into Gaussian and image tokens
</span><span class="n">num_gaussians</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gaussians</span><span class="p">.</span><span class="n">n_gaussians</span>
<span class="n">gaussian_tokens</span><span class="p">,</span> <span class="n">image_patch_tokens</span> <span class="o">=</span> <span class="n">combined_tokens</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span>
    <span class="p">[</span><span class="n">num_gaussians</span><span class="p">,</span> <span class="n">num_views</span> <span class="o">*</span> <span class="n">num_patches</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Generate Gaussian parameters from transformer outputs
</span><span class="n">gaussian_params</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gaussian_upsampler</span><span class="p">(</span><span class="n">gaussian_tokens</span><span class="p">,</span> <span class="n">image_patch_tokens</span><span class="p">)</span>

<span class="c1"># Generate pixel-aligned Gaussians from image tokens
</span><span class="n">pixel_aligned_gaussian_params</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pixel_gaussian_decoder</span><span class="p">(</span><span class="n">image_patch_tokens</span><span class="p">)</span>

<span class="c1"># Calculate Gaussian parameter dimensions
</span><span class="n">sh_degree</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gaussians</span><span class="p">.</span><span class="n">sh_degree</span>
<span class="n">gaussian_param_dim</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">(</span><span class="n">sh_degree</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">pixel_aligned_gaussian_params</span> <span class="o">=</span> <span class="n">pixel_aligned_gaussian_params</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">gaussian_param_dim</span>
<span class="p">)</span>  <span class="c1"># [batch, views*pixels, gaussian_params]
</span><span class="n">num_pixel_aligned_gaussians</span> <span class="o">=</span> <span class="n">pixel_aligned_gaussian_params</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Combine all Gaussian parameters
</span><span class="n">all_gaussian_params</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">gaussian_params</span><span class="p">,</span> <span class="n">pixel_aligned_gaussian_params</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Convert to final Gaussian format
</span><span class="n">xyz</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">scaling</span><span class="p">,</span> <span class="n">rotation</span><span class="p">,</span> <span class="n">opacity</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gaussian_upsampler</span><span class="p">.</span><span class="nf">to_gs</span><span class="p">(</span><span class="n">all_gaussian_params</span><span class="p">)</span>

<span class="c1"># Extract pixel-aligned Gaussian positions for processing
</span><span class="n">pixel_aligned_xyz</span> <span class="o">=</span> <span class="n">xyz</span><span class="p">[:,</span> <span class="o">-</span><span class="n">num_pixel_aligned_gaussians</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">image_tokenizer</span><span class="p">.</span><span class="n">patch_size</span>

<span class="n">pixel_aligned_xyz</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span>
    <span class="n">pixel_aligned_xyz</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">batch (views height width patch_h patch_w) coords -&gt; batch views coords (height patch_h) (width patch_w)</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">views</span><span class="o">=</span><span class="n">num_views</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">height</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">width</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">,</span>
    <span class="n">patch_h</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
    <span class="n">patch_w</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Code Review]]></summary></entry><entry><title type="html">[Papers] Generalizable and Animatable Gaussian Head Avatar (NeurIPS 2024)</title><link href="https://oweixx.github.io/blog/2025/papers_GAGAvatars/" rel="alternate" type="text/html" title="[Papers] Generalizable and Animatable Gaussian Head Avatar (NeurIPS 2024)"/><published>2025-11-19T00:00:00+00:00</published><updated>2025-11-19T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_GAGAvatars</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_GAGAvatars/"><![CDATA[<h2 id="generalizable-and-animatable-gaussian-head-avatar">Generalizable and Animatable Gaussian Head Avatar</h2> <h3 id="papergithubproject"><a href="https://arxiv.org/abs/2410.07971">[Paper]</a><a href="https://github.com/xg-chu/GAGAvatar">[Github]</a><a href="https://xg-chu.site/project_gagavatar/">[Project]</a></h3> <blockquote> <p><strong>Title:</strong> Generalizable and Animatable Gaussian Head Avatar <br/> <strong>Journal name &amp; Publication Date:</strong> NeurIPS 2024-10-10<br/> <strong>Affiliation:</strong> The University of Tokyo, RIKEN AIP</p> </blockquote> <hr/> <blockquote> <h2 id="0-inference">0. inference</h2> </blockquote> <p>inference는 아래와 같이 수행할 수 있다고 한다.</p> <p><code class="language-plaintext highlighter-rouge">inference.py</code> 부분을 살펴보면, build_model함수로 GAGAvatars model을 만들어서 model을 불러오는데 해당 부분이 아마 중요한 의미들을 많이 담고 있을 것 같은데 해당 부분은 다음 section에서 따로 더 찾아보는 것으로 해야겠다. 일단은 GAGAvatars model을 불러왔다는 가정으로 시작한다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre><span class="c1"># Driven by another image:
# This will track the images online, which is slow.
</span><span class="n">python</span> <span class="n">inference</span><span class="p">.</span><span class="n">py</span> <span class="o">-</span><span class="n">d</span> <span class="p">.</span><span class="o">/</span><span class="n">demos</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="mf">2.j</span><span class="n">pg</span> <span class="o">-</span><span class="n">i</span> <span class="p">.</span><span class="o">/</span><span class="n">demos</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="mf">1.j</span><span class="n">pg</span>

<span class="c1"># Driven by a tracked video:
</span><span class="n">python</span> <span class="n">inference</span><span class="p">.</span><span class="n">py</span> <span class="o">-</span><span class="n">d</span> <span class="p">.</span><span class="o">/</span><span class="n">demos</span><span class="o">/</span><span class="n">drivers</span><span class="o">/</span><span class="n">obama</span> <span class="o">-</span><span class="n">i</span> <span class="p">.</span><span class="o">/</span><span class="n">demos</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="mf">1.j</span><span class="n">pg</span>

<span class="c1"># Driven by a tracked image_lmdb
</span><span class="n">python</span> <span class="n">inference</span><span class="p">.</span><span class="n">py</span> <span class="o">-</span><span class="n">d</span> <span class="p">.</span><span class="o">/</span><span class="n">demos</span><span class="o">/</span><span class="n">drivers</span><span class="o">/</span><span class="n">vfhq_demo</span> <span class="o">-</span><span class="n">i</span> <span class="p">.</span><span class="o">/</span><span class="n">demos</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="mf">1.j</span><span class="n">pg</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>아래와 같이 GAGAvatars model과 monocular face trucker인 GAGAvatar_Track model을 초기화 해준다.</p> <p>결국 face trucker model은 단일 이미지, 카메라에 대해 FLAME parameters와 camera parameters를 예측하여 반환하는 모델이다.</p> <p>그렇게 reference image와 track_engine을 이용하여 <code class="language-plaintext highlighter-rouge">feature_data</code>를 반환 받게 된다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre><span class="n">model</span> <span class="o">=</span> <span class="nf">build_model</span><span class="p">(</span><span class="n">model_cfg</span><span class="o">=</span><span class="n">meta_cfg</span><span class="p">.</span><span class="n">MODEL</span><span class="p">)</span>
<span class="bp">...</span>
<span class="n">track_engine</span> <span class="o">=</span> <span class="nc">TrackEngine</span><span class="p">(</span><span class="n">focal_length</span><span class="o">=</span><span class="mf">12.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">feature_name</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">basename</span><span class="p">(</span><span class="n">image_path</span><span class="p">).</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">feature_data</span> <span class="o">=</span> <span class="nf">get_tracked_results</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">track_engine</span><span class="p">,</span> <span class="n">force_retrack</span><span class="o">=</span><span class="n">force_retrack</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>driving image에 대해서도 위와 같은 일련의 과정으로 tacking을 하여 driver_data를 준비한다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre><span class="k">if</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">isdir</span><span class="p">(</span><span class="n">driver_path</span><span class="p">):</span>
    <span class="n">driver_name</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">basename</span><span class="p">(</span><span class="n">driver_path</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">driver_path</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">/</span><span class="sh">'</span><span class="p">)</span> <span class="k">else</span> <span class="n">driver_path</span><span class="p">)</span>
    <span class="n">driver_dataset</span> <span class="o">=</span> <span class="nc">DriverData</span><span class="p">(</span><span class="n">driver_path</span><span class="p">,</span> <span class="n">feature_data</span><span class="p">,</span> <span class="n">meta_cfg</span><span class="p">.</span><span class="n">DATASET</span><span class="p">.</span><span class="n">POINT_PLANE_SIZE</span><span class="p">)</span>
    <span class="n">driver_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">driver_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">driver_name</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">basename</span><span class="p">(</span><span class="n">driver_path</span><span class="p">).</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">driver_data</span> <span class="o">=</span> <span class="nf">get_tracked_results</span><span class="p">(</span><span class="n">driver_path</span><span class="p">,</span> <span class="n">track_engine</span><span class="p">,</span> <span class="n">force_retrack</span><span class="o">=</span><span class="n">force_retrack</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">driver_data</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Finish inference, no face in driver: </span><span class="si">{</span><span class="n">image_path</span><span class="si">}</span><span class="s">.</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="n">driver_dataset</span> <span class="o">=</span> <span class="nc">DriverData</span><span class="p">({</span><span class="n">driver_name</span><span class="p">:</span> <span class="n">driver_data</span><span class="p">},</span> <span class="n">feature_data</span><span class="p">,</span> <span class="n">meta_cfg</span><span class="p">.</span><span class="n">DATASET</span><span class="p">.</span><span class="n">POINT_PLANE_SIZE</span><span class="p">)</span>
    <span class="n">driver_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">driver_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <hr/> <blockquote> <h2 id="1-gagavatars-model">1. GAGAvatars Model</h2> </blockquote> <p>위에서 확인했던 inference.py에는 module화된 조각들을 잘 처리하는 pipeline자체를 보여주고 있기 때문에 사실상 어떠한 과정이 진행되고 있는지에 대한 부분을 확인하기는 어렵다.</p> <p>model config file을 보더라도 model을 불러올 때 <code class="language-plaintext highlighter-rouge">models/GAGAvatar/models.py</code>에서의 GAGAvatar class를 불러오고 있다. 해당 model의 init 속성을 보았을 때 논문의 Reconstruction Branch를 수행하는 것으로 예상이 된다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">GAGAvatar</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model_cfg</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">base_model</span> <span class="o">=</span> <span class="nc">DINOBase</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">base_model</span><span class="p">.</span><span class="n">dino_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="c1"># dir_encoder
</span>        <span class="n">n_harmonic_dir</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="n">self</span><span class="p">.</span><span class="n">direnc_dim</span> <span class="o">=</span> <span class="n">n_harmonic_dir</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">3</span>
        <span class="n">self</span><span class="p">.</span><span class="n">harmo_encoder</span> <span class="o">=</span> <span class="nc">HarmonicEmbedding</span><span class="p">(</span><span class="n">n_harmonic_dir</span><span class="p">)</span>
        <span class="c1"># pre_trained
</span>        <span class="n">self</span><span class="p">.</span><span class="n">head_base</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">5023</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gs_generator_g</span> <span class="o">=</span> <span class="nc">LinearGSGenerator</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">dir_dim</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">direnc_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gs_generator_l0</span> <span class="o">=</span> <span class="nc">ConvGSGenerator</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">dir_dim</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">direnc_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gs_generator_l1</span> <span class="o">=</span> <span class="nc">ConvGSGenerator</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">dir_dim</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">direnc_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cam_params</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">focal_x</span><span class="sh">'</span><span class="p">:</span> <span class="mf">12.0</span><span class="p">,</span> <span class="sh">'</span><span class="s">focal_y</span><span class="sh">'</span><span class="p">:</span> <span class="mf">12.0</span><span class="p">,</span> <span class="sh">'</span><span class="s">size</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">]}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">upsampler</span> <span class="o">=</span> <span class="nc">StyleUNet</span><span class="p">(</span><span class="n">in_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_size</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">percep_loss</span> <span class="o">=</span> <span class="nc">FacePerceptualLoss</span><span class="p">(</span><span class="n">loss_type</span><span class="o">=</span><span class="sh">'</span><span class="s">l1</span><span class="sh">'</span><span class="p">,</span> <span class="n">weighted</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>하나씩 살펴보자. 먼저 <code class="language-plaintext highlighter-rouge">self.gs_generator_g</code>는 <code class="language-plaintext highlighter-rouge">LinearGSGenerator</code> model을 사용한다. 해당 class model은 밑에 정의 되어 있다.</p> <p>보면 Lienar,ReLU의 조합으로 구성된 Sequential 형태이다. 입력으로 들어오는 input_features와 plane_direnc에 대하여 gaussian parameter를 추출 할 수 있는 형태로 설계 되어 있다. 아마 이 부분은 train에서 충분히 학습되어 weight가 담겨있을 것 같다.</p> <p>그리고 해당 부분은 위 Expression Branch에서 사용되는 global part형태에 사용되는 gs로 확인이 된다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre><span class="c1"># global part
</span><span class="n">gs_params_g</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gs_generator_g</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span>
            <span class="n">self</span><span class="p">.</span><span class="n">head_base</span><span class="p">[</span><span class="bp">None</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">f_feature1</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5023</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> 
        <span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
    <span class="p">),</span> <span class="n">plane_direnc</span>
<span class="p">)</span>
<span class="n">gs_params_g</span><span class="p">[</span><span class="sh">'</span><span class="s">xyz</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">t_points</span>
</pre></td></tr></tbody></table></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">LinearGSGenerator</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">dir_dim</span><span class="o">=</span><span class="mi">27</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># params
</span>        <span class="n">self</span><span class="p">.</span><span class="n">feature_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">layer_in_dim</span> <span class="o">=</span> <span class="n">in_dim</span><span class="o">//</span><span class="mi">4</span> <span class="o">+</span> <span class="n">dir_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">color_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">layer_in_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">opacity_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">layer_in_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scale_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">layer_in_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rotation_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">layer_in_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">plane_direnc</span><span class="p">):</span>
        <span class="n">input_features</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feature_layers</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span>
        <span class="n">plane_direnc</span> <span class="o">=</span> <span class="n">plane_direnc</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">input_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">input_features</span><span class="p">,</span> <span class="n">plane_direnc</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># color
</span>        <span class="n">colors</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">color_layers</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span>
        <span class="n">colors</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">colors</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">3</span><span class="p">])</span>
        <span class="c1"># opacity
</span>        <span class="n">opacities</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">opacity_layers</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span>
        <span class="n">opacities</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">opacities</span><span class="p">)</span>
        <span class="c1"># scale
</span>        <span class="n">scales</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">scale_layers</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span>
        <span class="c1"># scales = torch.exp(scales) * 0.01
</span>        <span class="n">scales</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.05</span>
        <span class="c1"># rotation
</span>        <span class="n">rotations</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">rotation_layers</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span>
        <span class="n">rotations</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">rotations</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="sh">'</span><span class="s">colors</span><span class="sh">'</span><span class="p">:</span><span class="n">colors</span><span class="p">,</span> <span class="sh">'</span><span class="s">opacities</span><span class="sh">'</span><span class="p">:</span><span class="n">opacities</span><span class="p">,</span> <span class="sh">'</span><span class="s">scales</span><span class="sh">'</span><span class="p">:</span><span class="n">scales</span><span class="p">,</span> <span class="sh">'</span><span class="s">rotations</span><span class="sh">'</span><span class="p">:</span><span class="n">rotations</span><span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>밑에 해당 <code class="language-plaintext highlighter-rouge">self.gs_generator_10</code> 과 <code class="language-plaintext highlighter-rouge">self.gs_generator_l1</code>은 local DINO에서 추출된 local feature에 대한 dual-lifting에 관한 코드 부분이다. feature와 plane_direnc에 대하여 양쪽으로 gaussian을 예측하여 Gaussian을 두 층으로 예측을 하게 된다. Gaussian의 parameter를 예측할 때는 Global과는 다르게 얇은 Conv층으로 <code class="language-plaintext highlighter-rouge">32+1+3+4+1</code>의 gaussian parameter들을 예측한다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre> <span class="c1"># local part
</span><span class="n">gs_params_l0</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gs_generator_l0</span><span class="p">(</span><span class="n">f_feature0</span><span class="p">,</span> <span class="n">plane_direnc</span><span class="p">)</span>
<span class="n">gs_params_l1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gs_generator_l1</span><span class="p">(</span><span class="n">f_feature0</span><span class="p">,</span> <span class="n">plane_direnc</span><span class="p">)</span>
<span class="n">gs_params_l0</span><span class="p">[</span><span class="sh">'</span><span class="s">xyz</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">f_planes</span><span class="p">[</span><span class="sh">'</span><span class="s">plane_points</span><span class="sh">'</span><span class="p">]</span> <span class="o">+</span> <span class="n">gs_params_l0</span><span class="p">[</span><span class="sh">'</span><span class="s">positions</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="n">f_planes</span><span class="p">[</span><span class="sh">'</span><span class="s">plane_dirs</span><span class="sh">'</span><span class="p">][:,</span> <span class="bp">None</span><span class="p">]</span>
<span class="n">gs_params_l1</span><span class="p">[</span><span class="sh">'</span><span class="s">xyz</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">f_planes</span><span class="p">[</span><span class="sh">'</span><span class="s">plane_points</span><span class="sh">'</span><span class="p">]</span> <span class="o">+</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">gs_params_l1</span><span class="p">[</span><span class="sh">'</span><span class="s">positions</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="n">f_planes</span><span class="p">[</span><span class="sh">'</span><span class="s">plane_dirs</span><span class="sh">'</span><span class="p">][:,</span> <span class="bp">None</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>궁금한 부분은 해당 두께의 Conv층으로 충분히 해당 Gaussian Parameter들이 잘 예측이 될 수 있는건지이다.</p> <p>GPT에게 물어봤을 때는 이 부분은 당연히 가능하다는 의견이다. 애초에 DINO에서 추출한 feature형태가 저차원의 형태가 아닌 고차원의 형태이고 지금 현재 상황에서는 아예 다른 차원으로써의 이동이 아닌 mapping에 가까운 의도이기 때문에 이정도 깊이의 conv로 해당 역할을 수행하는 것은 일반적인 쪽에 가깝다고 한다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">ConvGSGenerator</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">dir_dim</span><span class="o">=</span><span class="mi">27</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">out_dim</span> <span class="o">=</span> <span class="mi">32</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># color + opacity + scale + rotation + position
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gaussian_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_dim</span><span class="o">+</span><span class="n">dir_dim</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_dim</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_dim</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_dim</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">plane_direnc</span><span class="p">):</span>
        <span class="n">plane_direnc</span> <span class="o">=</span> <span class="n">plane_direnc</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">input_features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
        <span class="n">input_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">input_features</span><span class="p">,</span> <span class="n">plane_direnc</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">gaussian_params</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gaussian_conv</span><span class="p">(</span><span class="n">input_features</span><span class="p">)</span>
        <span class="c1"># color
</span>        <span class="n">colors</span> <span class="o">=</span> <span class="n">gaussian_params</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">32</span><span class="p">]</span>
        <span class="n">colors</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">colors</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">3</span><span class="p">])</span>
        <span class="c1"># opacity
</span>        <span class="n">opacities</span> <span class="o">=</span> <span class="n">gaussian_params</span><span class="p">[:,</span> <span class="mi">32</span><span class="p">:</span><span class="mi">33</span><span class="p">]</span>
        <span class="n">opacities</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">opacities</span><span class="p">)</span>
        <span class="c1"># scale
</span>        <span class="n">scales</span> <span class="o">=</span> <span class="n">gaussian_params</span><span class="p">[:,</span> <span class="mi">33</span><span class="p">:</span><span class="mi">36</span><span class="p">]</span>
        <span class="c1"># scales = torch.exp(scales) * 0.01
</span>        <span class="n">scales</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.05</span>
        <span class="c1"># rotation
</span>        <span class="n">rotations</span> <span class="o">=</span> <span class="n">gaussian_params</span><span class="p">[:,</span> <span class="mi">36</span><span class="p">:</span><span class="mi">40</span><span class="p">]</span>
        <span class="n">rotations</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">rotations</span><span class="p">)</span>
        <span class="c1"># position
</span>        <span class="n">positions</span> <span class="o">=</span> <span class="n">gaussian_params</span><span class="p">[:,</span> <span class="mi">40</span><span class="p">:</span><span class="mi">41</span><span class="p">]</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">colors</span><span class="sh">'</span><span class="p">:</span><span class="n">colors</span><span class="p">,</span> <span class="sh">'</span><span class="s">opacities</span><span class="sh">'</span><span class="p">:</span><span class="n">opacities</span><span class="p">,</span> <span class="sh">'</span><span class="s">scales</span><span class="sh">'</span><span class="p">:</span><span class="n">scales</span><span class="p">,</span> <span class="sh">'</span><span class="s">rotations</span><span class="sh">'</span><span class="p">:</span><span class="n">rotations</span><span class="p">,</span> <span class="sh">'</span><span class="s">positions</span><span class="sh">'</span><span class="p">:</span><span class="n">positions</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="n">key</span><span class="p">].</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="n">key</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">results</span><span class="p">[</span><span class="n">key</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">results</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>그럼 그렇게 위에서 global하게 나온 head의 gaussian과 밑에서 dual-lifting으로 나온 두 개의 gaussian을 concat하여 하나의 gaussian scene으로 구성하게 된다. 그렇게 image를 생성해서 dict형태의 results로 반환이 되게 된다.</p> <p>이게 위에 dual-lifting으로 plane에 예측되는 gaussian points의 개수가 $(H,W) = 296 \times 296$ 형태로 나오고 2개의 층으로 나오기 때문에 175,232개의 point가 나온다고 한다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="n">gs_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">k</span><span class="p">:</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">gs_params_g</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">gs_params_l0</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">gs_params_l1</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">gs_params_g</span><span class="p">.</span><span class="nf">keys</span><span class="p">()</span>
<span class="p">}</span>
<span class="n">gen_images</span> <span class="o">=</span> <span class="nf">render_gaussian</span><span class="p">(</span>
    <span class="n">gs_params</span><span class="o">=</span><span class="n">gs_params</span><span class="p">,</span> <span class="n">cam_matrix</span><span class="o">=</span><span class="n">t_transform</span><span class="p">,</span> <span class="n">cam_params</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">cam_params</span>
<span class="p">)[</span><span class="sh">'</span><span class="s">images</span><span class="sh">'</span><span class="p">]</span>
<span class="n">sr_gen_images</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">upsampler</span><span class="p">(</span><span class="n">gen_images</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">t_image</span><span class="sh">'</span><span class="p">:</span><span class="n">t_image</span><span class="p">,</span> <span class="sh">'</span><span class="s">t_bbox</span><span class="sh">'</span><span class="p">:</span><span class="n">t_bbox</span><span class="p">,</span> <span class="sh">'</span><span class="s">t_points</span><span class="sh">'</span><span class="p">:</span> <span class="n">t_points</span><span class="p">,</span> 
    <span class="sh">'</span><span class="s">p_points</span><span class="sh">'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">gs_params_l0</span><span class="p">[</span><span class="sh">'</span><span class="s">xyz</span><span class="sh">'</span><span class="p">],</span> <span class="n">gs_params_l1</span><span class="p">[</span><span class="sh">'</span><span class="s">xyz</span><span class="sh">'</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="sh">'</span><span class="s">gen_image</span><span class="sh">'</span><span class="p">:</span> <span class="n">gen_images</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="sh">'</span><span class="s">sr_gen_image</span><span class="sh">'</span><span class="p">:</span> <span class="n">sr_gen_images</span>
<span class="p">}</span>
<span class="k">return</span> <span class="n">results</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Code Review]]></summary></entry><entry><title type="html">[Papers] CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models (CVPR 2025 Oral)</title><link href="https://oweixx.github.io/blog/2025/papers_CAP4D/" rel="alternate" type="text/html" title="[Papers] CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models (CVPR 2025 Oral)"/><published>2025-11-18T00:00:00+00:00</published><updated>2025-11-18T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_CAP4D</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_CAP4D/"><![CDATA[<h2 id="cap4d-creating-animatable-4d-portrait-avatars-with-morphable-multi-view-diffusion-models">CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models</h2> <h3 id="papergithubproject"><a href="https://arxiv.org/abs/2412.12093">[Paper]</a><a href="https://github.com/felixtaubner/cap4d/">[Github]</a><a href="https://felixtaubner.github.io/cap4d/">[Project]</a></h3> <blockquote> <p><strong>Title:</strong> CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models<br/> <strong>Journal name &amp; Publication Date:</strong> CVPR 2024-12-16<br/> <strong>Affiliation:</strong> University of Toronto, Vector Institute, LG Electronics</p> </blockquote> <hr/> <blockquote> <h2 id="0-download-flame-and-mdmm-weights">0. Download FLAME and MDMM weights</h2> </blockquote> <p>먼저 FLAME model과 pretrained된 mmdm weights를 다운 받을 수 있는 shell script를 실행해준다. 이때 flame model은 <code class="language-plaintext highlighter-rouge">flame2023_no_jaw.pkl</code>을 사용하는 것 같다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre><span class="c1"># 1. Download FLAME blendshapes
# set your flame username and password
</span><span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">download_flame</span><span class="p">.</span><span class="n">sh</span> 

<span class="c1"># 2. Download CAP4D MMDM weights
</span><span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">download_mmdm_weights</span><span class="p">.</span><span class="n">sh</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>이후 해당 pkl file을 해당 환경 설정 version과 같은 numpy version으로 수행하기 위해 아래 code를 실행해준다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">fixes</span><span class="o">/</span><span class="n">fix_flame_pickle</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">pickle_path</span> <span class="n">data</span><span class="o">/</span><span class="n">assets</span><span class="o">/</span><span class="n">flame</span><span class="o">/</span><span class="n">flame2023_no_jaw</span><span class="p">.</span><span class="n">pkl</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>이후 test와 inference는 아래와 같은 scripts로 수행하면 된다고 한다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre><span class="c1"># for check installation with a test run
</span><span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">test_pipeline</span><span class="p">.</span><span class="n">sh</span>

<span class="c1"># for inference
</span><span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">generate_felix</span><span class="p">.</span><span class="n">sh</span>
<span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">generate_lincoln</span><span class="p">.</span><span class="n">sh</span>
<span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">generate_tesla</span><span class="p">.</span><span class="n">sh</span>
</pre></td></tr></tbody></table></code></pre></figure> <hr/> <blockquote> <h2 id="1-custom-inference">1. Custom inference</h2> </blockquote> <p>논문의 저자가 작년 CVPR 2024에 냈었던 FlowFace 방법론을 여기서 Flame Tracking이자 MMDM Estimator로 사용했었는데, 아직 해당 FlowFace Model의 코드 공개가 되어있지 않아 있는 상태라 Pixel3DMM tracking의 코드를 이용한다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/c503d688-c5ea-4b86-a0a0-c309a9b3661a/image.png" alt=""/></p> <p>해당 논문도 FLAME tracking을 지원하고 있다. 신기하게도 1저자 이신분이 GaussianAvatar와 Nersemble을 작성하셨던 분이었다.</p> <p>아래가 example로 제공된 felix 선생님으로 tracking을 돌려볼 수 있는 부분이다. 아마 돌리면 위와 같이 FLAME mesh를 추출할 수 있는 것 같다. (나중에 시간이 나면 해봐야겠다…)</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="n">export</span> <span class="n">PIXEL3DMM_PATH</span><span class="o">=</span><span class="err">$</span><span class="p">(</span><span class="n">realpath</span> <span class="sh">"</span><span class="s">../PATH/TO/pixel3dmm</span><span class="sh">"</span><span class="p">)</span>
<span class="n">export</span> <span class="n">CAP4D_PATH</span><span class="o">=</span><span class="err">$</span><span class="p">(</span><span class="n">realpath</span> <span class="sh">"</span><span class="s">./</span><span class="sh">"</span><span class="p">)</span> 

<span class="n">mkdir</span> <span class="n">examples</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">custom</span><span class="o">/</span>

<span class="c1"># For more information on arguments
</span><span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">track_video_pixel3dmm</span><span class="p">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">help</span>

<span class="c1"># Process a directory of (reference) images
</span><span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">track_video_pixel3dmm</span><span class="p">.</span><span class="n">sh</span> <span class="n">examples</span><span class="o">/</span><span class="nb">input</span><span class="o">/</span><span class="n">felix</span><span class="o">/</span><span class="n">images</span><span class="o">/</span><span class="n">cam0</span><span class="o">/</span> <span class="n">examples</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">custom</span><span class="o">/</span><span class="n">reference_tracking</span><span class="o">/</span>

<span class="c1"># Optional: process a driving (or reference) video
</span><span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">track_video_pixel3dmm</span><span class="p">.</span><span class="n">sh</span> <span class="n">examples</span><span class="o">/</span><span class="nb">input</span><span class="o">/</span><span class="n">animation</span><span class="o">/</span><span class="n">example_video</span><span class="p">.</span><span class="n">mp4</span> <span class="n">examples</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">custom</span><span class="o">/</span><span class="n">driving_video_tracking</span><span class="o">/</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>아래와 같은 cli로 MMDM을 통한 generated image를 할 수 있는 부분이다. <code class="language-plaintext highlighter-rouge">cap4d/inference/generate_images.py</code> 부분은 뒤에서 한 번 더 살펴봐야겠다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="c1"># Generate images with single reference image
</span><span class="n">python</span> <span class="n">cap4d</span><span class="o">/</span><span class="n">inference</span><span class="o">/</span><span class="n">generate_images</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config_path</span> <span class="n">configs</span><span class="o">/</span><span class="n">generation</span><span class="o">/</span><span class="n">default</span><span class="p">.</span><span class="n">yaml</span> <span class="o">--</span><span class="n">reference_data_path</span> <span class="n">examples</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">custom</span><span class="o">/</span><span class="n">reference_tracking</span><span class="o">/</span> <span class="o">--</span><span class="n">output_path</span> <span class="n">examples</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">custom</span><span class="o">/</span><span class="n">mmdm</span><span class="o">/</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>이후에 GaussianAvatars를 base로 하는 code를 기반으로 Fit Gaussian avatar를 진행한다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="n">python</span> <span class="n">gaussianavatars</span><span class="o">/</span><span class="n">train</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config_path</span> <span class="n">configs</span><span class="o">/</span><span class="n">avatar</span><span class="o">/</span><span class="n">default</span><span class="p">.</span><span class="n">yaml</span> <span class="o">--</span><span class="n">source_paths</span> <span class="n">examples</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">custom</span><span class="o">/</span><span class="n">mmdm</span><span class="o">/</span><span class="n">reference_images</span><span class="o">/</span> <span class="n">examples</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">custom</span><span class="o">/</span><span class="n">mmdm</span><span class="o">/</span><span class="n">generated_images</span><span class="o">/</span> <span class="o">--</span><span class="n">model_path</span> <span class="n">examples</span><span class="o">/</span><span class="n">output</span><span class="o">/</span><span class="n">custom</span><span class="o">/</span><span class="n">avatar</span><span class="o">/</span> <span class="o">--</span><span class="n">interval</span> <span class="mi">5000</span>
</pre></td></tr></tbody></table></code></pre></figure> <hr/> <blockquote> <h2 id="2-generate-images-with-mmdm">2. Generate Images (with MMDM)</h2> </blockquote> <p>해당 Section에서는 <code class="language-plaintext highlighter-rouge">cap4d/inference/generate_images.py</code> 부분에 대해 알아볼 예정이다. Diffusion 분야에 대해서는 Code딴으로 다뤄본적이 많지 않아서 자세하게는 못 볼 수 있지만 논문 본문과 비교하여 예상이 가는 곳과 특징적인 곳들을 살펴볼 예정이다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="kn">from</span> <span class="n">omegaconf</span> <span class="kn">import</span> <span class="n">OmegaConf</span>
<span class="n">gen_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">gen_config_path</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>omegaconf라는 라이브러리 모듈을 사용하면 config를 <code class="language-plaintext highlighter-rouge">.</code> 속성이 아닌 dictionary 스타일로 접근할 수도 있다고 한다.</p> <p>먼저 56번째 줄에서 본격적으로 이미지 생성을 위한 Dataset을 만들기 시작한다. 먼저 <code class="language-plaintext highlighter-rouge">GenerationDataset</code>에서는 생성할 이미지 sample수와 flame dict등에 대한 정보를 담고 있다</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre><span class="c1"># line 56
# ./generate_images.py
</span><span class="n">genset</span> <span class="o">=</span> <span class="nc">GenerationDataset</span><span class="p">(...)</span>
</pre></td></tr></tbody></table></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre><span class="c1">#./data/generation_data.py
</span><span class="k">class</span> <span class="nc">GenerationDataset</span><span class="p">(</span><span class="n">CAP4DInferenceDataset</span><span class="p">):</span>
	<span class="bp">...</span>
</pre></td></tr></tbody></table></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre><span class="c1">#./data/inference_data.py
</span><span class="k">class</span> <span class="nc">CAP4DInferenceDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
	<span class="bp">...</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>상속받고 있는 <code class="language-plaintext highlighter-rouge">CAP4DInferenceDataset</code>에서 이제 기본적인 전체적인 baseline형태의 dataset class를 제공한다. 여기서는 <code class="language-plaintext highlighter-rouge">__getitem__</code>에서 idx를 받으면 condition 정보들과 reference 정보를 dict형태로 반환해준다. 자세하게는 아래와 같은 형태로 반환해준다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre><span class="n">cond_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">out_crop_mask</span><span class="sh">"</span><span class="p">:</span> <span class="n">out_crop_mask</span><span class="p">[</span><span class="bp">None</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">reference_mask</span><span class="sh">"</span><span class="p">:</span> <span class="n">reference_mask</span><span class="p">[</span><span class="bp">None</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">ray_map</span><span class="sh">"</span><span class="p">:</span> <span class="n">ray_map</span><span class="p">[</span><span class="bp">None</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">verts_2d</span><span class="sh">"</span><span class="p">:</span> <span class="n">verts_2d</span><span class="p">[</span><span class="bp">None</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">offsets_3d</span><span class="sh">"</span><span class="p">:</span> <span class="n">offsets_3d</span><span class="p">[</span><span class="bp">None</span><span class="p">],</span>
<span class="p">}</span>  <span class="c1"># [None] is for fake time dimension
</span>
<span class="n">out_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">jpg</span><span class="sh">"</span><span class="p">:</span> <span class="n">img</span><span class="p">[</span><span class="bp">None</span><span class="p">],</span>  <span class="c1"># jpg names comes from controlnet implementation
</span>    <span class="sh">"</span><span class="s">hint</span><span class="sh">"</span><span class="p">:</span> <span class="n">cond_dict</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">flame_params</span><span class="sh">"</span><span class="p">:</span> <span class="n">flame_item</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">return</span> <span class="n">out_dict</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>윗 부분은 reference의 dataloader가 아닌 generation에 대한 data이다. 그렇다 보니 일종의 계산된 dataset을 사용해서 840개의 condition을 이용하는 느낌인 것 같다.</p> <p>이제 여기서부터 본격적으로 논문에 수도코드 형태로 제공된 것과 같이 StochasticIOSampler를 돌리는 것 같다. sample의 형태는 reference와 gen에 대한 condition과 여러 parameter들이 들어간다.</p> <p>해당 부분의 main generate 마지막 부분이다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="n">stochastic_io_sampler</span> <span class="o">=</span> <span class="nc">StochasticIOSampler</span><span class="p">(</span><span class="n">device_model_map</span><span class="p">)</span>

<span class="n">z_gen</span> <span class="o">=</span> <span class="n">stochastic_io_sampler</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span>
    <span class="n">S</span><span class="o">=</span><span class="n">gen_config</span><span class="p">[</span><span class="sh">"</span><span class="s">n_ddim_steps</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">ref_cond</span><span class="o">=</span><span class="n">ref_data</span><span class="p">[</span><span class="sh">"</span><span class="s">cond_frames</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">ref_uncond</span><span class="o">=</span><span class="n">ref_data</span><span class="p">[</span><span class="sh">"</span><span class="s">uncond_frames</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">gen_cond</span><span class="o">=</span><span class="n">gen_data</span><span class="p">[</span><span class="sh">"</span><span class="s">cond_frames</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">gen_uncond</span><span class="o">=</span><span class="n">gen_data</span><span class="p">[</span><span class="sh">"</span><span class="s">uncond_frames</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">latent_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">gen_config</span><span class="p">[</span><span class="sh">"</span><span class="s">resolution</span><span class="sh">"</span><span class="p">]</span> <span class="o">//</span> <span class="mi">8</span><span class="p">,</span> <span class="n">gen_config</span><span class="p">[</span><span class="sh">"</span><span class="s">resolution</span><span class="sh">"</span><span class="p">]</span> <span class="o">//</span> <span class="mi">8</span><span class="p">),</span>
    <span class="n">V</span><span class="o">=</span><span class="n">gen_config</span><span class="p">[</span><span class="sh">"</span><span class="s">V</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">R_max</span><span class="o">=</span><span class="n">gen_config</span><span class="p">[</span><span class="sh">"</span><span class="s">R_max</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">cfg_scale</span><span class="o">=</span><span class="n">gen_config</span><span class="p">[</span><span class="sh">"</span><span class="s">cfg_scale</span><span class="sh">"</span><span class="p">],</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <hr/> <blockquote> <h2 id="3-stochasticiosampler">3. StochasticIOSampler</h2> </blockquote> <p>해당 논문에서 어느 부분이 가장 중요하냐고 하면 나는 당연하게도.. MMDM을 학습할 때 특정 domaion의 detail을 위한 condition들이 라고 생각한다. 결국 MMDM이 완벽하게 잘 학습되었고 좋은 성능을 보여주기 때문에, 뒤에 부분들과 같이 4D Avatar로 만들어질 수 있는 것이라고 생각하기 때문이다.</p> <p>해당 논문에서도 MMDM의 중요성을 중심으로 설명해주고 있긴 하지만, 동시에 Stochastic부분도 그 다음으로 중요한 부분으로 설명하고 있다.</p> <p>해당 부분은 논문에서 의도한 바로는, 결국 <strong>reference images를 4장 밖에 사용하지 못하니 이를 diffusion의 각 timestep에 reference images를 4장씩 random으로 sampling하여 diffusion에 적용하여 더 general한 image들을 생성</strong>하자라는 목적이다.</p> <p>해당 Section에서는 기본적으로 DDIM을 기반으로 하는 Stochastic I/O conditioning에 대한 설명을 간략하게 해본다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="code"><pre><span class="c1"># cap4d/mmdm/sampler.py
</span><span class="k">class</span> <span class="nc">StochasticIOSampler</span><span class="p">(</span><span class="nb">object</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">sample</span><span class="p">(...)</span>
    <span class="n">Parameters</span><span class="p">:</span>
        <span class="nc">S </span><span class="p">(</span><span class="nb">int</span><span class="p">):</span> <span class="n">Number</span> <span class="n">of</span> <span class="n">diffusion</span> <span class="n">steps</span><span class="p">.</span>
        <span class="nf">ref_cond </span><span class="p">(</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]):</span> <span class="n">Conditioning</span> <span class="n">images</span> <span class="n">used</span> <span class="k">for</span> <span class="nf">reference </span><span class="p">(</span><span class="n">ref</span> <span class="n">latents</span><span class="p">,</span> <span class="n">pose</span> <span class="n">maps</span><span class="p">,</span> <span class="n">reference</span> <span class="n">masks</span> <span class="n">etc</span><span class="p">.).</span>
        <span class="nf">ref_uncond </span><span class="p">(</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]):</span> <span class="n">Unconditional</span> <span class="n">conditioning</span> <span class="n">images</span> <span class="n">used</span> <span class="k">for</span> <span class="nf">reference </span><span class="p">(</span><span class="n">zeroed</span> <span class="n">conditioning</span><span class="p">).</span>
        <span class="nf">gen_cond </span><span class="p">(</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]):</span> <span class="n">Conditioning</span> <span class="n">images</span> <span class="n">used</span> <span class="k">for</span> <span class="nf">reference </span><span class="p">(</span><span class="n">pose</span> <span class="n">maps</span><span class="p">,</span> <span class="n">reference</span> <span class="n">masks</span> <span class="n">etc</span><span class="p">.).</span>
        <span class="nf">gen_uncond </span><span class="p">(</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]):</span> <span class="n">Unconditional</span> <span class="n">conditioning</span> <span class="n">images</span> <span class="n">used</span> <span class="k">for</span> <span class="nf">reference </span><span class="p">(</span><span class="n">pose</span> <span class="n">maps</span><span class="p">,</span> <span class="n">reference</span> <span class="n">masks</span> <span class="n">etc</span><span class="p">.).</span>
        <span class="nf">latent_shape </span><span class="p">(</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span> <span class="n">Shape</span> <span class="n">of</span> <span class="n">the</span> <span class="n">latent</span> <span class="n">to</span> <span class="n">be</span> <span class="nf">generated </span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">).</span>
        <span class="nc">V </span><span class="p">(</span><span class="nb">int</span><span class="p">):</span> <span class="n">Number</span> <span class="n">of</span> <span class="n">views</span> <span class="n">supported</span> <span class="n">by</span> <span class="n">the</span> <span class="n">MMDM</span><span class="p">.</span>
        <span class="nc">R_max </span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">optional</span><span class="p">):</span> <span class="n">Maximum</span> <span class="n">number</span> <span class="n">of</span> <span class="n">reference</span> <span class="n">images</span> <span class="n">to</span> <span class="n">use</span><span class="p">.</span> <span class="n">Defaults</span> <span class="n">to</span> <span class="mf">4.</span>
        <span class="nf">cfg_scale </span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">optional</span><span class="p">):</span> <span class="n">Classifier</span><span class="o">-</span><span class="n">free</span> <span class="n">guidance</span> <span class="n">scale</span><span class="p">.</span> <span class="n">Higher</span> <span class="n">values</span> <span class="n">increase</span> <span class="n">conditioning</span> <span class="n">strength</span><span class="p">.</span> <span class="n">Defaults</span> <span class="n">to</span> <span class="mf">1.0</span><span class="p">.</span>
        <span class="nf">eta </span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">optional</span><span class="p">):</span> <span class="n">Noise</span> <span class="n">scaling</span> <span class="n">factor</span> <span class="k">for</span> <span class="n">DDIM</span> <span class="n">sampling</span><span class="p">.</span> <span class="mi">0</span> <span class="n">means</span> <span class="n">deterministic</span> <span class="n">sampling</span><span class="p">.</span> <span class="n">Defaults</span> <span class="n">to</span> <span class="mf">0.</span>
        <span class="nf">verbose </span><span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="n">optional</span><span class="p">):</span> <span class="n">Whether</span> <span class="n">to</span> <span class="k">print</span> <span class="n">detailed</span> <span class="n">logs</span> <span class="n">during</span> <span class="n">sampling</span><span class="p">.</span> <span class="n">Defaults</span> <span class="n">to</span> <span class="bp">False</span><span class="p">.</span>    
    <span class="n">Returns</span><span class="p">:</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span> <span class="n">A</span> <span class="n">tensor</span> <span class="n">representing</span> <span class="n">the</span> <span class="n">generated</span> <span class="nf">sample</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="ow">in</span> <span class="n">latent</span> <span class="n">space</span><span class="p">.</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>timestep은 기본적으로 250에서 1까지 reverse로 돌아가게 되고, <code class="language-plaintext highlighter-rouge">shuffle generated latents</code>를 수행하는 부분으로 np.random.permuation을 사용하는 것 같다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>	<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>
		<span class="n">index</span> <span class="o">=</span> <span class="n">total_steps</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span>
		<span class="n">ref_batches</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span>
        	<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">permutation</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n_all_ref</span><span class="p">))[:</span><span class="n">R</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_its</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>아래와 같이 batch 단위로 돌면서 위에서 자연스럽게 shuffle된 sampliing을 이용하여 ref와 gen의 condition들의 sample을 뽑아서 사용한다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre><span class="k">for</span> <span class="n">dev_batches</span> <span class="ow">in</span> <span class="n">batch_indices</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">dev_id</span><span class="p">,</span> <span class="n">dev_batch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">dev_batches</span><span class="p">):</span>
        <span class="n">dev_key</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device_model_map</span><span class="p">)[</span><span class="n">dev_id</span><span class="p">]</span>
        <span class="n">dev_device</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">device_model_map</span><span class="p">[</span><span class="n">dev_key</span><span class="p">].</span><span class="n">device</span>

        <span class="n">curr_ref_cond</span> <span class="o">=</span> <span class="nf">dict_sample</span><span class="p">(</span><span class="n">ref_cond</span><span class="p">,</span> <span class="n">ref_batches</span><span class="p">[</span><span class="n">dev_batch</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">dev_device</span><span class="p">)</span>
        <span class="n">curr_ref_uncond</span> <span class="o">=</span> <span class="nf">dict_sample</span><span class="p">(</span><span class="n">ref_uncond</span><span class="p">,</span> <span class="n">ref_batches</span><span class="p">[</span><span class="n">dev_batch</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">dev_device</span><span class="p">)</span>

        <span class="n">curr_gen_cond</span> <span class="o">=</span> <span class="nf">dict_sample</span><span class="p">(</span><span class="n">gen_cond</span><span class="p">,</span> <span class="n">gen_batches</span><span class="p">[</span><span class="n">dev_batch</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">dev_device</span><span class="p">)</span>
        <span class="n">curr_gen_uncond</span> <span class="o">=</span> <span class="nf">dict_sample</span><span class="p">(</span><span class="n">gen_uncond</span><span class="p">,</span> <span class="n">gen_batches</span><span class="p">[</span><span class="n">dev_batch</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">dev_device</span><span class="p">)</span>	
</pre></td></tr></tbody></table></code></pre></figure> <p>이제 위에서 sampling된 batch들을 이용하여 $\mathbf{Z}<em>{\text{ref}}^{\prime},\mathbf{C}</em>{\text{ref}}^{\prime},\mathbf{C}_{\text{gen}}^\prime$ 값들을 뽑아서 MMDM Model에 적용시켜 e_t를 얻어 latent를 이후에 한꺼번에 업데이트 하는 로직으로 동작하는 것 같다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre>  <span class="k">for</span> <span class="n">dev_id</span><span class="p">,</span> <span class="n">dev_batch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">dev_batches</span><span class="p">):</span>
      <span class="n">dev_key</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device_model_map</span><span class="p">)[</span><span class="n">dev_id</span><span class="p">]</span>
      <span class="n">dev_device</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">device_model_map</span><span class="p">[</span><span class="n">dev_key</span><span class="p">].</span><span class="n">device</span>
      <span class="n">model_uncond</span><span class="p">,</span> <span class="n">model_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">device_model_map</span><span class="p">[</span><span class="n">dev_key</span><span class="p">].</span><span class="nf">apply_model</span><span class="p">(</span>
                              <span class="n">x_in_list</span><span class="p">[</span><span class="n">dev_id</span><span class="p">],</span> 
                              <span class="n">t_in_list</span><span class="p">[</span><span class="n">dev_id</span><span class="p">],</span> 
                              <span class="n">c_in_list</span><span class="p">[</span><span class="n">dev_id</span><span class="p">],</span>
                              <span class="p">).</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">model_output</span> <span class="o">=</span> <span class="n">model_uncond</span> <span class="o">+</span> <span class="n">cfg_scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">model_t</span> <span class="o">-</span> <span class="n">model_uncond</span><span class="p">)</span>
      <span class="n">e_t</span> <span class="o">=</span> <span class="n">model_output</span><span class="p">[:,</span> <span class="n">R</span><span class="p">:]</span>  <span class="c1"># eps prediction mode, extract the generation samples starting at n_ref
</span>      <span class="n">e_t_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">e_t</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">dev_id</span><span class="p">,</span> <span class="n">dev_batch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">dev_batches</span><span class="p">):</span>
      <span class="n">all_e_t</span><span class="p">[</span><span class="n">gen_batches</span><span class="p">[</span><span class="n">dev_batch</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">e_t_list</span><span class="p">[</span><span class="n">dev_id</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">mem_device</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>이제 해당 부분이 한꺼번에 update하고 latent를 return하는 부분인 것 같은데 정확한 의미를 알지 못해서 일단 코드 첨부만 한다….</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="code"><pre><span class="n">alpha_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ddim_alphas</span><span class="p">.</span><span class="nf">float</span><span class="p">()[</span><span class="n">index</span><span class="p">]</span>
<span class="n">sqrt_one_minus_alpha_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ddim_sqrt_one_minus_alphas</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
<span class="n">sigma_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">ddim_sigmas</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
<span class="n">alpha_prev_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">ddim_alphas_prev</span><span class="p">).</span><span class="nf">float</span><span class="p">()[</span><span class="n">index</span><span class="p">]</span>

<span class="n">alpha_prev_t</span> <span class="o">=</span> <span class="n">alpha_prev_t</span><span class="p">.</span><span class="nf">double</span><span class="p">()</span>
<span class="n">sqrt_one_minus_alpha_t</span> <span class="o">=</span> <span class="n">sqrt_one_minus_alpha_t</span><span class="p">.</span><span class="nf">double</span><span class="p">()</span>
<span class="n">alpha_t</span> <span class="o">=</span> <span class="n">alpha_t</span><span class="p">.</span><span class="nf">double</span><span class="p">()</span>
<span class="n">alpha_prev_t</span> <span class="o">=</span> <span class="n">alpha_prev_t</span><span class="p">.</span><span class="nf">double</span><span class="p">()</span>

<span class="n">e_t_factor</span> <span class="o">=</span> <span class="o">-</span><span class="n">alpha_prev_t</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">()</span> <span class="o">*</span> <span class="n">sqrt_one_minus_alpha_t</span> <span class="o">/</span> <span class="n">alpha_t</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alpha_prev_t</span> <span class="o">-</span> <span class="n">sigma_t</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nf">sqrt</span><span class="p">()</span>
<span class="n">x_t_factor</span> <span class="o">=</span> <span class="n">alpha_prev_t</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">alpha_t</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">()</span> 

<span class="n">e_t_factor</span> <span class="o">=</span> <span class="n">e_t_factor</span><span class="p">.</span><span class="nf">float</span><span class="p">()</span>
<span class="n">x_t_factor</span> <span class="o">=</span> <span class="n">x_t_factor</span><span class="p">.</span><span class="nf">float</span><span class="p">()</span>

<span class="n">all_x_T</span> <span class="o">=</span> <span class="n">all_x_T</span> <span class="o">*</span> <span class="n">x_t_factor</span> <span class="o">+</span> <span class="n">all_e_t</span> <span class="o">*</span> <span class="n">e_t_factor</span>
<span class="k">return</span> <span class="n">all_x_T</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Code Review]]></summary></entry><entry><title type="html">[Papers] GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians (CVPR 2024 Highlight)</title><link href="https://oweixx.github.io/blog/2025/papers_GaussianAvatars/" rel="alternate" type="text/html" title="[Papers] GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians (CVPR 2024 Highlight)"/><published>2025-11-17T00:00:00+00:00</published><updated>2025-11-17T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_GaussianAvatars</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_GaussianAvatars/"><![CDATA[<h2 id="gaussianavatars-photorealistic-head-avatars-with-rigged-3d-gaussians">GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</h2> <h3 id="papergithubproject"><a href="https://arxiv.org/abs/2312.02069">[Paper]</a><a href="https://github.com/ShenhanQian/GaussianAvatars">[Github]</a><a href="https://shenhanqian.github.io/gaussian-avatars">[Project]</a></h3> <blockquote> <p><strong>Title:</strong> GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians<br/> <strong>Journal name &amp; Publication Date:</strong> CVPR 2023-12-04 <br/> <strong>Authors:</strong> Shenhan Qian, Tobias Kirschstein, Liam Schoneveld</p> </blockquote> <hr/> <blockquote> <h2 id="0-parameters--functions">0. Parameters &amp; Functions</h2> </blockquote> <p>먼저 아래와 같이 train.py에서 gaussian을 학습한다고 하면, 그냥 주의할 parsing으로는 <code class="language-plaintext highlighter-rouge">bind_to_mesh</code> 정도인 것 같다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre><span class="n">SUBJECT</span><span class="o">=</span><span class="mi">306</span>
<span class="n">python</span> <span class="n">train</span><span class="p">.</span><span class="n">py</span> \
<span class="o">-</span><span class="n">s</span> <span class="n">data</span><span class="o">/</span><span class="n">UNION10_</span><span class="err">$</span><span class="p">{</span><span class="n">SUBJECT</span><span class="p">}</span><span class="n">_EMO1234EXP234589_v16_DS2</span><span class="o">-</span><span class="mf">0.5</span><span class="n">x_lmkSTAR_teethV3_SMOOTH_offsetS_whiteBg_maskBelowLine</span> \
<span class="o">-</span><span class="n">m</span> <span class="n">output</span><span class="o">/</span><span class="n">UNION10EMOEXP_</span><span class="err">$</span><span class="p">{</span><span class="n">SUBJECT</span><span class="p">}</span><span class="n">_eval_600k</span> \
<span class="o">--</span><span class="nb">eval</span> <span class="o">--</span><span class="n">bind_to_mesh</span> <span class="o">--</span><span class="n">white_background</span> <span class="o">--</span><span class="n">port</span> <span class="mi">60000</span>
</pre></td></tr></tbody></table></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre><span class="k">if</span> <span class="n">dataset</span><span class="p">.</span><span class="n">bind_to_mesh</span><span class="p">:</span>
	<span class="n">gaussians</span> <span class="o">=</span> <span class="nc">FlameGaussianModel</span><span class="p">(...)</span>
    <span class="n">mesh_renderer</span> <span class="o">=</span> <span class="nc">NVDiffRenderer</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>training 초반 <code class="language-plaintext highlighter-rouge">bind_to_mesh</code> argument를 통해 아마 강제적으로 실행하는 부분인 것 같다. train에서 사용하는 gaussians는 FlameGaussianModel로 초기화 한다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="c1"># scene/flame_gaussian_model.py
</span><span class="k">class</span> <span class="nc">FlameGaussianModel</span><span class="p">(</span><span class="n">GaussianModel</span><span class="p">):</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>애초에 FlameGaussianModel은 GaussianModel을 상속받고 있기 때문에 GaussianModel 자체도 봐야하는 것 같다. 아마 FlameGaussianModel은 Flame Model과 관련된 부분들 관련한 function, parameters 들을 관리하려고 만들어 놓은 것 같다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre><span class="n">self</span><span class="p">.</span><span class="n">face_center</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">self</span><span class="p">.</span><span class="n">face_scaling</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">self</span><span class="p">.</span><span class="n">face_orien_mat</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">self</span><span class="p">.</span><span class="n">face_orien_quat</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">self</span><span class="p">.</span><span class="n">binding</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">self</span><span class="p">.</span><span class="n">binding_counter</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">self</span><span class="p">.</span><span class="n">timestep</span> <span class="o">=</span> <span class="bp">None</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>scene/gaussian_model.py에 늘 익숙한 parameter들을 제외하고 어색한 parameter들이 보인다. 해당 parameter들이 mesh binding에 필요한 GaussianModel parameter인 것 같다. 논문에서도 볼 수 있는 triangle mesh의 global parameter들을 의미하는 것 같다. binding에서 gaussian과 binding되는 부모 triangle mesh의 index를 부여하는 것 같다. 만약 None이라면 binding되지 않은 gaussian인 것 같다. binding_counter 역시 해당 gaussian이 묶여있는 triangle에서의 gaussian counter인 것 같은데, 이는 pruning을 할 때 mesh에서 1개 이상의 gaussian을 유지한다는 정책을 이행하기 위한 부분인 것 같다.</p> <h3 id="3d-gaussian-parameters">3D Gaussian parameters</h3> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">get_scaling</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">binding</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">scaling_activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_scaling</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">face_scaling</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">select_mesh_by_timestep</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">scaling</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">scaling_activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_scaling</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scaling</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">face_scaling</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">binding</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>gaussian parameter 부분의 global scaling 같은 경우는 face scaling k와 local gaussian parameter s와의 곱 연산으로 이루어진다.</p> \[\mathbf{s}' = k\mathbf{s}\] <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">get_xyz</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">binding</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">_xyz</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">face_center</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">select_mesh_by_timestep</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">xyz</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">face_orien_mat</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">binding</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">_xyz</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">]).</span><span class="nf">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">xyz</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">face_scaling</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">binding</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">face_center</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">binding</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>gaussian의 global position도 아래와 같은 triangle의 global parameter와 local gaussian parameter의 조합으로 이루어 진다. self.binding 자체가 Gaussian별로 대응되는 triangle의 파라미터를 한 번에 gather해서 연산한다</p> \[\mathbf{\mu}' = k\mathbf{R}\mathbf{\mu} + \mathbf{T}\] <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">get_rotation</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">binding</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">rotation_activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_rotation</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">face_orien_quat</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">select_mesh_by_timestep</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># always need to normalize the rotation quaternions before chaining them
</span>        <span class="n">rot</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">rotation_activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_rotation</span><span class="p">)</span>
        <span class="n">face_orien_quat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">rotation_activation</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">face_orien_quat</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">binding</span><span class="p">])</span>
        <span class="k">return</span> <span class="nf">quat_xyzw_to_wxyz</span><span class="p">(</span><span class="nf">quat_product</span><span class="p">(</span><span class="nf">quat_wxyz_to_xyzw</span><span class="p">(</span><span class="n">face_orien_quat</span><span class="p">),</span> <span class="nf">quat_wxyz_to_xyzw</span><span class="p">(</span><span class="n">rot</span><span class="p">)))</span>  <span class="c1"># roma
</span>        <span class="c1"># return quaternion_multiply(face_orien_quat, rot)  # pytorch3d</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>rotation도 같은 개념으로 사용된다.</p> \[\mathbf{r}' = \mathbf{R}\mathbf{r}\] <hr/> <blockquote> <h2 id="1-training-iteration">1. Training Iteration</h2> </blockquote> <p>해당 부분에서는 train.py의 61번째 줄 부터 시작하는 train iteration에서의 부분을 살펴본다. 기본 GaussianModel과 같은 부분은 생략하고 특징적인 부분만 순차적으로 살펴본다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre><span class="c1"># 118 line
</span><span class="k">if</span> <span class="n">gaussians</span><span class="p">.</span><span class="n">binding</span> <span class="o">!=</span> <span class="bp">None</span><span class="p">:</span>
	<span class="n">gaussians</span><span class="p">.</span><span class="nf">select_mesh_by_timestep</span><span class="p">(...)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>먼저 118번째 줄의 if문인데 초반에 FlameGaussianModel이 init되면서 bbinding을 flame_model.faces수만큼 초기화 되고 counter도 각 face마다 1로 초기화 되게 된다. 이 부분으로 인해 gaussians.binding은 None값을 가지지는 않으니 해당 부분이 실행된다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre></td><td class="code"><pre>    <span class="k">def</span> <span class="nf">select_mesh_by_timestep</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">original</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">timestep</span> <span class="o">=</span> <span class="n">timestep</span>
        <span class="n">flame_param</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">flame_param_orig</span> <span class="k">if</span> <span class="n">original</span> <span class="ow">and</span> <span class="n">self</span><span class="p">.</span><span class="n">flame_param_orig</span> <span class="o">!=</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">self</span><span class="p">.</span><span class="n">flame_param</span>

        <span class="n">verts</span><span class="p">,</span> <span class="n">verts_cano</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">flame_model</span><span class="p">(</span>
            <span class="n">flame_param</span><span class="p">[</span><span class="sh">'</span><span class="s">shape</span><span class="sh">'</span><span class="p">][</span><span class="bp">None</span><span class="p">,</span> <span class="p">...],</span>
            <span class="n">flame_param</span><span class="p">[</span><span class="sh">'</span><span class="s">expr</span><span class="sh">'</span><span class="p">][[</span><span class="n">timestep</span><span class="p">]],</span>
            <span class="n">flame_param</span><span class="p">[</span><span class="sh">'</span><span class="s">rotation</span><span class="sh">'</span><span class="p">][[</span><span class="n">timestep</span><span class="p">]],</span>
            <span class="n">flame_param</span><span class="p">[</span><span class="sh">'</span><span class="s">neck_pose</span><span class="sh">'</span><span class="p">][[</span><span class="n">timestep</span><span class="p">]],</span>
            <span class="n">flame_param</span><span class="p">[</span><span class="sh">'</span><span class="s">jaw_pose</span><span class="sh">'</span><span class="p">][[</span><span class="n">timestep</span><span class="p">]],</span>
            <span class="n">flame_param</span><span class="p">[</span><span class="sh">'</span><span class="s">eyes_pose</span><span class="sh">'</span><span class="p">][[</span><span class="n">timestep</span><span class="p">]],</span>
            <span class="n">flame_param</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][[</span><span class="n">timestep</span><span class="p">]],</span>
            <span class="n">zero_centered_at_root_node</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="n">return_landmarks</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="n">return_verts_cano</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">static_offset</span><span class="o">=</span><span class="n">flame_param</span><span class="p">[</span><span class="sh">'</span><span class="s">static_offset</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">dynamic_offset</span><span class="o">=</span><span class="n">flame_param</span><span class="p">[</span><span class="sh">'</span><span class="s">dynamic_offset</span><span class="sh">'</span><span class="p">][[</span><span class="n">timestep</span><span class="p">]],</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">update_mesh_properties</span><span class="p">(</span><span class="n">verts</span><span class="p">,</span> <span class="n">verts_cano</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">update_mesh_properties</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">verts</span><span class="p">,</span> <span class="n">verts_cano</span><span class="p">):</span>
        <span class="n">faces</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">flame_model</span><span class="p">.</span><span class="n">faces</span>
        <span class="n">triangles</span> <span class="o">=</span> <span class="n">verts</span><span class="p">[:,</span> <span class="n">faces</span><span class="p">]</span>

        <span class="c1"># position
</span>        <span class="n">self</span><span class="p">.</span><span class="n">face_center</span> <span class="o">=</span> <span class="n">triangles</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># orientation and scale
</span>        <span class="n">self</span><span class="p">.</span><span class="n">face_orien_mat</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">face_scaling</span> <span class="o">=</span> <span class="nf">compute_face_orientation</span><span class="p">(</span><span class="n">verts</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">faces</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">return_scale</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># self.face_orien_quat = matrix_to_quaternion(self.face_orien_mat)  # pytorch3d (WXYZ)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">face_orien_quat</span> <span class="o">=</span> <span class="nf">quat_xyzw_to_wxyz</span><span class="p">(</span><span class="nf">rotmat_to_unitquat</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">face_orien_mat</span><span class="p">))</span>  <span class="c1"># roma
</span>
        <span class="c1"># for mesh rendering
</span>        <span class="n">self</span><span class="p">.</span><span class="n">verts</span> <span class="o">=</span> <span class="n">verts</span>
        <span class="n">self</span><span class="p">.</span><span class="n">faces</span> <span class="o">=</span> <span class="n">faces</span>

        <span class="c1"># for mesh regularization
</span>        <span class="n">self</span><span class="p">.</span><span class="n">verts_cano</span> <span class="o">=</span> <span class="n">verts_cano</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>select_mesh_by_timestep은 4D 특성상 timestep이 변할 때 마다 flame_model의 parameter들로 vertex와 triangle, faces에 대한 정보들을 받아오는 것 같다. 동시에 update_mesh_properties를 통해 mesh 정보들(verts, faces…) update를 진행한다.</p> <p>이후에는 loss dict를 이용해서 계산한다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="n">losses</span><span class="p">[</span><span class="sh">'</span><span class="s">l1</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">l1_loss</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">gt_image</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">opt</span><span class="p">.</span><span class="n">lambda_dssim</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="n">losses</span><span class="p">[</span><span class="sh">'</span><span class="s">ssim</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="nf">ssim</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">gt_image</span><span class="p">))</span> <span class="o">*</span> <span class="n">opt</span><span class="p">.</span><span class="n">lambda_dssim</span>
</pre></td></tr></tbody></table></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="n">losses</span><span class="p">[</span><span class="sh">'</span><span class="s">xyz</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">((</span><span class="n">gaussians</span><span class="p">.</span><span class="n">_xyz</span><span class="o">*</span><span class="n">gaussians</span><span class="p">.</span><span class="n">face_scaling</span><span class="p">[</span><span class="n">gaussians</span><span class="p">.</span><span class="n">binding</span><span class="p">])[</span><span class="n">visibility_filter</span><span class="p">]</span> <span class="o">-</span> <span class="n">opt</span><span class="p">.</span><span class="n">threshold_xyz</span><span class="p">).</span><span class="nf">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span> <span class="o">*</span> <span class="n">opt</span><span class="p">.</span><span class="n">lambda_xyz</span>
</pre></td></tr></tbody></table></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="n">losses</span><span class="p">[</span><span class="sh">'</span><span class="s">scale</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">gaussians</span><span class="p">.</span><span class="n">get_scaling</span><span class="p">[</span><span class="n">visibility_filter</span><span class="p">]</span> <span class="o">-</span> <span class="n">opt</span><span class="p">.</span><span class="n">threshold_scale</span><span class="p">).</span><span class="nf">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span> <span class="o">*</span> <span class="n">opt</span><span class="p">.</span><span class="n">lambda_scale</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>논문에서 아래와 같은 loss항들이 열겨되는데 해당 부분을 계산하는데 해당 부분을 loss dict형태로 계산하는 부분인 것 같다.</p> \[\mathcal{L}_{\text{rgb}} = (1-\lambda)\mathcal{L}_1 + \lambda\mathcal{L}_{\text{D-SSIM}}\\ \mathcal{L}_{\text{position}} = \lvert\lvert \text{max}(\mu, \epsilon_{\text{position}}) \rvert\rvert_2 \\ \mathcal{L}_{\text{scaling}} = \lvert\lvert \text{max}(\mu, \epsilon_{\text{scaling}}) \rvert\rvert_2 \\ \mathcal{L} = \mathcal{L}_{\text{rgb}} + \lambda_{\text{position}}\mathcal{L}_{\text{position}} + \lambda_{\text{scaling}}\mathcal{L}_{\text{scaling}}\]]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Code Review]]></summary></entry><entry><title type="html">[Papers] Learning a model of facial shape and expression from 4D scans (SIGGRAPH 2017)</title><link href="https://oweixx.github.io/blog/2025/papers_FLAME/" rel="alternate" type="text/html" title="[Papers] Learning a model of facial shape and expression from 4D scans (SIGGRAPH 2017)"/><published>2025-11-16T00:00:00+00:00</published><updated>2025-11-16T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_FLAME</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_FLAME/"><![CDATA[<h2 id="learning-a-model-of-facial-shape-and-expression-from-4d-scans">Learning a model of facial shape and expression from 4D scans</h2> <h3 id="papergithubproject"><a href="https://dl.acm.org/doi/10.1145/3130800.3130813">[Paper]</a><a href="https://github.com/soubhiksanyal/FLAME_PyTorch">[Github]</a><a href="https://flame.is.tue.mpg.de/">[Project]</a></h3> <blockquote> <p><strong>Title:</strong> Learning a model of facial shape and expression from 4D scans<br/> <strong>Journal name &amp; Publication Date:</strong> SIGGRAPH 2017-11-16<br/> <strong>Authors:</strong> Tianye Li, Timo Bolkart, Michael J. Black</p> </blockquote> <hr/> <blockquote> <h2 id="1-abstract--introduction">1. Abstract &amp; Introduction</h2> </blockquote> <p>3D Face Modeling에서 face capture를 통한 아주 detail한 face reconstruction 방법이 있지만 이는 종종 mesh를 제대로 구현하지 못하는 artifacts들이 발생한다. (맨 위의 그림) 반대로 low resolution일 경우 face expression에 대한 표현이 제대로 이루어지지 못하는 현상이 발생하게 된다. (맨 아래) 해당 논문에서는 이 지점들의 middle ground를 목표로 하는 FLAME model (Faces Learned with an Articulated Model and Expressions)를 제안한다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/373cfb1e-268a-48fa-9b94-1a532e10d947/image.png" alt=""/></p> <ul> <li>FLAME은 low-dimensional이지만 기존의 FaceWarehouse, Basel face model보다 more expressive한 FLAME Model을 제안한다.</li> <li>FLAME Model은 더 정확하며 추후 연구에 활용될 수 있게 더 적합한 모델이다.</li> <li>Face mesh model을 parameterization하여 더 유연하게 다룰 수 있게 만들었다.</li> </ul> <hr/> <blockquote> <h2 id="3-model-formulation">3. Model Formulation</h2> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/49220311-d522-4d52-8ee3-2097b6f0450c/image.png" alt=""/></p> <p>Flame mesh model은 기본적으로 SMPL model formulation을 따른다. SMPL이 Facial에 집중적으로 formulation 되어 있지는 않지만, 이를 이용하면 더 computationally efficient하며 더 경쟁력 있는 부분이라 사용했다고 한다.</p> <p>Flame은 정확한 blendshapes을 위해 linear blend skinning(LBS)를 사용하고 $N = 5023$ 개의 vertices(정점), $K = 4$ 개의 joints(neck, jaw and eyeballs)을 사용한다고 한다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/7041c752-51ac-4538-9ee9-e9a101bcd712/image.png" width="900"/></p> <blockquote> <h3 id="parameter-descrition">Parameter descrition</h3> <ul> <li><strong>shape coefficients:</strong> $\vec\beta \in \R^{\left\vert \vec\beta \right\vert}$</li> <li><strong>pose coefficients:</strong> $\vec\theta \in \R^{\left\vert \vec\theta \right\vert}$</li> <li><strong>expression coefficients:</strong> $\psi \in \R^{\left\vert \vec\psi \right\vert}$</li> <li>$J(\vec{\beta})$: shape에 따라 달라지는 joint 위치</li> <li>$\mathcal{W}$: 각 vertex에 대한 skinning weight 행렬</li> </ul> </blockquote> <p>아래는 최종 FLAME Model에 대한 equation이다. $$ T_P(\vec{\beta}, \vec{\theta}, \vec{\psi}) = \bar{T}</p> <ul> <li>B_S(\vec{\beta}; \mathcal{S})</li> <li>B_P(\vec{\theta}; \mathcal{P})</li> <li>B_E(\vec{\psi}; \mathcal{E}). $$ 해당 부분 부터 보자면, 기본 모델 $\bar{T}$에서 model을 변형하는 parameter shape, pose, expression에 대한 parameter들을 통해 blendshape을 하여 변형된 model을 반환하게 된다.</li> </ul> \[M(\vec{\beta}, \vec{\theta}, \vec{\psi}) = W\big(T_P(\vec{\beta}, \vec{\theta}, \vec{\psi}), J(\vec{\beta}), \vec{\theta}, \mathcal{W}\big),\] <p>그렇게 반환된 mesh model $T_P$는 관절 위치 Joint와 관절 회전, blendshape 가중치와 함께 최종 모델로 변환이 된다.</p> <p>여기까지는 매우 간단한 설명이었고, 이후 부터는 해당 model을 직접적으로 변형하는 blendshape에 대한 설명이다. 해당 부분에 대해서는 직접적으로 이해가 되는 부분들이 많지 않아서 gpt와 함께 확인하였다.</p> <h3 id="blendshapes">blendshapes</h3> <p><strong>Shape blendshapes</strong></p> \[B_S(\vec{\beta}; \mathcal{S}) = \sum_{n=1}^{|\beta|} \beta_n \, \mathbf{S}_n,\] \[\vec{\beta} = [\beta_1, \dots, \beta_{|\beta|}]^\mathsf{T}, \quad \mathcal{S} = [\mathbf{S}_1, \dots, \mathbf{S}_{|\beta|}] \in \mathbb{R}^{3N \times |\beta|}.\] <p>$\vec{\beta}$는 사람의 얼굴형을 결정하는 shape coefficient이고 $\mathbf{S}$는 n번째 vertices의 basis로 각 basis 방향으로 $\beta_n$만큼의 이동하는 형태를 의미한다.</p> <p><strong>Pose blendshapes</strong></p> \[R(\vec{\theta}) : \mathbb{R}^{|\theta|} \to \mathbb{R}^{9K},\] <p>회전 함수 $R(\vec\theta)$ joint의 회전 행렬 요소들을 한 벡터로 정의한다.</p> \[B_P(\vec{\theta}; \mathcal{P}) = \sum_{n=1}^{9K} \big( R_n(\vec{\theta}) - R_n(\vec{\theta}^\ast) \big)\, \mathbf{P}_n,\] \[\mathcal{P} = [\mathbf{P}_1, \dots, \mathbf{P}_{9K}] \in \mathbb{R}^{3N \times 9K}.\] <p>zero pose를 의미하는 $R_n(\vec{\theta}^\ast)$와 현재포즈 사이에서의 차이를 통해 얼마나 회전 행렬이 바뀌었는지를 계산하고, 회전 요소가 변할 때 생기는 vertex 보정 $\mathbf{P}_n$을 선형조합해 Pose를 표현하게 된다.</p> <p><strong>Expression blendshapes</strong> Expression blendshape은 “웃음, 찡그림, 놀람”과 같은 표정의 표현에 대한 부분이고 <strong>non-rigid facial deformation</strong>을 설명하는 pose와 독립적인 공간이다.</p> \[B_E(\vec{\psi}; \mathcal{E}) = \sum_{n=1}^{|\psi|} \psi_n \, \mathbf{E}_n,\] <p>$\mathbf{E}_n$은 특정 표정 방향에 해당하는 expression basis에 해당하여 여러 표정 basis들을 가중치로 섞어서 만든 expression deformation을 의미하는 blendshape이다.</p> \[\vec{\psi} = [\psi_1, \dots, \psi_{|\psi|}]^\mathsf{T}, \quad \mathcal{E} = [\mathbf{E}_1, \dots, \mathbf{E}_{|\psi|}] \in \mathbb{R}^{3N \times |\psi|}.\] <p>$\mathcal{E}$는 orthonormal expression basis를 의미한다고 한다.</p> <hr/> <blockquote> <h2 id="4-temporal-registration">4. Temporal Registration</h2> </blockquote> <h3 id="initial-model">Initial model</h3> <p>FLAME model은 shape ${\bar{T},\mathcal{S}}$, pose ${\mathcal{P}, \mathcal{W}, \mathcal{J}}$, expression $\mathcal{E}$ parameter들을 통해 변형이 되기 때문에 이들에게 일관성있게 변화해야한다.</p> <p><strong>Shape</strong> 초반의 initial model을 만들기 위해 SMPL의 full-body model에서 head model을 refined하여 이용하였다고 한다. 더 안정적이고 visual quality를 위해 눈 양쪽에 eyeball을 추가하여 현실성과 디테일을 더하였다.</p> <p><strong>Pose</strong> blendweights $\mathcal{W}$와 joint regressor$\mathcal{J}$는 사용자에 의해 정해지며, eyeball의 pose는 기본적으로 geometric center에 해당하는 pose로 initial된다.</p> <p><strong>Expression</strong> expression parameter $\mathcal{E}$를 초기화하기 위해 기본 head와 artist generated FACS-based blendshape과 비교하여 초기화를 하고, deformation transfer를 통해 model 변환을 하게 된다.</p> <h3 id="single-frame-registration">Single-frame registration</h3> <p>위에서 정의한 FLAME Model을 실제 3D Scan, Multi-View image에 대하여 어떻게 맞추는지에 대한 일련의 과정을 설명한다.</p> <p><strong>Our model-baed registration of a face scan consists of three steps.</strong> <strong>Model-only -&gt; Coupled -&gt; Texture-based</strong></p> <p><strong>1. Model-only</strong></p> \[E(\vec{\beta}, \vec{\theta}, \vec{\psi}) = E_D + \lambda_L E_L + E_P.\] <p>먼저 model coefficients만을 이용하여 scan과 landmark가 잘 맞을 수 있게 대략적인 optimizing을 하는 단계이다.</p> \[E_D = \lambda_D \sum_{v_s \in \mathcal{S}} \rho \!\left( \min_{v_m \in \mathcal{M}(\vec{\beta}, \vec{\theta}, \vec{\psi})} \left\| v_s - v_m \right\|_2 \right),\] <p>스캔의 각 점을 FLAME mesh 표면에 closest point로 매칭시키고 robust penalty를 걸어 outlier에 강건할 수 있게 한다.</p> \[E_L = \sum_{i=1}^{N_L} \left\| \Pi\!\big(\mathbf{v}_i(\vec{\beta}, \vec{\theta}, \vec{\psi})\big) - \mathbf{l}_i \right\|_2^2,\] <p>multi-view에서 landmark가 잘 align 되도록 하는 역할을 한다.</p> \[E_P = \lambda_{\theta} E_{\theta} + \lambda_{\beta} E_{\beta} + \lambda_{\psi} E_{\psi},\] <p>Prior term $E_P$에서는 파라미터들이 너무 큰 값으로 튀지 않게 정규항을 사용한다.</p> <p><strong>2. Coupled</strong></p> \[E(T, \vec{\beta}, \vec{\theta}, \vec{\psi}) = E_D + E_C + E_R + E_P.\] <p>이번엔 model이 설명하지 못하는 부분에 대하여 T가 deform될 수 있도록 하는 부분이다.</p> \[E_C = \sum_{e} \lambda_e \left\| T_e - M(\vec{\beta}, \vec{\theta}, \vec{\psi})_e \right\|_2^2,\] <p>단순히 vertex의 위치 차이를 줄이는 것 보다 edge 차이를 줄이는 것을 optimize objective로 두어 더 자연스럽고 매끄러운 deformation을 할 수 있게 한다.</p> \[E_R = \sum_{k} \lambda_k \left\| U(\mathbf{v}_k) \right\|_2^2,\] \[U(\mathbf{v}_k) = \frac{1}{|\mathcal{N}(k)|} \sum_{r \in \mathcal{N}(k)} (\mathbf{v}_r - \mathbf{v}_k),\] <p>outlier, fold-over나 찌그러지는 현상을 피하고, noisy regularization을 위해 사용한다.</p> <p><strong>3. Texture-based</strong></p> \[E(T, \vec{\beta}, \vec{\theta}, \vec{\psi}) = E_D + E_C + \lambda_T E_T + E_R + E_P.\] <p>앞에 두 단계에서는 geometry (scan, landmark)기반의 optimize 기반이의 과정이었다. 실제 detail에 관한 부분을 보강하고 expression/pose를 더 정밀하게 optimize한다.</p> \[E_T = \sum_{l = 0}^{L-1} \sum_{v = 1}^{V} \left\| \Gamma\!\big(I_l^{(v)}\big) - \Gamma\!\big(\hat{I}_l^{(v)}\big) \right\|_F^2,\] <p>직관적으로는 실제 이미지와 렌더된 이미지가 비슷해지도록 template parameter들을 조정하고, 해당 과정을 coarse level에서 큰 misalignment를 먼저 맞추고 finer level에서 detail들을 refine할 수 있게 유도한다.</p> <h3 id="sequential-registration">Sequential registration</h3> <p>여기서는 두 section으로 나뉜다. Personalization에서 개인화된 template을 생성하고, Sequence fitting에서 해당 template을 부드럽게 프레임별로 움직일 수 있도록 해준다.</p> <p><strong>Personalization</strong> multiple sequence에서 neutral pose, expression을 구하기 위해 $T_i$의 average를 구한다. 그다음 neutral result결과 중 하나의 $T_i$를 랜덤으로 골라 UV texture map을 만들고 이는 나중에 texture-based registration에서 사용된다.</p> <p><strong>Sequence fitting</strong> 간단하게 말해 그 template을 고정한 채, 이전 프레임 결과로 초기화된 single-frame registration을 프레임마다 반복해서 시간적으로 일관성을 가진 4D Face sequences를 등록하게 된다.</p> <hr/> <hr/>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Paper Review]]></summary></entry><entry><title type="html">[Papers] 3D Gaussian Splatting for Real-Time Radiance Field Rendering (SIGGRAPH 2023)</title><link href="https://oweixx.github.io/blog/2025/papers_3dgaussian/" rel="alternate" type="text/html" title="[Papers] 3D Gaussian Splatting for Real-Time Radiance Field Rendering (SIGGRAPH 2023)"/><published>2025-04-01T00:00:00+00:00</published><updated>2025-04-01T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_3dgaussian</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_3dgaussian/"><![CDATA[<h2 id="3d-gaussian-splatting-for-real-time-radiance-field-rendering">3D Gaussian Splatting for Real-Time Radiance Field Rendering</h2> <h3 id="papergithubdemo"><a href="https://arxiv.org/abs/2308.04079">[Paper]</a><a href="https://github.com/graphdeco-inria/gaussian-splatting">[Github]</a><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">[DEMO]</a></h3> <blockquote> <p><strong>Title:</strong> 3D Gaussian Splatting for Real-Time Radiance Field Rendering<br/> <strong>Journal name &amp; Publication Date:</strong> SIGGRAPH 2023-08-08<br/> <strong>First and Last Authors:</strong> Kerbl, Bernhard</p> </blockquote> <hr/> <blockquote> <h2 id="about-3d-gaussian">About 3D Gaussian</h2> <p>논문을 들어가기전 Gaussian 및 3D Gaussian에 대한 기본적인 개념들을 복기하고 정리하는 부분.</p> </blockquote> <p><strong>Gaussian</strong><br/> 가우시안은 확률에서 정규분포의 확률 밀도 함수(PDF)를 나타낼 때 주로 사용되는 함수이다. <strong>평균, 분산, 표준편차만으로 정의될 수 있다는 것이 특징</strong>이다.</p> \[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\] <ul> <li>$\mu$: 평균</li> <li>$\sigma^2$: 분산 (표준편차 $\sigma$의 제곱)</li> <li>$x$: 확률 변수</li> </ul> <p><strong>특징:</strong> 평균을 중심으로 대칭적으로 분포하며, 분산이 클수록 데이터가 더 넓게 퍼지며 분포의 첨도(kurtosis)가 낮아진다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/11b063d4-5180-4522-8e49-bd65f11b97f8/image.png" alt=""/></p> <p><strong>공분산(Covariance)</strong><br/> 공분산이란 두 개 이상의 변수 간의 상관 관계를 수량화하는 척도이다. 데이터의 분산이 한 변수에서 다른 변수로 어떻게 함께 변화하는지, 어떤 관계를 갖고 있는지를 나타낸다.</p> <p>**공분산 행렬$\sum$(Covariance Matrix) **<br/> 공분산 행렬은 다변량 데이터(여러 변수)에서 각 변수 간 공분산을 행렬 형태로 나타낸 행렬이다.</p> \[\Sigma = \begin{bmatrix} \text{Var}(X_1) &amp; \text{Cov}(X_1, X_2) &amp; \cdots &amp; \text{Cov}(X_1, X_n) \\ \text{Cov}(X_2, X_1) &amp; \text{Var}(X_2) &amp; \cdots &amp; \text{Cov}(X_2, X_n) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \text{Cov}(X_n, X_1) &amp; \text{Cov}(X_n, X_2) &amp; \cdots &amp; \text{Var}(X_n) \end{bmatrix}\] <ul> <li>$\text{Var}(X_i)$: 변수 $X_i$의 분산$(\sigma_i^2)$</li> <li>$\text{Cov}(X_i,X_j)$: $X_i$와$X_j$ 간의 공분산</li> </ul> <p><strong>특징</strong></p> <ul> <li>대칭 행렬: $\text{Cov}(X_i,X_j) = \text{Cov}(X_j,X_i)$</li> <li>대각 성분: 대각선 요소는 각 변수의 분산.</li> </ul> <p><strong>3D Gaussian</strong></p> \[f(x,y,z)=\frac{1}{(2\pi)^{3/2}\left|\sum\right|^{1/2}}e^{-\frac{1}{2}r^\top\sum^{-1}r}\] <ul> <li>$r$ = $\left[x-\mu_x,y-\mu_y,z-\mu_z\right]^T$: 평균($\mu$)에서의 거리 벡터</li> <li>$\sum$: 3x3 공분산 행렬 (크기와 방향성을 조정)</li> </ul> <p><strong>특징</strong></p> <ul> <li><strong>공간적 분포(위치):</strong> 3D 공간의 특정 지점(평균)을 중심으로 데이터가 퍼진다.</li> <li><strong>공분산 행렬(모양, 방향):</strong> 3D Gaussian의 분포 모양은 공분산 행렬 $\sum$에 의해 결정된다. <ul> <li>대각 성분(분산): 각 축의 분산을 통해 Gaussian의 장축, 단축 크기를 결정한다.</li> <li>비대각 성분(공분산): 비대각의 공분산 값을 통해 축 사이의 회전을 나타낸다. 즉, 타원의 방향을 결정한다.</li> </ul> </li> </ul> \[\Sigma = \begin{bmatrix} \sigma_x^2 &amp; \sigma_{xy} &amp; \sigma_{xz} \\ \sigma_{xy} &amp; \sigma_y^2 &amp; \sigma_{yz} \\ \sigma_{xz} &amp; \sigma_{yz} &amp; \sigma_z^2 \end{bmatrix}\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/d554ccf8-2c6c-41b9-932c-a94f5cb849be/image.png" alt=""/></p> <p><strong>질문</strong></p> <ul> <li>3D Gaussian은 함수형태로 point cloud를 표현하므로 <strong>implicit(암시)하게 정의</strong>되는것이 아닌가? <ul> <li>함수 형태로 표현되기 때문에 implicit한 것은 맞다. 하지만 Gaussian이라는 것도 결국 $\mu$와 $\sum$로 표현이 되기 때문에 굳이 말하자면 명시적으로 나열될 수 있는 표현이다.</li> <li>결론적으로 implicit한 표현으로 보았을 때 Gaussian함수는 연속적인 표현이 가능하며 메모리 효율성을 챙기고 부드러운 분포를 보일 수 있다는 장점을 가지게 된다.</li> </ul> </li> <li>왜 공분산 행렬을 RSS^TR^T와 같이 정의하는것일까 ? <ul> <li>Symmetric한 성질을 만들기 위하여.</li> <li>$(AA^\top)^\top = A^\top A$</li> </ul> </li> </ul> <p><strong>radiance field</strong></p> <ul> <li>3D 공간에서 빛과 색상 분포를 의미하는 함수, 개념.</li> </ul> <hr/> <blockquote> <h2 id="1-abstract--introduction">1. Abstract &amp; Introduction</h2> </blockquote> <p>3D 장면 표현 방식에서는 그동안 많은 발전이 이루어져 왔다. 대표적으로 NeRF는 MLP를 사용하여 암묵적인 특징을 최적화(implicit optimization)하며, 높은 성능을 보여주고 관련 논문들이 지속적으로 등장하고 있는 중이다.</p> <p>이 분야에서는 공통적으로 해결해야 할 문제와 도전 과제로 다음과 같은 점들이 존재한다.</p> <blockquote> <ul> <li>여러 장의 사진을 통해 장면을 <strong>효율적이고 빠르게 최적화 하고 표현</strong>하는 것.</li> <li><strong>실시간 렌더링(real-time rendering)</strong> 을 가능하게 하는 것.</li> </ul> </blockquote> <p>이번 논문에서는 기존의 SOTA 모델보다 더 빠르고 효율적인 최적화와 표현, 그리고 실시간 렌더링을 지원하는 <strong>3D Gaussian Splatting</strong>을 제안한다.</p> <p>논문에서는 다음과 같은 3가지 주요 방법론을 통해 이를 실현한다.</p> <blockquote> <ul> <li><strong>비등방성 3D Gaussian의 도입 (Anisotropic 3D Gaussians)</strong> <ul> <li>high-quality radiance field를 <strong>비구조적</strong>으로 표현하기 위한 방식.</li> <li>이르르 통해 장면의 세부 정보를 더 정확하게 표현이 가능하다.</li> </ul> </li> <li><strong>3D Gaussian 속성 최적화 (Optimization Method of 3D Gaussian Properties)</strong> <ul> <li>3D Gaussian의 속성을 최적화.</li> <li>adaptive density control을 교차적으로 활용</li> </ul> </li> <li><strong>GPU 기반 빠른 미분 가능 렌더링 (Fast, Differentiable Rendering Approach for the GPU)</strong> <ul> <li>가시성 인식(visibility-aware): 시점에서 보이는 부분만 효율적으로 계산</li> <li>비등방성 splatting(anisotropic splatting): 더 정밀한 장면 렌더링 가능</li> <li>fast backpropagaton: 효율적인 학습과 novel view synthesis 지원</li> </ul> </li> </ul> </blockquote> <hr/> <blockquote> <h2 id="2-related-work">2. Related Work</h2> </blockquote> <hr/> <blockquote> <h2 id="3-overview">3. Overview</h2> </blockquote> <ol> <li><strong>Input</strong> <ul> <li>입력 데이터는 정적인 장면의 이미지 세트와 <strong>SfM(Structure-from-Motion)</strong>을 통해 보정된 카메라 정보가 들어온다.</li> </ul> </li> <li><strong>3D Gaussian</strong> <ul> <li>SfM을 통해 들어온 point cloud를 통해 3D Gaussian 집합을 생성한다.</li> <li>3D Gaussian은 위치(mean), 모양과 방향(covariance matrix), 불투명도($\alpha$, opacity)를 통해 정의된다.</li> </ul> </li> <li><strong>Radiance Field</strong> <ul> <li>Radiance Field Color는 <strong>spherical harmonics, SH</strong>를 사용하여 표현된다.</li> </ul> </li> <li><strong>Optimization</strong> <ul> <li>3D Gaussian의 주요 파라미터인 mean, covariance matrix, $\alpha$, SH coefficients를 최적화하여 randiance field를 올바르게 표현한다.</li> <li>이 과정에서 <strong>adaptive Gaussian density control</strong>가 교차적으로(interleaved) 적용된다.</li> </ul> </li> <li><strong>Tile-based rasterizer</strong> <ul> <li>타일기반의 3D에서 2D 이미지로의 변환은 다음과 같은 효율성과 기능을 가진다. <ul> <li><strong>가시성 순서를 고려한 빠른 정렬로 $\alpha$-blending</strong> 지원.</li> <li>빠른 backward pass 구현</li> <li>가우시안의 개수 제한 없이 그래디언트 계산 가능.</li> </ul> </li> </ul> </li> </ol> <p>전체적인 procedure에 대한 도식화는 아래와 같다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/a1cc2eda-44a0-4449-a46a-e3df572235ad/image.png" width="900"/></p> <p><strong>추가적으로 알아볼 것(SfM, spherical harmonic)</strong></p> <hr/> <blockquote> <h2 id="4-differentiable-3d-gaussian-splatting">4. Differentiable 3D Gaussian Splatting</h2> </blockquote> <p>먼저 위에서의 한계점들을 극복하기 위해서는 미분가능한 volumetric 표현을 가지며 explicit한 특징으로 빠르게 렌더링이 가능한 표현법이 필요하다고 한다.<br/> 3D Gaussian은 미분가능한 표현법이며 2d splats로의 projection과 $\alpha$-blending을 통해 빠른 렌더링이 가능하기 때문에 선택했다고 한다.</p> <p><strong>3D Gaussian initialization</strong><br/> 이전의 표현법들은 normal을 통해 추정하고 optimizing하는게 굉장히 challeging하다고 한다. 그래서 해당 논문에서는 normal정보가 필요없고 <strong>covariance matrix ($\sum$),centered point ($\mu$)를 통해 정의할 수 있는 3D Gaussian</strong>을 사용한다.</p> \[G(x) = e^{-\frac{1}{2}(x)^T\sum^{-1}(x)}\] <p><strong>2D Projection Transform</strong><br/> 위에서 표현한 3D Gaussian을 projection을 위해 2D camera 좌표계인 $\Sigma’$로 변환 할 필요가 있다.</p> \[\Sigma' = J W \Sigma W^T J^T\] <blockquote> <p>$W$: <strong>viewing transformation</strong>으로 카메라 좌표계로의 변환을 의미한다.<br/> $J$: <strong>projective transformation</strong>인 Jacobian행렬로 비선형적인 투영변환을 통해 선형 근사하는 역할을 한다.<br/> <strong>Why 이런 식이 나올까 ?</strong></p> <ul> <li>쉽게 말해 $\Sigma_{WJ} = \Sigma’$으로 볼 수 있는데 <strong>Covariance matrix $\Sigma$</strong> Covariance matrix를 직접적으로 최적화 하기 위해서는 $\Sigma$가 positive semi-definite 조건을 가지고 있어야 한다. 만약 해당 조건을 충족하지 못하고 최적화를 시킨다면 잘못된 optimizing을 할 수도 있게 된다.</li> </ul> </blockquote> <p><strong>Positive semi-definite</strong> 이를 위해 <strong>scaling matrix $S$와 rotation matrix $R$을 사용하여 더 직관적이고 표현력이 높은 $\Sigma$를 구성</strong>하였다.</p> <p><strong>어떻게 이렇게 정의가 가능한 것인가 ?</strong></p> <ul> <li>이는 기존의 공분산이 가지는 의미를 통해 이해를 할 수 있다. 3D Gaussian에서 <strong>공분산은 타원체의 shape과 방향을 조정하는 역할</strong>을 하기 때문에 이는 Scale과 Rotation matrix만을 통해 표현이 가능하다는 것이다.</li> <li>공분산의 대칭성 성질을 유지하기 위해 S와 R을 통해 표현할 때 <strong>전치행렬도 같이 곱해주어 공분산의 대칭행렬 성질을 그대로 유지</strong>하게 된다.</li> </ul> \[\Sigma = RSS^TR^T\] <hr/> <blockquote> <h2 id="5-optimization-with-adaptive-density-control-of-3d-gaussians">5. Optimization with Adaptive Density Control of 3D Gaussians</h2> </blockquote> <h3 id="optimization">Optimization</h3> <p>3D Gaussian Splatting의 최적화는 렌더링 결과와 훈련 데이터셋 이미지를 비교하며 반복적으로 수행한다. 이 과정에서 발생하는 3D-2D 투영의 모호성을 해결하고 효율적인 장면 표현을 만들어내기 위한 다양한 기법이 사용된다.<br/> Optimization에는 geometry에서 생성, 제거, 이동하는 표현들이 가능해야 한다.</p> <p><strong>SGD</strong><br/> Optimizing 알고리즘으로는 GPU와 CUDA의 장점을 최대한 활용하기 위해 Stochastic Gradient Descent를 사용한다. 덕분에 빠른 rasterization이 가능했다고 한다.</p> <p><strong>Sigmoid</strong><br/> $\alpha$를 최적화하기 위해 sigmoid 함수를 사용하고 [0 - 1)로 제한하며 지수함수를 통해 공분산의 크기를 조절한다.</p> <p><strong>Inital &amp; Loss</strong><br/> 3D Gaussian의 각 축의 길이는 가장 가까운 세점까지의 평균 거리를 이용해서 초기화 해준다. 이를 통해 초반에 빠르고 안정적으로 수렴이 가능하다.<br/> 손실함수는 $\mathcal{L}_1$과 D-SSIM을 합친 함수로 구성되게 된다.</p> \[\mathcal{L} = (1-\lambda)\mathcal{L}_1 + \lambda\mathcal{L}_{\text{D-SSIM}}\] <blockquote> <p><strong>D-SSIM이란 ?</strong></p> <ul> <li><strong>D-SSIM</strong>은 Structural Similarity Index(SSIM)을 손실 함수로 사용할 수 있도록 미분 가능하게 변형한 버전이다.</li> <li>D-SSIM은 렌더링된 이미지와 캡쳐된 훈련 뷰 간의 <strong>구조적 유사성을 비교</strong>하며 모델이 더 나은 3D 표현을 하도록 학습을 돕는다. <ul> <li>노이즈 제거, 초해상도, 스타일 전이 등의 구조적 차이?….</li> </ul> </li> </ul> </blockquote> <h3 id="adaptive-control-of-gaussians">Adaptive Control of Gaussians</h3> <p>초반 SfM을 통해 얻은 sparse set에서부터 adaptively control을 통해 denser Gaussian으로 만들어 장면을 더 잘 표현하게 된다. 이후 <strong>100 iteration마다 밀도를 높이며 $\alpha(\text{transparent})$가 threshold보다 낮을 경우 제거</strong>하는 과정을 거친다.</p> <p><strong>Adaptive Control</strong><br/> Adpative Control에서 필요한 것은 <strong>빈 공간에 Gaussian을 통해 채워 넣는 것</strong>이다. 문제는 너무 적게 채워버리면 <strong>under-reconstruction</strong> 문제가 발생하고, 너무 큰 부분을 커버하면 <strong>over-reconstruction</strong> 문제가 발생한다는 것이다.</p> <p><strong>Under-Reconstruction</strong><br/> 만약 해당 Gaussain에서 채워야 할 곳이 필요하다면 <strong>동일한 size를 가진 Gaussian을 복제하고 positinal gradient쪽으로 이동하여 빈공간을 채우게된다.</strong></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/23cb3554-c6ac-482d-9ccf-400b642b35c5/image.png" alt=""/></p> <p><strong>Over-Reconstruction</strong> <br/> 만약 너무 큰 Gaussian이라면 더 작은 Gaussian으로 나눌 필요가 있다. 이때는 <strong>실험적으로 얻은 $\phi$ = 1.6의 scale factor를 통해 작은 2개의 Gaussian으로 나누고 position을 이동</strong>하게된다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/ccc20cbc-0304-41c6-9390-a620280816c2/image.png" alt=""/></p> <p><strong>Remove Gaussian</strong><br/> 위에서 제시한 방법들을 토대로 Scene의 Gaussian을 최적화하면 Gaussian의 개수는 수도 없이 늘어난다. 이를 해결하기 위해 400 iteration마다 일정 threshold보다 낮은 $\alpha$값을 가진 Gaussian은 주기적으로 제거하는 방법을 사용한다.<br/> 결국 중요한, 유의미한 Gaussian들만 남게 되는 것이다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/c673342b-2e6e-4095-9d5d-006d216ce418/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="6-fast-differentiable-rasterizer-for-gaussians">6. Fast Differentiable Rasterizer for Gaussians</h2> </blockquote> <p><strong>Goal</strong><br/> 결론적으로 목표는 빠른 rendering과 빠른 $\alpha$-blending,최적화 구조를 개선하는 것으로 볼 수 있다.</p> <p><strong>Tile-Based Rasterizer</strong><br/> 화면을 16x16 타일로 분할하여 병렬처리를 극대화한다. 각 타일은 독립적으로 처리되어 데이터 로드 및 계산 부분에서 효율화 한다. 99%의 Confidence에 해당하는 Gaussian만 유지하며 이 외의 Gaussain은 제거하여 계산의 불안정성을 방지한다.</p> <p><strong>$\alpha$-blending</strong><br/> 타일 단위의 정렬을 기반으로 블렌딩을 수행한다. 추가적인 픽셀 단위 정렬 없이도 효율적으로 $\alpha$-blending이 가능하다. 픽셀의 불투명도가 1에 도달하면 해당 픽셀의 처리를 종료하며 각 타일 내 모든 픽셀이 포화되어도 종료하게 된다.</p> <p><strong>Backward Pass</strong><br/> Backward Pass 과정에서 Foward Pass의 Blending 정보를 활용하고 계산하여 효율성을 극대화한다.</p> <p><strong>차별점</strong></p> <ul> <li><strong>픽셀 단위 정렬 제거:</strong> 성능을 크게 향상</li> <li><strong>제한 없는 기울기 계산:</strong> Gaussian 개수와 무관하게 처리 가능.</li> <li><strong>근사 $\alpha$-blending:</strong> 성능을 극대화하면서도 시각적으로 자연스러운 결과를 유지.</li> </ul> <p><img src="https://velog.velcdn.com/images/lowzxx/post/3b0b8245-0a91-4f90-b9bc-b477068dd821/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="7-implementation-results-and-evaluation">7. Implementation, Results and Evaluation</h2> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/688b6437-2783-4059-bdeb-06d94a41e1e6/image.png" alt=""/></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/3fa22134-2b86-400a-b6f9-a9d7b9199d82/image.png" width="900"/></p> <hr/> <blockquote> <h2 id="8-discussion-and-conclusions">8. Discussion and Conclusions</h2> </blockquote> <hr/> <h3 id="reference">Reference.</h3> <p><a href="https://arxiv.org/abs/2308.04079">https://arxiv.org/abs/2308.04079</a><br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a> <br/> <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/</a> <br/> <a href="https://www.researchgate.net/figure/sualization-of-a-3D-Gaussian-model-a-Uncertainty-ellipsoid-for_fig5_231212225">https://www.researchgate.net/figure/sualization-of-a-3D-Gaussian-model-a-Uncertainty-ellipsoid-for_fig5_231212225</a></p>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Paper Review]]></summary></entry><entry><title type="html">[Papers] DETR: End-to-End Object Detection with Transformers (CVPR 2020)</title><link href="https://oweixx.github.io/blog/2025/papers_detr/" rel="alternate" type="text/html" title="[Papers] DETR: End-to-End Object Detection with Transformers (CVPR 2020)"/><published>2025-03-31T00:00:00+00:00</published><updated>2025-03-31T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_detr</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_detr/"><![CDATA[<h2 id="end-to-end-object-detection-with-transformers-cvpr-2020">End-to-End Object Detection with Transformers (CVPR 2020)</h2> <h3 id="paper-github"><a href="https://arxiv.org/abs/2005.12872">[Paper]</a> <a href="https://github.com/facebookresearch/detr">[Github]</a></h3> <blockquote> <p><strong>Title:</strong> End-to-End Object Detection with Transformers<br/> <strong>Journal name &amp; Publication year:</strong> Computer Vision and Pattern Recognition 2020<br/> <strong>First and Last Authors:</strong> Nicolas Carion, Francisco Massa<br/> <strong>First Affiliations:</strong> Facebook AI</p> </blockquote> <p>저번주차 수업을 들으면서 중요하다고 판단되는 몇몇 논문중 한 논문으로 최대한 논문 원문을 보면서 이해해보려고 노력하며 정리해본다.</p> <hr/> <blockquote> <h2 id="1-abstract--introduction">1. Abstract &amp; Introduction</h2> </blockquote> <p>DETR은 <strong>Detection pipeline을 streamlines(간소화)</strong> 하며 hand-designed된 부분들을 최대한 제거하려고 노력했다고 한다. DETR의 큰 특징으로는 transformer의 encoder-decoder부분을 차용한 것과 predictions과 ground_truth의 <strong>bipartite matching(이분 매칭) loss를 적용</strong>한다는 것이다.</p> <p>Object detection 분야에서의 목표는 boding boxes와 category labels 제공하는 것이다. DETR은 마지막 부분에서 prediction과 ground truth를 직접 비교하며 loss를 계산한다는 것인데, <strong>DETR은 (non-autoregressive)parallel decoding을 사용함으로써 병렬 처리 및 출력을 하며 출력된 bounding boxes를 각 ground truth의 짝지어진 box들과 매칭</strong>하고 bipartite matching loss를 이용하여 loss 계산을 한다고 한다.</p> <p>Detection 분야에서 여러번 시험되고 많은 성능 개선이 된 Faster R-CNN과 performance적으로 비슷한 결과를 냈다고 한다. 큰 dataset에서는 더 좋은 성능을 주기도 했었지만 작은 dataset에서는 성능이 더 낮은 결과도 보였다고 한다.</p> <p>DETR은 Detection 뿐만이 아니라 다른 더 복잡한 task 활용하여 좋은 성능을 뽑아냈다고 한다. 예를 들어 segmentation or pixel-level recognition 등등…</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/19a86e1f-3fd2-493f-8266-a086cd33f2d4/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="2-related-work">2. Related Work</h2> </blockquote> <h3 id="21-set-prediction">2.1 Set Prediction</h3> <p>DETR에서는 set of box predictions를 위해 decoder구조에서 multi-task를 수행해야한다. 기존의 Detection model들에서 postprocessings + NMS(non-maximal suppression)가 수행하던 중복 bbox 제거가 set prediction에서의 걸림돌이다. direct set prediction에서 near-duplicates을 피하기 위해서 <strong>Hungarian algorithm을 기반으로 loss function을 설계</strong>한다고 한다. 이는 <strong>permutation-invariance(순열 불변성)을 적용하며 각 대상요소가 unique하게 일치</strong>하도록 해준다.</p> <p><strong>Hungarian Algorithm</strong><br/> match 해야할 두 vector $I$와 $J$가 존재할 때 I와 J에서 매칭되어 나온 cost를 최소화 또는 최대화 하는 이분 매칭 방법에 사용되는 알고리즘이다. 현재 Detection에서는 최대가 되는 cost를 원하는 것이니 최대 Hungarian Algorithm으로 예시를 들어본다.</p> <ul> <li> <p>먼저 행렬의 모든 값에서 최대가 되는 값(78)을 고르고 해당 값에서 각 원소들을 빼준다.</p> <table> <thead> <tr> <th> </th> <th>Track-1</th> <th>Track-2</th> <th>Track-3</th> <th>Track-4</th> </tr> </thead> <tbody> <tr> <td>Detection-1</td> <td>10</td> <td>70</td> <td>55</td> <td>25</td> </tr> <tr> <td>Detection-2</td> <td>62</td> <td>15</td> <td>58</td> <td>35</td> </tr> <tr> <td>Detection-3</td> <td>23</td> <td>78</td> <td>14</td> <td>63</td> </tr> <tr> <td>Detection-4</td> <td>55</td> <td>34</td> <td>47</td> <td>0</td> </tr> </tbody> </table> </li> <li> <p>다음 Detection 행 기준으로 최솟값들에 대하여 행에서 값을 빼준다. (1 = 8, 2 = 16, 3 = 0, 4 = 23)</p> <table> <thead> <tr> <th> </th> <th>Track-1</th> <th>Track-2</th> <th>Track-3</th> <th>Track-4</th> </tr> </thead> <tbody> <tr> <td>Detection-1</td> <td>68</td> <td>8</td> <td>23</td> <td>53</td> </tr> <tr> <td>Detection-2</td> <td>16</td> <td>63</td> <td>20</td> <td>43</td> </tr> <tr> <td>Detection-3</td> <td>55</td> <td>0</td> <td>64</td> <td>15</td> </tr> <tr> <td>Detection-4</td> <td>23</td> <td>44</td> <td>31</td> <td>78</td> </tr> </tbody> </table> </li> <li> <p>Track 열 기준으로 최솟값을에 대하여 해당 열에서 값을 빼준다. (1 = 0, 2 = 0, 3 = 4, 4 = 15)</p> <table> <thead> <tr> <th> </th> <th>Track-1</th> <th>Track-2</th> <th>Track-3</th> <th>Track-4</th> </tr> </thead> <tbody> <tr> <td>Detection-1</td> <td>60</td> <td>0</td> <td>15</td> <td>45</td> </tr> <tr> <td>Detection-2</td> <td>0</td> <td>47</td> <td>4</td> <td>27</td> </tr> <tr> <td>Detection-3</td> <td>55</td> <td>0</td> <td>64</td> <td>15</td> </tr> <tr> <td>Detection-4</td> <td>0</td> <td>21</td> <td>8</td> <td>55</td> </tr> </tbody> </table> </li> <li> <p>그렇게 완성된 행렬을 확인해보았을 때 행렬에 있는 모든 0들을 vector의 개수에 맞게 덮을 수 있다면 최대 값을 구할 수 있게 된다. (ex. 0들을 열 기준으로 1,2,3,4를 선으로 덮을 수 있음.)</p> <table> <thead> <tr> <th> </th> <th>Track-1</th> <th>Track-2</th> <th>Track-3</th> <th>Track-4</th> </tr> </thead> <tbody> <tr> <td>Detection-1</td> <td>60</td> <td>0</td> <td>11</td> <td>30</td> </tr> <tr> <td>Detection-2</td> <td>0</td> <td>47</td> <td>0</td> <td>12</td> </tr> <tr> <td>Detection-3</td> <td>55</td> <td>0</td> <td>60</td> <td>0</td> </tr> <tr> <td>Detection-4</td> <td>0</td> <td>21</td> <td>4</td> <td>40</td> </tr> </tbody> </table> </li> <li> <p>해당 0에 대한 값들을 matching을 시켜보면 (1,2) (2,3) (3,4) (4,1)의 값들이 되고 해당 값들을 모두 더하면 Hungarian Algorithm과 bipartite matching을 통한 최댓값을 구할 수 있게 된다. Final Assignment (Optimal Matching)</p> <ul> <li><strong>Detection-1</strong> to <strong>Track-2</strong>: 70</li> <li><strong>Detection-2</strong> to <strong>Track-3</strong>: 58</li> <li><strong>Detection-3</strong> to <strong>Track-1</strong>: 55</li> <li><strong>Detection-4</strong> to <strong>Track-4</strong>: 63 <strong>Maximum Total Value:</strong> 70 + 58 + 55 + 63 = <strong>246</strong></li> </ul> </li> </ul> <p><br/></p> <h3 id="22-transformers-and-parallel-decoding">2.2 Transformers and Parallel Decoding</h3> <p>DETR에서 핵심적으로 중요한 부분이 Transformer 구조인데 Transformer는 처음에 NLP쪽에서 쓰이던 모델 구조였지만 memory 구조적으로나 long squences를 다루는 부분에서 기존의 RNN보다 낫다는 판단이었고 이를 Vision에서도 사용했었던 여러 논문을 토대로 Transformer 구조를 채택했다고 한다.</p> <p>기존의 Transformer는 Sequence-to-Sequence구조로 출력이 하나씩 나오는 구조라 costly한 단점이 있었다. 이 부분을 해결하기 효율적으로 해결하기 위해 <strong>주어진 위치에서 객체의 위치와 클래스를 한꺼번에 예측하는 즉, 병렬적인 Decoding 문제로 변환하였다는 부분이 특징</strong>이다. 이는 기존의 Transformer대로 사용했을 때 순차적으로 예측하지 않고 병렬적으로 예측하여 inference 속도가 월등히 빨라지게 된다.</p> <p><br/></p> <h3 id="23-object-detection">2.3 Object Detection</h3> <p>기존의 Detection 분야에서의 model들은 One-stage detector나 Two-staege detector나 모두 초반에 설정되는 추측 설정들에 따라 성능이 크게 좌우되는 경향이 있었다. 기존의 이런 불편함들을 모두 간소화시켜 end-to-end detection하는 방법을 보여준다. 기존의 Detection 마지막 부분에서 사용됐던 NMS 대신 direct set losses를 사용하여 이러한 post-processing 부분도 줄일 수 있었다.</p> <hr/> <blockquote> <h2 id="3-the-detr-model">3. The DETR model</h2> </blockquote> <h3 id="31-object-detection-set-prediction-loss">3.1 Object detection set prediction loss</h3> <p>먼저 볼 부분은 set prediction loss로 해당 loss에서는 ground truth와 unique한 matching이 되어야 한다. 기본적으로 처음에 fixed-size로 지정된 N은 $\varnothing$(no object)로도 표현이 될 수 있기 때문에 이미지 내에서 detection 할 객체보다 더 많은 개수로 지정이 되어야 한다.</p> <p>${\sigma \in S_N}$를 따라 N개의 예측값들을 permutation을 통해 $y_i$와 $\hat y_{\sigma(i)}$의 bipartite matching값들의 합이 최소가 되는 <strong>$\sigma$(permutation)를 Hungarian algorithm을 이용해서 찾는게 목표</strong>이다. 해당 match에서는 class의 일치와 boxe의 일치 모두 고려한다고 한다. <strong>$y_i$는 $(c_i,b_i)$로 $c_i$는 class, $b_i$는 bbox</strong>에 대한 4개의 숫자로 이루어진 vector정보로 구성되어있다.</p> \[\sigma = \underset{\sigma \in S_N}{\arg\min} \sum_{i}^{N} \mathcal{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)})\] <p>이렇게 되었을 때 <strong>$\hat y_{\sigma(i)}$는 $\hat p_{\sigma_(i)}(c_i)$로 표현이 되는데, 이는 $\sigma_(i)$일 때 $c_i$일 확률을 의미</strong>하게 된다. 그렇게 $\mathcal{L}<em>{\text{match}}(y_i, \hat y</em>{\sigma_(i)})$는 아래와 같은 식으로 표현 될 수 있다. 왼쪽은 class 분류 손실, 오른쪽은 bbox 손실로 표현이 된다. 왼쪽식은 해당 정답 클래스 확률이 큰게 목표이니 커지면서 -가 붙어 loss가 작아지는 쪽이 되고, 오른쪽은 두 bbox가 같아서 0으로 수렴하게 되는게 목표가 되어 총 손실함수는 작아지는 쪽으로 표현이 된다.</p> \[-1_{\{\hat{c}_{i} \neq \emptyset\}} \, \hat{p}_{\sigma(i)}(c_{i}) + 1_{\{c_{i} \neq \emptyset\}} \, \mathcal{L}_{\text{box}}(b_{i}, \hat{b}_{\sigma(i)}).\] <p>해당 부분이 기존의 Detector model들에서 사용된 match proposal, anchors를 맞추는 부분을 대체 한다고 볼 수 있다. <strong>가장 큰 차이점은 direct set prediction을 이용하기 때문에 중복 제거가 되며 one-to-one matching이 된다는 부분</strong>이다.</p> <p>위에서 구한 표현법을 이용해서 Hungarian algorithm의 loss를 구하게 되면 아래와 같은 수식으로 정의된다. 여기서 $\hat \sigma$는 처음에 구했던 최적의 $\sigma$이다. 만약 $c_i = \varnothing$인 경우 가중치를 10배 낮춰 클래스 불균형을 해소한다고 한다. 그렇게 $\varnothing$ matching cost는 예측에 의존하지 않고 cost는 일정하다.</p> \[\mathcal{L}_{\text{Hungarian}}(y, \hat{y}) = \sum_{i=1}^{N} \left( -\log \hat{p}_{\hat \sigma(i)}(c_{i}) + 1_{\{c_{i} \neq \emptyset\}} \, \mathcal{L}_{\text{box}}(b_{i}, \hat{b}_{\hat \sigma(i)}) \right),\] <h3 id="bounding-box-loss"><strong>Bounding box loss.</strong></h3> <p>일반적으로 많이 사용되는 $\ell_1$ loss는 상대적인 오차가 비슷하더라도 작은 박스와 큰 박스에 대해 서로 다른 갖는 문제점이 있엇기 때문에 이를 완화하기 위해 <strong>스케일 불변인 $\ell_1$ loss와 $\mathcal{L}_\text{iou}$를 결합하여 bounding box loss가 표현</strong>이 된다.</p> \[\lambda_{\text{iou}} \, \mathcal{L}_{\text{iou}}(b_{i}, \hat{b}_{\sigma(i)}) + \lambda_{\text{L1}} \, \|b_{i} - \hat{b}_{\sigma(i)}\|_{1}, \quad \text{where } \lambda_{\text{iou}}, \lambda_{\text{L1}} \in \mathbb{R}\] <p><br/></p> <h3 id="32-detr-architecture">3.2 DETR architecture</h3> <p><img src="https://velog.velcdn.com/images/lowzxx/post/84620733-10de-4673-9158-0bb46541c22e/image.png" alt=""/> DETR은 크게 CNN, encoder-decoder Transformer, feed forward network (FFN)로 simple하게 구성되어 있다.</p> <h3 id="backbone"><strong>Backbone.</strong></h3> <p>입력으로 들어오는 image $x_{\text{img}} \in \mathbb{R}^{3 \times H_0 \times W_0}$를 <strong>compact feature representation으로 표현하기 위해 CNN backbone</strong>에 들어가게 되고 $C=2048$, $H,W = \frac{H_0}{32}, \frac{W_0}{32}$로 정의된 $\mathbb{R}^{C \times H \times W}$차원을 가진 형태로 출력이 된다.</p> <h3 id="transformer-encoder"><strong>Transformer encoder.</strong></h3> <p>먼저 channel dimension을 줄이기 위해 1x1 convolution을 이용하여 $\mathbb{R}^{d \times H \times W}$ 차원으로 줄여주고 이를 $z_0$라고 표현한다. 또 sequence 형태로 표현하기 위해 $d \times HW$형태로 표현해준다. encoder 구조는 Multi-Head Self-Attention과 FFN구조로 이루어져 있고 <strong>Transformer는 입력 시퀀스의 순서를 인식하지 못하는 순열 불변성(permutation-invariant)이므로 순서 정보를 보존하기 위해 positional encoding을 추가</strong>해준다.</p> <h3 id="transformer-decoder"><strong>Transformer decoder.</strong></h3> <p>decoder는 기존의 transformer의 standard한 architecture를 따르며 $d$크기의 $N$개의 embedding으로 변환하는 Multi-Head Attention 구조를 가진다. <strong>기존의 decoder 다른 부분은 N개의 object를 병렬적으로 decoding한다는 것이다.</strong> <br/> decoder역시 permutation-invariant 특성을 가지므로 learned positinal encodings인 Object Query를 디코더의 입력으로 사용된다. 신기하게도 <strong>object query는 positinal encoding의 역할과 encoder의 출력값들에 대한 정보를 학습하는 query의 역할</strong>을 동시에 하고 있다. 그렇게 추가된 N개의 object query는 decoder의 단계를 거쳐 class와 bbox를 최종 예측할 수 있게 된다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/5166db6c-5bfe-4897-b408-a2dc9a3bd7a3/image.png" alt=""/></p> <p><strong>Prediction feed-forward network (FFNs).</strong><br/> 마지막 FFN은 ReLU함수를 사용하는 3-layer perceptron과 linear projection으로 구성되어 있다. Bounding Box는 중심좌표와 h,w로 구성되어있고, Class는 softmax function을 통해 예측을 한다. $\varnothing$로 검출이 되기도 하는데 이는 background class로 이해시킨다고 한다.</p> <p><strong>Auxiliary decoding losses.</strong><br/> decoder label뒤에 예측 FFN과 Hungarian loss를 추가하여 보조 loss로 사용하면 학습에 도움된다고 한다.</p> <hr/> <blockquote> <h2 id="4-experiments">4. Experiments</h2> </blockquote> <h3 id="41-comparison-with-faster-r-cnn">4.1 Comparison with Faster R-CNN</h3> <p>기존의 SOTA 모델이였던 Faster R-CNN과 성능 비교를 보여주는 정량적 지표이다.<br/> <img src="https://velog.velcdn.com/images/lowzxx/post/3f448739-a35c-4091-8693-0e6a3ed0837f/image.png" alt=""/></p> <p><br/></p> <h3 id="42-ablations">4.2 Ablations</h3> <p>row에서 encoder layers에 따른 성능 변화를 확인할 수 있다. <img src="https://velog.velcdn.com/images/lowzxx/post/66bdcf18-4860-45fb-bcc2-7eec420c547b/image.png" alt=""/></p> <p>마지막 encoder layer에서의 attention maps를 visualize한 모습이다. <img src="https://velog.velcdn.com/images/lowzxx/post/9444f3a8-de81-4f81-b9fa-5ee2d91a3ae8/image.png" alt=""/></p> <p>다음은 rare classes의 distribution generalization을 보여주는 모습이다.<br/> <img src="https://velog.velcdn.com/images/lowzxx/post/f5a5089e-d081-4e99-86d3-0e3b2ccfdce7/image.png" alt=""/></p> <p>출력 결과물과 decoder의 attention maps를 visualize한 모습이다.<br/> <img src="https://velog.velcdn.com/images/lowzxx/post/6e3b689c-e06e-4ae7-9f72-3d3aab6ea6e9/image.png" alt=""/></p> <p><br/></p> <h3 id="43-analysis">4.3 Analysis</h3> <p><strong>Decoder output slot analysis</strong><br/> <img src="https://velog.velcdn.com/images/lowzxx/post/d803c3ca-0b4d-4c95-9429-d3e88d42758e/image.png" alt=""/></p> <p><strong>Generalization to unseen numbers of instances.</strong><br/> <img src="https://velog.velcdn.com/images/lowzxx/post/93165fc6-513a-402c-a00c-a073bdaf0bc2/image.png" alt=""/></p> <p><br/></p> <h3 id="44-detr-for-panoptic-segmentation">4.4 DETR for panoptic segmentation</h3> <p>DETR의 decoder outputs단에 adding mask를 통하여 panoptic segmentation task를 수행하는 것을 도식화로 보여준다.<br/> <img src="https://velog.velcdn.com/images/lowzxx/post/5e41b16e-8f99-41ff-a50c-12f533099bb4/image.png" alt=""/></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/39131794-4594-4db3-ab47-d38ac8354878/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="5-conclusion">5. Conclusion</h2> </blockquote> <p>DETR(Detection Transformer)은 새로운 객체 탐지 접근 방식으로, Transformer를 활용하여 객체 탐지 문제를 End-to-End로 해결하는 모델이다. <strong>전통적인 Detection과 달리 anchor와 비최대 억제(non-maximum suppression) 같은 후처리 과정을 필요로 하지 않으며</strong>, transformer 구조를 이용하여 detection task를 성공적으로 수행한다.</p> <hr/> <h3 id="reference">Reference.</h3> <p><a href="https://arxiv.org/abs/2005.12872">https://arxiv.org/abs/2005.12872</a></p>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Paper Review]]></summary></entry><entry><title type="html">[Papers] Multiply: Reconstruction of Multiple People from Monocular Video in the Wild (CVPR 2024)</title><link href="https://oweixx.github.io/blog/2025/papers_multiply/" rel="alternate" type="text/html" title="[Papers] Multiply: Reconstruction of Multiple People from Monocular Video in the Wild (CVPR 2024)"/><published>2025-03-31T00:00:00+00:00</published><updated>2025-03-31T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_multiply</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_multiply/"><![CDATA[<h2 id="multiply-reconstruction-of-multiple-people-from-monocular-video-in-the-wild">MultiPly: Reconstruction of Multiple People from Monocular Video in the Wild</h2> <h3 id="paper-github-demo"><a href="https://arxiv.org/abs/2406.01595">[Paper]</a> <a href="https://github.com/eth-ait/MultiPly">[Github]</a> <a href="https://eth-ait.github.io/MultiPly/">[Demo]</a></h3> <blockquote> <p><strong>Title:</strong> MultiPly: Reconstruction of Multiple People from Monocular Video in the Wild<br/> <strong>Journal name &amp; Publication year:</strong> CVPR 2024<br/> <strong>First and Last Authors:</strong> Jiang, Zeren<br/> <strong>First Affiliations:</strong> ETH Z ̈urich</p> </blockquote> <hr/> <blockquote> <h2 id="abstract--introduction">Abstract &amp; Introduction</h2> </blockquote> <p><strong>Limitation</strong> 기존 3D Shape estimating과 3D Reconstruction 분야는 빠르게 발전해왔지만 monocular(단일 카메라) video에서 <strong>인물들간의 interaction이 있을 때 즉, occlusion이 있을 때 잘못된 예측과 생성을 하는 한계점</strong>들이 존재하였다.</p> <p><strong>Expected Outcomes</strong> 만약 이를 통해 해결이 되고 더욱 발전이 된 Model과 방법론들이 등장한다면 장비가 비싼 Virtual이나 증강, 가상현실 산업에서 카메라 하나를 가지고 인물을 온전히 Reconsturction 할 수 있는 AI 기술도 나올 수 있지 않을까 싶다.</p> <p>결론적으로 MultiPly는 위의 multi-person reconstruction의 limitation을 해결하고 3D, AR, XR, 4D등 다양한 산업에서도 사용할 수 있는 방법론을 제시한다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/86af97b8-71de-4e59-a2b6-2c498e00dbc0/image.png" width="900"/></p> <hr/> <blockquote> <h2 id="method-1-layered-neural-representation">Method 1: Layered Neural Representation</h2> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/cd306b4f-2423-4f7b-80a9-0b76e43893a7/image.png" width="900"/></p> <h3 id="neural-avatars">Neural Avatars.</h3> <p>기본적으로 3D Reconstruction을 위해 인물을 표현하는 3D shape 정보와 인물의 외관을 표현하기 위한 Appearance 정보가 필요하게 된다.</p> <blockquote> <p><strong>Implicit Representation</strong> 지속적으로 나오는 <strong>Implicit Representation</strong>이라는 표현은 단순히 3D 형태로 표현하기 위한 mesh나 point cloud가 아닌 수학적 표현으로 함축하겠다는 것을 의미하게 된다. 이는 계산과 자원의 효율성을 충족하기 위해서 사용한다고 봐야할 것이다.</p> </blockquote> <p><strong>3D Shape</strong> 먼저 3D shape의 기하학적인 구조를 표현하기 위해 <strong>signed-distance field(SDF)</strong> 를 사용하게 된다. <strong>SDF는 3D 공간의 각 점에 대해 표면까지의 거리와 방향을 계산해 저장하는 함수</strong>이다. 이를 통해 3D shape정보를 mesh나 point cloud 없이 컴팩트하게 저장할 수 있게 된다.</p> <blockquote> <p><strong>Signed Distance Field (SDF)</strong> SDF는 3D 공간 내의 각 점에서 특정 표면까지의 <strong>부호 있는 거리(signed distance)</strong> 를 정의하는 스칼라 필드이다. 이 값은 기하학적 정보를 컴팩트하게 표현할 수 있어 다양한 3D 표현 방식에서 사용된다.</p> <p><strong>SDF의 구성 원리</strong></p> <ul> <li> <p><strong>정의:</strong> SDF(p)는 3D 공간의 점 p에서 특정 표면까지의 최소 유클리드 거리와 부호를 반환하는 함수입니다.</p> </li> <li><strong>양수 값 (+):</strong> 점이 표면의 외부에 위치함을 의미.</li> <li><strong>음수 값 (-):</strong> 점이 표면의 내부에 위치함을 의미.</li> <li><strong>값이 0일 때:</strong> 점이 표면 위에 정확히 위치함을 의미.</li> </ul> </blockquote> <p><strong>Appearance</strong> 다음으로 인물의 외관 Appearance을 Texture Field로 표현한다. 이는 특정 점에서의 색상이나 방사량(빛의 강도)을 표현하는 함수로 사람의 외형(옷, 피부색)을 나타낼 수 있게 된다. <strong>SDF로 출력된 값들 중 값이 0인 것들은 물체의 표면</strong>을 의미하기 때문에 이에 집중하여 3D 좌표와 SDF값에 따라서 $c^p$(RGB, 광학 방사량)을 계산하게 되는 것이다.</p> <p><strong>Layerd Representation</strong> 해당 논문에서는 multiple people들을 분리하여 3D Representation하는 것이 목표이므로 이를 위해 Layer를 기반으로 각 인물에 대한 정보들을 표현하며 최종적으로 모든 계층을 종합하여 구성하는 방식을 따르고 있다. 결론적으로 <strong>하나의 모델로 학습</strong>을 하지만 <strong>독립적인 계층으로 분리</strong>하여 각 인물들의 정보를 표현한다는 것이다.</p> <p>최종적으로 수식적으로 살펴보면 <strong>사람 $p$를 표현하기 위한 신경망 $f^p$로 표현</strong>이된다. \(c^p, s^p = f^p(x_c^p, \theta^p)\)</p> <blockquote> <p>$f^p$: 사람 p를 표현하는 신경망 $x_c^p$: 특정 점의 좌표 $\theta^p$: 인물의 pose paramter $c^p$: texture field 정보 $s^p$: SDF 정보</p> </blockquote> <h3 id="deformation-module">Deformation Module.</h3> <p>현재 우리가 표현하는 <strong>canonical space의 해당하는 $x_c^p$좌표들은 인물의 포즈 정보가 담겨있지 않은 T-Pose의 정보</strong>로 구성되어있다. 이를 <strong>포즈 정보를 포함한 Deformed space의 $x_c^p$로 표현하기 위해 대중적으로 사용하는 SMPL을 사용</strong>한다. 추가적으로 자연스러운 skin 표현을 위해 LBS도 사용한다고 한다.</p> <p>반대로 T-Pose를 표현하기 위해 $x_d^p$와 $\theta^p$를 SMPL의 역함수를 이용하여 $x_c^p$로 표현할 수도 있다고 한다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/ae8c105f-9773-40c6-8661-d5e18cd679eb/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="method-2-layer-wise-volume-rendering">Method 2: Layer-Wise Volume Rendering</h2> <p>해당 부분은 Layer별 인물들을 Volume Rendering 하기 위한 방법들을 설명한다.</p> </blockquote> <h3 id="volume-rendering-for-human-layers">Volume Rendering for Human Layers.</h3> <p>기존의 Vanila Volume Rendering과 조금 다르게 dynamic한 장면들과 인물별로 layered된 표현을 Volume Rendering하고 싶어하는 것이 목표이며 차이점이다.</p> <p><strong>opacity &amp; density</strong><br/> 먼저 Volume Rendering을 위한 각 점에 대한 opacity와 density를 구하게 된다. 밀도 $\sigma$는 deformed 좌표와 SMPL의 역함수를 이용한 값에 Laplace distribution’s Cumulative Distribution Function(CDF)를 이용하여 계산하고 해당 밀도를 이용하여 sampling된 점들의 좌표값 차이와 함께 해당 sampling의 opacity값을 계산하게 된다.</p> \[o_i^p = 1 - \exp(-\sigma_i^p \Delta x_i)\] \[\sigma_i^p = \sigma \left( f_s^p \left( T_{\text{SMPL}}^{-1}(x_{d,i}^p, \theta^p), \theta^p \right) \right)\] <p><strong>Radiance Accumulation</strong><br/> 위에서 구한 opacity값과 color값을 이용하여 인물별 Volume Rendering을 진행하게 된다.</p> <p>핵심적으로 multi-people에서의 Volume Rendering 문제를 해결하기 위해 $Z_i^{q,p}$를 이용하여 샘플 i점보다 앞에 있는 점들의 집합을 이용하여 가려지는(occlusion)문제를 해결했다고 한다.</p> \[\hat{C}_H = \sum_{i=1}^N \sum_{p=1}^P \left[o_i^p c_i^p \prod_{q=1}^P \prod_{j \in Z_i^{q,p}} \left( 1 - o_j^q \right) \right]\] \[Z_i^{q,p} = \{j \in [1, N] \mid z(x_{d,j}^q) &lt; z(x_{d,i}^p) \}\] <h3 id="scene-composition">Scene Composition</h3> <p>위에서 구한 인물에 대한 Volume Rendering Color값 $\hat C_H$값과 NeRF++에서 사용되는 Background Color값 구하는 $f^b$를 이용하여 나온 $\hat C^b$을 Composition 하여 최종 Color값 $\hat C$를 계산하게 된다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/8a4006f8-9a81-41ab-8e9d-e72b1ad4b516/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="method-3-progressive-prompt-for-sam">Method 3: Progressive Prompt for SAM</h2> <p>SAM을 이용하여 더 정확한 instance segmentation mask를 업데이트하고 생성하는 방법론들을 설명한다.</p> </blockquote> <p>아직도 occlusion에 대한 문제가 있기 때문에 이를 해결하기 위해 <strong>promptable segmentation model인 SAM을 이용하여 더 정확한 인물별 instance segmentation이 가능하게 된다.</strong></p> <p><strong>Deformed Mesh</strong><br/> SDF에서 매쉬를 효율적으로 추출할 수 있는 Multiresolution IsoSurface Extraction(MISE) 알고리즘을 이용하여 해당 p에 대한 mesh 값들을 구하게 된다.</p> \[S_d^p = &lt;V_d^p, F^p&gt; = \text{MISE}(f_s^p,\theta^p)\] <p><strong>Instance Mask</strong><br/> 이후 변형된 mesh를 differentiable rasterizer $R$을 이용하여 instance mask $\mathcal{M}$을 만들어준다. $\mathcal{M}$ = 1 : 메쉬 내부 영역 $\mathcal{M}$ = 0 : 메쉬 외부 또는 가려진 영역</p> \[\mathcal{M}_{\text{mesh}}^p = R(S_d^p).\] <p><strong>Point Prompt</strong><br/> 추가적으로 SAM에 전달할 point prompt를 2D keypoint기반으로 생성한다. 해당 값들은 SMPL을 통해 나온 파라미터들로 구할 수 있다.</p> \[\mathcal{K}_{2d}^p = \{ \Pi (\mathcal{J}(\theta^p, \beta^p)) \},\] <p><strong>Progressive Update</strong><br/> 위에서 구한 prompt들을 이용하여 instance mask $\mathcal{M}$을 지속적으로 업데이트하여 보다 정확한 segmentation 정보를 얻게 된다.</p> \[\mathcal{M}_\text{sam}^p = \text{SAM}(\mathcal{M}_\text{mesh}^p, \mathcal{P}_\text{+}^p,\mathcal{P}_\text{-}^p)\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/281e6fdc-e7b2-4cbd-abdb-e6c2f6d0d7de/image.png" alt=""/></p> <hr/> <blockquote> <h2 id="method-4-confidence-guided-alternating-optimization">Method 4: Confidence-Guided Alternating Optimization</h2> </blockquote> <p>인물간의 가려짐으로 인한 부정확한 pose나 잘못된 depth를 예측하는 경우가 생기게 된다. 기존의 pose와 shape을 동시에 optimize하는 것보다 좋은 방법으로 <strong>pose와 shape을 번갈아 최적화하는 confidence-guided optimization(신뢰도 기반 최적화)을 제시</strong>한다.</p> <p><strong>Confidence-Guided Optimization</strong><br/> mesh기반 mask $\mathcal{M}<em>{\text mesh}^{p,i}$ 와 SAM을 통해 정제된 $\mathcal{M}</em>{\text sam}^{p,i}$ 를 통해 IoU 계산을 함으로써 해당 mask에 대한 신뢰도를 측정한다.</p> <p>한 frame의 scene에 있는 mask에 대해 모두 계산한 IoU의 평균값이 $\alpha (\text{threshold})$ 이상일 경우 해당 frame을 reliable한 frame이라고 정의한다.</p> \[\mathcal{L}_r = \{ I_i \in \mathcal{L} \mid \frac{1}{P} \sum_{p=1}^P \text{IoU}(M_{\text mesh}^{p,i}, M_{sam}^{p,i}) \geq \alpha \},\] <p><strong>Alternating Optimization</strong></p> <blockquote> <p>“To avoid damaging shape updates that are due to wrong poses, we only optimize pose parameters for unreliable frames and jointly optimize pose and shape parameters for reliable frames.”</p> </blockquote> <p>위의 표현에서 볼 수 있듯이 wrong poses로 인해 shape이 손상되지 않도록 unreliable frame에서는 pose만, reliable frame에서는 pose와 shape을 동시에 optimization하는 방법을 제안한다.</p> <hr/> <blockquote> <h2 id="method-5-objectives">Method 5: Objectives</h2> </blockquote> <h3 id="reconstruction-loss">Reconstruction Loss.</h3> <p>예측된 Color와 GT의 Color의 $L_1-\text{distance}$ 값으로 Loss를 구성한다. $\mathcal{R}$은 샘플링된 ray를 의미한다.</p> \[L_{rgb} = \frac{1}{\left|\mathcal{R}\right|} \sum_{r \in \mathcal{R}} \left| C(r) - \hat {C}(r) \right|\] <h3 id="instance-mask-loss">Instance Mask Loss.</h3> <p>Volume Rendering에서 사용하는 opacity $o^p$ 값을 최적화 하는 Loss이다.</p> \[\hat{O}^p(r) = \sum_{i=1}^N \left[ o_i^p \prod_{q=1}^P \prod_{j \in Z_i^{q,p}} (1 - o_j^q) \right].\] <p>아래는 sam의 refined mask와 인물 $p$에 대한 투명도 값의 $L_1-\text{distance}$를 계산한다. mask와 mask가 아닌것이 의아했지만, 생각해보면 <strong>두 값 모두 해당 pixel에 해당 인물이 있냐 없냐를 표현한다는 동일한 목표</strong>를 가지고 있기 때문에 충분히 계산이 가능하고 최적화가 가능한 부분이다.</p> \[L_{\text{mask}} = \frac{1}{|\mathcal{R}|} \sum_{r \in \mathcal{R}} \sum_{p=1}^P \left| \mathcal{M}_{\text{sam}}^p(r) - \hat{O}^p(r) \right|.\] <h3 id="eikonal-loss">Eikonal Loss.</h3> <p>SDF 함수의 gradient 값을 1로 제약을 두는 loss를 구성한다. 만약 gradient 크기가 1을 유지하지 못하면 SDF의 성질을 갖지 못하는 것이므로 신뢰하기 어렵게 된다.</p> \[L_e = \sum_{p=1}^P \mathbb{E}_{x_c} \left( \left\| \nabla f_s^p(x_c^p, \theta^p) \right\| - 1 \right)^2.\] <h3 id="depth-order-loss">Depth Order Loss.</h3> <p>결국 3D Reconstruction이기 때문에 중요한 depth order에 대한 loss를 구현한다.</p> \[L_\text{depth} = \sum_{(u,p,q) \in \mathcal{D}} log(1+exp(D_p(u)-D_q(u))),\] <h3 id="interpenetration-loss">Interpenetration Loss.</h3> <p>해당 Loss는 물리적으로 불가능한 사람들의 겹치는 현상을 방지하기 위해 쓰이는 Loss이다. mesh기반의 3D 분야에서 많이 보이는 문제로 한 사람의 팔이 다른 사람의 팔을 침투하는 것이 예시가 된다.</p> <p>이는 복잡한 다중 사람 장면에서 메쉬 복원의 정확도를 높이고, 물리적으로 타당한 결과를 생성하는 데 도움을 준다.</p> \[L_\text{inter} = \sum_{p=1}^{P} \sum_{q=1,q\neq p}^{P} \left|| \mathcal{V}_{in}^{p,q} - NN(\mathcal{V}_{in}^{p,q},S_d^q) \right||\] <hr/> <blockquote> <h2 id="experiments">Experiments</h2> </blockquote> <p>보다 높은 성능을 보여주기 위해 사용된 방법론들이 추가적으로 많은 디테일들을 끌어 올려주는 것을 확인할 수 있다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/624cb23e-47dd-43e3-8de3-2007d39ee0ed/image.png" alt=""/></p> <p>multi-people Reconstruction에서 겹침이 있음에도 인물간의 복원과 분리가 완벽히 되는 것을 확실하게 보여주는 사진이다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/fad61999-66a1-447a-8808-81207287e011/image.png" width="900"/></p> <hr/> <h3 id="reference">Reference.</h3> <p>https://arxiv.org/abs/2406.01595 https://github.com/eth-ait/MultiPly https://eth-ait.github.io/MultiPly/ https://www.youtube.com/watch?v=r9giQPUp1Gw</p>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Paper Review]]></summary></entry><entry><title type="html">[Papers] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020)</title><link href="https://oweixx.github.io/blog/2025/papers_nerf/" rel="alternate" type="text/html" title="[Papers] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020)"/><published>2025-03-31T00:00:00+00:00</published><updated>2025-03-31T00:00:00+00:00</updated><id>https://oweixx.github.io/blog/2025/papers_nerf</id><content type="html" xml:base="https://oweixx.github.io/blog/2025/papers_nerf/"><![CDATA[<h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h3 id="paper-github-demo"><a href="https://arxiv.org/abs/2003.08934">[Paper]</a> <a href="https://github.com/bmild/nerf">[Github]</a> <a href="https://www.matthewtancik.com/nerf">[Demo]</a></h3> <blockquote> <p><strong>Title:</strong> NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis <br/> <strong>Journal name &amp; Publication year:</strong> ECCV 2020<br/> <strong>First and Last Authors:</strong> Ben Mildenhall <br/> <strong>First Affiliations:</strong> UC Berkeley, Google Research, UC San Diego</p> </blockquote> <hr/> <blockquote> <h2 id="abstract--introduction">Abstract &amp; Introduction</h2> </blockquote> <p>NeRF는 <strong>Novel View Synthesis(NVS)</strong> 계열의 기술이며 입력으로 들어오는 이미지들을 통해 특정 위치에서 해당 물체를 바라보는 synthetic image를 생성하는 기술이다. “for View Synthesis”라는 표현을 새로운 시점의 생성이라는 뜻으로 이해할 수 있다.</p> <p><em><strong>한마디로 지금까지 관측한 이미지들로부터 관측하지 못한 시점에서의 image를 생성하는 기술이다.</strong></em></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/6ebdd725-00bb-44b8-acb6-3f3a23cca597/image.gif" width="900"/></p> <p>NeRF의 PipeLine은 크게 2단계의 과정으로 나눌 수 있다.</p> <ul> <li><strong>Neural Network(MLP)를 통한 3D 공간 특징 추출</strong></li> <li><strong>Volume Rendering을 통한 2D 이미지 생성</strong></li> </ul> <p><img src="https://velog.velcdn.com/images/lowzxx/post/83b84661-d3ad-4f6e-9cf2-12a528795e86/image.png" width="900"/></p> <p>이 외에도 High-Resolution과 High-Frequency를 위한 <strong>Positional Encoding</strong>, <strong>Sampling</strong> 등에 대한 부분은 Optimizing 부분에서 확인해볼 예정이다.</p> <hr/> <blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> </blockquote> <p>가장 먼저 살펴볼 부분은 <strong>밀도 및 색상을 예측 하는 MLP</strong>부분이다. 입력으로는 3D 좌표인 $x = (x, y, z)$와 시점을 나타내는 $d(θ, φ)$ 값이 들어가서 해당 좌표의 RGB 값 $c = (R,G,B)$와 density 값인 $σ$가 출력이 된다.</p> \[FΘ : (x, d) → (c, σ) \quad{} FΘ : (x,y,z,θ, φ) → (R,G,B,σ)\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/f91c69ed-38a8-4649-9bb6-6bdfc0d8ddb7/image.png" width="900"/></p> <p>MLP는 아래와 같이 구성되어 있다. 검은색 화살표는 Linear + ReLU을 거치게 되고 노란색 화살표는 Linear로만 이루어져있으며, 마지막 점선 화살표는 Linear + Sigmoid로 이루어져 있다.중간중간의 <strong>+</strong> 는 Concatenate를 의미한다.</p> <p>처음으로 들어오는 입력값 position x가 3차원이 아닌 60차원으로 들어오게 된다. 이는 Positional Encoding과정을 거치기 때문인데 이는 쉽게 말해서 <strong>3차원 값으로는 표현하지 못하는 영역을 60차원으로 표현하여 디테일을 높여주기 위함</strong>이다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/2b0b8e15-cb5f-436f-9f0f-6214b1f89e96/image.png" width="900"/></p> <blockquote> <p><strong>Positional Encoding의 목적</strong></p> <p><strong>원인</strong> : 일반적으로, NeRF의 MLP는 10개의 층과 뉴런으로 이루어진 단순한 구조이다. 이는 저주파(low-frequency) 정보를 학습하는 데 적합하며, 고주파(high-frequency) 정보를 학습하는 데 한계가 있다.</p> <ul> <li><strong>저주파 정보</strong>: 부드럽고 점진적인 변화 (배경 색상)</li> <li><strong>고주파 정보</strong>: 날카로운 경계나 세부적인 구조 (물체의 윤곽, 텍스쳐 등)</li> </ul> <p>따라서, <strong>단순히 3D 좌표를 입력하면 고주파 정보를 제대로 학습할 수 없고, 결과적으로 부드럽고 디테일이 부족한 장면을 생성하게 된다.</strong></p> <p><strong>그럼 왜 60차원인가?</strong> 3D 좌표의 각 차원을 2L개의 주파수 성분으로 확장(L=10)하며, 이는 총 $3 \times 2L = 60$이 된다.</p> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/812a8098-88a4-460b-b19e-8d5f36a9ab95/image.png" width="900"/></p> <p>그리고 중간 5번째 layer에서 입력으로 들어온 60차원의 좌표값과 똑같은 값이 concatenate되는 부분은 일종의 <strong>skip connection</strong>의 역할로 모델 학습의 안정성과 효율성을 높이기 위함이다.</p> <p>8번째 레이어에서 <strong>밀도(density)</strong> 값이 출력되는데, 이는 해당 좌표를 바라보는 direction 값과는 무관하다는 것을 알 수 있다. <strong>밀도란 개념은 바라보는 시점(viewpoint)에 따라 달라지는 값이 아니라, 특정 좌표 그 자체에서 고정되는 값</strong>이기 때문이다. 따라서 NeRF의 MLP 내부에서 밀도 값은 <strong>Positional Encoding을 통해 확장된 좌표 값</strong>만을 입력으로 받아 계산된다.</p> <p>이후, direction 값 $d$가 concatenate되어 MLP는 해당 좌표와 방향 정보를 조합해 픽셀의 $(R,G,B)$ 값을 출력하게 된다. 이는 색상 값이 시점에 따라 달라지는 <strong>view-dependent</strong> 특성을 학습하기 위해 설계된 과정이다. 예를 들어 빛 반사나 굴절은 시점에 따라 달라질 수 있기 때문이다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/2b0b8e15-cb5f-436f-9f0f-6214b1f89e96/image.png" width="900"/></p> <blockquote> <p><strong>밀도(density)와 direction의 관계</strong></p> <ul> <li>밀도는 특정 좌표가 얼마나 “물질”이 있는지 나타내는 값으로, 시점(view)에 독립적이다. 이는 density field가 3D 공간의 고유한 물리적 특성을 나타낸다고 할 수 있다.</li> <li>반면, 색상 정보는 바라보는 방향에 따라 빛의 반사나 굴절이 달라 질 수 있기 때문에 view-dependent한 특성을 가지게 된다.</li> </ul> <p><strong>오케이 알겠는데 그럼 왜 이렇게 설계 했을까 ?</strong></p> <ul> <li>이는 view-independent한 정보와 view-dependent한 정보를 분리하여 모델이 더 효율적으로 학습할 수 있도록 설계했기 때문이다. 즉, 밀도와 색상 예측 과정을 하나의 MLP내부에서 분리하여 보다 정교한 3D 표현을 학습할 수 있게 설계 한 것이다.</li> </ul> <p><strong>Lambertian effects</strong></p> <ul> <li>논문에서 나오는 표현으로 람베르트 반사라는 용어이다. 이는 관찰자가 바라보는 각도와 관계없이 같은 겉보기 밝기를 갖는 성질을 의미한다.</li> <li>하지만 <strong>NeRF는 direction값을 input으로 사용하기 때문에 각도에 따라 휘도가 달라지는 non-Labertian effects성질을 갖게 되는 것</strong>이다.</li> </ul> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/987984b5-e931-4cdf-93f9-d0f3f7e6b1ad/image.png" width="900"/></p> <hr/> <blockquote> <h2 id="volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</h2> </blockquote> <p>5D를 통해 나온 $c$(color)와 $\sigma$(density)를 통해서 <strong>2D Image를 생성하기 위해 Computer Graphics의 고전적인 방법론인 volume rendering을 사용</strong>한다.</p> <p>수식을 간단하게 살펴보자면 결과 값인 $C(\mathbf{r})$은 <strong>하나의 ray(pixel)에서 기대할 수 있는 Color값(expected color)</strong>을 의미하게 된다.</p> <blockquote> <ul> <li><strong>$t$:</strong> t는 ray의 깊이(depth)를 의미하는 parameter로, <strong>카메라에서 시작된 광선이 3D 공간에서 특정 지점에 도달하기까지의 거리(depth)를 의미</strong>한다. $t_n$은 광선이 시작되는 지점, $t_f$는 광선이 끝나는 지점을 의미한다.</li> <li><strong>$\sigma(\mathbf{r}(t))$:</strong> 해당 시점에서의 density값으로 볼 수 있으며 값이 커질수록 Weight가 커지게 된다.</li> <li><strong>$T(t)$:</strong> <strong>Transmittance(빛의 투과도)</strong>를 의미하며 수식적으로 보았을 때 <strong>density값이 커질 수록 작아진다</strong>는 것을 알 수 있다. 이를 해석해보자면 우리가 보려고 하는 물체 앞에 밀도를 가지는 물체가 있을 때 <strong>우리가 보고자 하는 물체가 가려지게 되는 것</strong>을 수식적으로 표현했다고 볼 수 있다. <strong>pixel은 해당 값이 클 수록 투명하고 작을수록 불투명하게 된다.</strong></li> <li><strong>$c(\mathbf r(t),d)$:</strong> 해당 ray와 시점에 대한 물체의 색을 나타내는 부분이다.</li> </ul> </blockquote> \[C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) c(\mathbf{r}(t), \mathbf{d}) \, dt, \quad \text{where} \quad T(t) = \exp \left( - \int_{t_n}^t \sigma(\mathbf{r}(s)) \, ds \right).\] <p>정리하면, 한 픽셀의 색상은 광선(ray)의 모든 지점에서 <strong>(전달된 투과도) × (밀도) × (색상)</strong> 을 누적하여 적분한 값과 같다. 이 적분은 광선 상의 작은 간격($dt$)에 대해 수행된다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/e795fdd8-aed6-47cb-94e0-067ec9c8791e/image.png" width="900"/></p> <p><strong>continuous(연속적인) 한 적분식을 실제로 프로그래밍에 사용할 수 있게 하기 위해 discrete(이산적인)하게 변환해야 한다.</strong> 그래서 수치적 방법으로 아래와 같이 근사하게 된다. 여기서 사용되는게 <strong>Stratified sampling</strong>으로, 고정된 간격의 샘플링을 하는 것이 아니라 <strong>각 구간에 대해서 무작위 샘플링을 하게 되어 적분의 정확성을 향상</strong> 시키게 되었다고 설명한다. <strong>결론적으로 무작위 샘플링을 통해 적분을 근사하여 연속적인 장면을 표현</strong>할 수 있는 것이다.</p> \[t_i \sim \mathcal{U} \left[ t_n + \frac{i-1}{N}(t_f - t_n), \, t_n + \frac{i}{N}(t_f - t_n) \right]\] <p>그렇게 위에서 샘플링된 $c_i$와 $\sigma_i$값들을 기반으로 $\hat{C}$를 계산하게 된다.</p> <blockquote> <ul> <li>$T_i$: 남아있는 빛의 양 (투과도)</li> <li>$(1-\exp(\sigma_i\delta_i))$: 해당 지점에서 흡수된 빛의 양 (불투명도)</li> <li>$c_i$: 해당 지점의 색상</li> <li>$\hat{C}$: 각 샘플링 지점의 색상 값을 가중합한 결과로 최종 픽셀 색상</li> </ul> </blockquote> \[\hat{C}(\mathbf{r}) = \sum_{i=1}^N T_i \left(1 - \exp(-\sigma_i \delta_i)\right) c_i, \quad \text{where} \quad T_i = \exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)\] <hr/> <blockquote> <h2 id="optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</h2> </blockquote> <h3 id="positional-encoding">Positional encoding</h3> <p>이전에 MLP 부분에서 3차원 좌표를 60차원 입력으로 변환할 때 사용되는 Positional Encoding 기법에 대한 설명이다. 다시 한번 복기하자면 더 높은 고차원으로 표현을 하여 고주파 정보 즉, 물체의 윤곽과 텍스쳐에 대한 detail 정보들을 출력할 수 있게 된다.</p> <blockquote> <ul> <li><strong>저주파 정보</strong>: 부드럽고 점진적인 변화 (배경 색상)</li> <li><strong>고주파 정보</strong>: 날카로운 경계나 세부적인 구조 (물체의 윤곽, 텍스쳐 등)</li> </ul> </blockquote> \[\gamma(p) = \left( \sin(2^0 \pi p), \cos(2^0 \pi p), \cdots, \sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p) \right).\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/88878216-bfa4-4ca6-bbc9-b5ed9147b5a7/image.png" width="900"/></p> <h3 id="hierarchical-volume-sampling">Hierarchical volume sampling</h3> <p>기존의 방식들은 빈 공간(free space) 이나 가려진 영역(occluded regions)과 같이 <strong>렌더링에 기여하지 않는 부분도 반복적으로 샘플링하였기 때문에 매우 비효율적</strong> 이었다.</p> <p>NeRF의 Hierarchical Sampling은 장면의 중요 영역에 샘플링을 집중하여 렌더링 효율과 품질을 높이는 전략이다. Coarse Network와 Fine Network를 동시에 최적화 하게 된다. <strong>Coarse Network에서는 전체적인 이미지</strong>, <strong>Fine Network에서는 중요한 영역</strong>에 대해 집중하게 된다.</p> <blockquote> <ol> <li><strong>Coarse Sampling:</strong> Stratified Sampling을 사용해 고르게 샘플링.</li> <li><strong>PDF 생성:</strong> Coarse Network의 출력을 바탕으로 확률 밀도 함수(PDF)를 생성.</li> <li><strong>Fine Sampling:</strong> PDF를 기반으로 Inverse Transform Samplingdmf 사용하여 중요한 영역에서 추가 샘플링.</li> <li><strong>최종 렌더링:</strong> Coarse와 Fine 샘플을 결합하여 최종 이미지를 생성.</li> </ol> <p>이 과정은 <strong>샘플링 효율성을 극대화</strong>하고, <strong>빈 공간에 낭비되는 계산을 줄이는 동시에 중요한 영역의 디테일을 더 잘 포착</strong>하도록 설계하였다</p> </blockquote> \[\hat{C}_c(\mathbf{r}) = \sum_{i=1}^{N_c} w_i c_i, \quad w_i = T_i \left(1 - \exp(-\sigma_i \delta_i) \right).\] <h3 id="loss">Loss</h3> <p>그렇게 Coarse Network와 Fine Network를 통해 나온 output을 통해 실제 Ground Truth와 L2 Norm을 이용하여 Loss를 간단하게 구성된다.</p> \[L = \sum_{r \in R} \left[ \| \hat{C}_c(r) - C(r) \|_2^2 + \| \hat{C}_f(r) - C(r) \|_2^2 \right]\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/01a206e4-e484-4041-a57b-89906c50886b/image.png" width="900"/></p> <hr/> <blockquote> <h2 id="results">Results</h2> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/0e4d7181-446e-4833-aeb6-b1a9d7cca4d9/image.png" width="900"/></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/1e680de8-5ba0-45b1-906a-0ff78b5530ef/image.png" width="900"/></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/6e158864-22bb-4d00-93fa-5152800da902/image.png" width="900"/></p> <hr/> <h3 id="reference">Reference.</h3> <p><a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a><br/> <a href="https://github.com/bmild/nerf">https://github.com/bmild/nerf</a><br/> <a href="https://csm-kr.tistory.com/64">https://csm-kr.tistory.com/64</a> <br/> <a href="https://jaehoon-daddy.tistory.com/26">https://jaehoon-daddy.tistory.com/26</a><br/> <a href="https://www.youtube.com/watch?v=Mk0y1L8TvKE">https://www.youtube.com/watch?v=Mk0y1L8TvKE</a> <br/> <a href="https://an067.pages.mi.hdm-stuttgart.de/or-jupyterbook/05_NeRF_improvements/05_NeRF_improvements">https://an067.pages.mi.hdm-stuttgart.de/or-jupyterbook/05_NeRF_improvements/05_NeRF_improvements</a></p>]]></content><author><name></name></author><category term="Paper"/><category term="Paper"/><summary type="html"><![CDATA[Paper Review]]></summary></entry></feed>