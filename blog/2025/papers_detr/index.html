<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> [Papers] DETR: End-to-End Object Detection with Transformers (CVPR 2020) | owei </title> <meta name="author" content="owei "> <meta name="description" content="Paper Review"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%96%A4&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://oweixx.github.io/blog/2025/papers_detr/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">owei</span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">[Papers] DETR: End-to-End Object Detection with Transformers (CVPR 2020)</h1> <p class="post-meta"> Created on March 31, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/paper"> <i class="fa-solid fa-hashtag fa-sm"></i> Paper</a>   ·   <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> Paper</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="end-to-end-object-detection-with-transformers-cvpr-2020">End-to-End Object Detection with Transformers (CVPR 2020)</h2> <h3 id="paper-github"> <a href="https://arxiv.org/abs/2005.12872" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/facebookresearch/detr" rel="external nofollow noopener" target="_blank">[Github]</a> </h3> <blockquote> <p><strong>Title:</strong> End-to-End Object Detection with Transformers<br> <strong>Journal name &amp; Publication year:</strong> Computer Vision and Pattern Recognition 2020<br> <strong>First and Last Authors:</strong> Nicolas Carion, Francisco Massa<br> <strong>First Affiliations:</strong> Facebook AI</p> </blockquote> <p>저번주차 수업을 들으면서 중요하다고 판단되는 몇몇 논문중 한 논문으로 최대한 논문 원문을 보면서 이해해보려고 노력하며 정리해본다.</p> <hr> <blockquote> <h2 id="1-abstract--introduction">1. Abstract &amp; Introduction</h2> </blockquote> <p>DETR은 <strong>Detection pipeline을 streamlines(간소화)</strong> 하며 hand-designed된 부분들을 최대한 제거하려고 노력했다고 한다. DETR의 큰 특징으로는 transformer의 encoder-decoder부분을 차용한 것과 predictions과 ground_truth의 <strong>bipartite matching(이분 매칭) loss를 적용</strong>한다는 것이다.</p> <p>Object detection 분야에서의 목표는 boding boxes와 category labels 제공하는 것이다. DETR은 마지막 부분에서 prediction과 ground truth를 직접 비교하며 loss를 계산한다는 것인데, <strong>DETR은 (non-autoregressive)parallel decoding을 사용함으로써 병렬 처리 및 출력을 하며 출력된 bounding boxes를 각 ground truth의 짝지어진 box들과 매칭</strong>하고 bipartite matching loss를 이용하여 loss 계산을 한다고 한다.</p> <p>Detection 분야에서 여러번 시험되고 많은 성능 개선이 된 Faster R-CNN과 performance적으로 비슷한 결과를 냈다고 한다. 큰 dataset에서는 더 좋은 성능을 주기도 했었지만 작은 dataset에서는 성능이 더 낮은 결과도 보였다고 한다.</p> <p>DETR은 Detection 뿐만이 아니라 다른 더 복잡한 task 활용하여 좋은 성능을 뽑아냈다고 한다. 예를 들어 segmentation or pixel-level recognition 등등…</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/19a86e1f-3fd2-493f-8266-a086cd33f2d4/image.png" alt=""></p> <hr> <blockquote> <h2 id="2-related-work">2. Related Work</h2> </blockquote> <h3 id="21-set-prediction">2.1 Set Prediction</h3> <p>DETR에서는 set of box predictions를 위해 decoder구조에서 multi-task를 수행해야한다. 기존의 Detection model들에서 postprocessings + NMS(non-maximal suppression)가 수행하던 중복 bbox 제거가 set prediction에서의 걸림돌이다. direct set prediction에서 near-duplicates을 피하기 위해서 <strong>Hungarian algorithm을 기반으로 loss function을 설계</strong>한다고 한다. 이는 <strong>permutation-invariance(순열 불변성)을 적용하며 각 대상요소가 unique하게 일치</strong>하도록 해준다.</p> <p><strong>Hungarian Algorithm</strong><br> match 해야할 두 vector $I$와 $J$가 존재할 때 I와 J에서 매칭되어 나온 cost를 최소화 또는 최대화 하는 이분 매칭 방법에 사용되는 알고리즘이다. 현재 Detection에서는 최대가 되는 cost를 원하는 것이니 최대 Hungarian Algorithm으로 예시를 들어본다.</p> <ul> <li> <p>먼저 행렬의 모든 값에서 최대가 되는 값(78)을 고르고 해당 값에서 각 원소들을 빼준다.</p> <table> <thead> <tr> <th> </th> <th>Track-1</th> <th>Track-2</th> <th>Track-3</th> <th>Track-4</th> </tr> </thead> <tbody> <tr> <td>Detection-1</td> <td>10</td> <td>70</td> <td>55</td> <td>25</td> </tr> <tr> <td>Detection-2</td> <td>62</td> <td>15</td> <td>58</td> <td>35</td> </tr> <tr> <td>Detection-3</td> <td>23</td> <td>78</td> <td>14</td> <td>63</td> </tr> <tr> <td>Detection-4</td> <td>55</td> <td>34</td> <td>47</td> <td>0</td> </tr> </tbody> </table> </li> <li> <p>다음 Detection 행 기준으로 최솟값들에 대하여 행에서 값을 빼준다. (1 = 8, 2 = 16, 3 = 0, 4 = 23)</p> <table> <thead> <tr> <th> </th> <th>Track-1</th> <th>Track-2</th> <th>Track-3</th> <th>Track-4</th> </tr> </thead> <tbody> <tr> <td>Detection-1</td> <td>68</td> <td>8</td> <td>23</td> <td>53</td> </tr> <tr> <td>Detection-2</td> <td>16</td> <td>63</td> <td>20</td> <td>43</td> </tr> <tr> <td>Detection-3</td> <td>55</td> <td>0</td> <td>64</td> <td>15</td> </tr> <tr> <td>Detection-4</td> <td>23</td> <td>44</td> <td>31</td> <td>78</td> </tr> </tbody> </table> </li> <li> <p>Track 열 기준으로 최솟값을에 대하여 해당 열에서 값을 빼준다. (1 = 0, 2 = 0, 3 = 4, 4 = 15)</p> <table> <thead> <tr> <th> </th> <th>Track-1</th> <th>Track-2</th> <th>Track-3</th> <th>Track-4</th> </tr> </thead> <tbody> <tr> <td>Detection-1</td> <td>60</td> <td>0</td> <td>15</td> <td>45</td> </tr> <tr> <td>Detection-2</td> <td>0</td> <td>47</td> <td>4</td> <td>27</td> </tr> <tr> <td>Detection-3</td> <td>55</td> <td>0</td> <td>64</td> <td>15</td> </tr> <tr> <td>Detection-4</td> <td>0</td> <td>21</td> <td>8</td> <td>55</td> </tr> </tbody> </table> </li> <li> <p>그렇게 완성된 행렬을 확인해보았을 때 행렬에 있는 모든 0들을 vector의 개수에 맞게 덮을 수 있다면 최대 값을 구할 수 있게 된다. (ex. 0들을 열 기준으로 1,2,3,4를 선으로 덮을 수 있음.)</p> <table> <thead> <tr> <th> </th> <th>Track-1</th> <th>Track-2</th> <th>Track-3</th> <th>Track-4</th> </tr> </thead> <tbody> <tr> <td>Detection-1</td> <td>60</td> <td>0</td> <td>11</td> <td>30</td> </tr> <tr> <td>Detection-2</td> <td>0</td> <td>47</td> <td>0</td> <td>12</td> </tr> <tr> <td>Detection-3</td> <td>55</td> <td>0</td> <td>60</td> <td>0</td> </tr> <tr> <td>Detection-4</td> <td>0</td> <td>21</td> <td>4</td> <td>40</td> </tr> </tbody> </table> </li> <li> <p>해당 0에 대한 값들을 matching을 시켜보면 (1,2) (2,3) (3,4) (4,1)의 값들이 되고 해당 값들을 모두 더하면 Hungarian Algorithm과 bipartite matching을 통한 최댓값을 구할 수 있게 된다. Final Assignment (Optimal Matching)</p> <ul> <li> <strong>Detection-1</strong> to <strong>Track-2</strong>: 70</li> <li> <strong>Detection-2</strong> to <strong>Track-3</strong>: 58</li> <li> <strong>Detection-3</strong> to <strong>Track-1</strong>: 55</li> <li> <strong>Detection-4</strong> to <strong>Track-4</strong>: 63 <strong>Maximum Total Value:</strong> 70 + 58 + 55 + 63 = <strong>246</strong> </li> </ul> </li> </ul> <p><br></p> <h3 id="22-transformers-and-parallel-decoding">2.2 Transformers and Parallel Decoding</h3> <p>DETR에서 핵심적으로 중요한 부분이 Transformer 구조인데 Transformer는 처음에 NLP쪽에서 쓰이던 모델 구조였지만 memory 구조적으로나 long squences를 다루는 부분에서 기존의 RNN보다 낫다는 판단이었고 이를 Vision에서도 사용했었던 여러 논문을 토대로 Transformer 구조를 채택했다고 한다.</p> <p>기존의 Transformer는 Sequence-to-Sequence구조로 출력이 하나씩 나오는 구조라 costly한 단점이 있었다. 이 부분을 해결하기 효율적으로 해결하기 위해 <strong>주어진 위치에서 객체의 위치와 클래스를 한꺼번에 예측하는 즉, 병렬적인 Decoding 문제로 변환하였다는 부분이 특징</strong>이다. 이는 기존의 Transformer대로 사용했을 때 순차적으로 예측하지 않고 병렬적으로 예측하여 inference 속도가 월등히 빨라지게 된다.</p> <p><br></p> <h3 id="23-object-detection">2.3 Object Detection</h3> <p>기존의 Detection 분야에서의 model들은 One-stage detector나 Two-staege detector나 모두 초반에 설정되는 추측 설정들에 따라 성능이 크게 좌우되는 경향이 있었다. 기존의 이런 불편함들을 모두 간소화시켜 end-to-end detection하는 방법을 보여준다. 기존의 Detection 마지막 부분에서 사용됐던 NMS 대신 direct set losses를 사용하여 이러한 post-processing 부분도 줄일 수 있었다.</p> <hr> <blockquote> <h2 id="3-the-detr-model">3. The DETR model</h2> </blockquote> <h3 id="31-object-detection-set-prediction-loss">3.1 Object detection set prediction loss</h3> <p>먼저 볼 부분은 set prediction loss로 해당 loss에서는 ground truth와 unique한 matching이 되어야 한다. 기본적으로 처음에 fixed-size로 지정된 N은 $\varnothing$(no object)로도 표현이 될 수 있기 때문에 이미지 내에서 detection 할 객체보다 더 많은 개수로 지정이 되어야 한다.</p> <p>${\sigma \in S_N}$를 따라 N개의 예측값들을 permutation을 통해 $y_i$와 $\hat y_{\sigma(i)}$의 bipartite matching값들의 합이 최소가 되는 <strong>$\sigma$(permutation)를 Hungarian algorithm을 이용해서 찾는게 목표</strong>이다. 해당 match에서는 class의 일치와 boxe의 일치 모두 고려한다고 한다. <strong>$y_i$는 $(c_i,b_i)$로 $c_i$는 class, $b_i$는 bbox</strong>에 대한 4개의 숫자로 이루어진 vector정보로 구성되어있다.</p> \[\sigma = \underset{\sigma \in S_N}{\arg\min} \sum_{i}^{N} \mathcal{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)})\] <p>이렇게 되었을 때 <strong>$\hat y_{\sigma(i)}$는 $\hat p_{\sigma_(i)}(c_i)$로 표현이 되는데, 이는 $\sigma_(i)$일 때 $c_i$일 확률을 의미</strong>하게 된다. 그렇게 $\mathcal{L}<em>{\text{match}}(y_i, \hat y</em>{\sigma_(i)})$는 아래와 같은 식으로 표현 될 수 있다. 왼쪽은 class 분류 손실, 오른쪽은 bbox 손실로 표현이 된다. 왼쪽식은 해당 정답 클래스 확률이 큰게 목표이니 커지면서 -가 붙어 loss가 작아지는 쪽이 되고, 오른쪽은 두 bbox가 같아서 0으로 수렴하게 되는게 목표가 되어 총 손실함수는 작아지는 쪽으로 표현이 된다.</p> \[-1_{\{\hat{c}_{i} \neq \emptyset\}} \, \hat{p}_{\sigma(i)}(c_{i}) + 1_{\{c_{i} \neq \emptyset\}} \, \mathcal{L}_{\text{box}}(b_{i}, \hat{b}_{\sigma(i)}).\] <p>해당 부분이 기존의 Detector model들에서 사용된 match proposal, anchors를 맞추는 부분을 대체 한다고 볼 수 있다. <strong>가장 큰 차이점은 direct set prediction을 이용하기 때문에 중복 제거가 되며 one-to-one matching이 된다는 부분</strong>이다.</p> <p>위에서 구한 표현법을 이용해서 Hungarian algorithm의 loss를 구하게 되면 아래와 같은 수식으로 정의된다. 여기서 $\hat \sigma$는 처음에 구했던 최적의 $\sigma$이다. 만약 $c_i = \varnothing$인 경우 가중치를 10배 낮춰 클래스 불균형을 해소한다고 한다. 그렇게 $\varnothing$ matching cost는 예측에 의존하지 않고 cost는 일정하다.</p> \[\mathcal{L}_{\text{Hungarian}}(y, \hat{y}) = \sum_{i=1}^{N} \left( -\log \hat{p}_{\hat \sigma(i)}(c_{i}) + 1_{\{c_{i} \neq \emptyset\}} \, \mathcal{L}_{\text{box}}(b_{i}, \hat{b}_{\hat \sigma(i)}) \right),\] <h3 id="bounding-box-loss"><strong>Bounding box loss.</strong></h3> <p>일반적으로 많이 사용되는 $\ell_1$ loss는 상대적인 오차가 비슷하더라도 작은 박스와 큰 박스에 대해 서로 다른 갖는 문제점이 있엇기 때문에 이를 완화하기 위해 <strong>스케일 불변인 $\ell_1$ loss와 $\mathcal{L}_\text{iou}$를 결합하여 bounding box loss가 표현</strong>이 된다.</p> \[\lambda_{\text{iou}} \, \mathcal{L}_{\text{iou}}(b_{i}, \hat{b}_{\sigma(i)}) + \lambda_{\text{L1}} \, \|b_{i} - \hat{b}_{\sigma(i)}\|_{1}, \quad \text{where } \lambda_{\text{iou}}, \lambda_{\text{L1}} \in \mathbb{R}\] <p><br></p> <h3 id="32-detr-architecture">3.2 DETR architecture</h3> <p><img src="https://velog.velcdn.com/images/lowzxx/post/84620733-10de-4673-9158-0bb46541c22e/image.png" alt=""> DETR은 크게 CNN, encoder-decoder Transformer, feed forward network (FFN)로 simple하게 구성되어 있다.</p> <h3 id="backbone"><strong>Backbone.</strong></h3> <p>입력으로 들어오는 image $x_{\text{img}} \in \mathbb{R}^{3 \times H_0 \times W_0}$를 <strong>compact feature representation으로 표현하기 위해 CNN backbone</strong>에 들어가게 되고 $C=2048$, $H,W = \frac{H_0}{32}, \frac{W_0}{32}$로 정의된 $\mathbb{R}^{C \times H \times W}$차원을 가진 형태로 출력이 된다.</p> <h3 id="transformer-encoder"><strong>Transformer encoder.</strong></h3> <p>먼저 channel dimension을 줄이기 위해 1x1 convolution을 이용하여 $\mathbb{R}^{d \times H \times W}$ 차원으로 줄여주고 이를 $z_0$라고 표현한다. 또 sequence 형태로 표현하기 위해 $d \times HW$형태로 표현해준다. encoder 구조는 Multi-Head Self-Attention과 FFN구조로 이루어져 있고 <strong>Transformer는 입력 시퀀스의 순서를 인식하지 못하는 순열 불변성(permutation-invariant)이므로 순서 정보를 보존하기 위해 positional encoding을 추가</strong>해준다.</p> <h3 id="transformer-decoder"><strong>Transformer decoder.</strong></h3> <p>decoder는 기존의 transformer의 standard한 architecture를 따르며 $d$크기의 $N$개의 embedding으로 변환하는 Multi-Head Attention 구조를 가진다. <strong>기존의 decoder 다른 부분은 N개의 object를 병렬적으로 decoding한다는 것이다.</strong> <br> decoder역시 permutation-invariant 특성을 가지므로 learned positinal encodings인 Object Query를 디코더의 입력으로 사용된다. 신기하게도 <strong>object query는 positinal encoding의 역할과 encoder의 출력값들에 대한 정보를 학습하는 query의 역할</strong>을 동시에 하고 있다. 그렇게 추가된 N개의 object query는 decoder의 단계를 거쳐 class와 bbox를 최종 예측할 수 있게 된다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/5166db6c-5bfe-4897-b408-a2dc9a3bd7a3/image.png" alt=""></p> <p><strong>Prediction feed-forward network (FFNs).</strong><br> 마지막 FFN은 ReLU함수를 사용하는 3-layer perceptron과 linear projection으로 구성되어 있다. Bounding Box는 중심좌표와 h,w로 구성되어있고, Class는 softmax function을 통해 예측을 한다. $\varnothing$로 검출이 되기도 하는데 이는 background class로 이해시킨다고 한다.</p> <p><strong>Auxiliary decoding losses.</strong><br> decoder label뒤에 예측 FFN과 Hungarian loss를 추가하여 보조 loss로 사용하면 학습에 도움된다고 한다.</p> <hr> <blockquote> <h2 id="4-experiments">4. Experiments</h2> </blockquote> <h3 id="41-comparison-with-faster-r-cnn">4.1 Comparison with Faster R-CNN</h3> <p>기존의 SOTA 모델이였던 Faster R-CNN과 성능 비교를 보여주는 정량적 지표이다.<br> <img src="https://velog.velcdn.com/images/lowzxx/post/3f448739-a35c-4091-8693-0e6a3ed0837f/image.png" alt=""></p> <p><br></p> <h3 id="42-ablations">4.2 Ablations</h3> <p>row에서 encoder layers에 따른 성능 변화를 확인할 수 있다. <img src="https://velog.velcdn.com/images/lowzxx/post/66bdcf18-4860-45fb-bcc2-7eec420c547b/image.png" alt=""></p> <p>마지막 encoder layer에서의 attention maps를 visualize한 모습이다. <img src="https://velog.velcdn.com/images/lowzxx/post/9444f3a8-de81-4f81-b9fa-5ee2d91a3ae8/image.png" alt=""></p> <p>다음은 rare classes의 distribution generalization을 보여주는 모습이다.<br> <img src="https://velog.velcdn.com/images/lowzxx/post/f5a5089e-d081-4e99-86d3-0e3b2ccfdce7/image.png" alt=""></p> <p>출력 결과물과 decoder의 attention maps를 visualize한 모습이다.<br> <img src="https://velog.velcdn.com/images/lowzxx/post/6e3b689c-e06e-4ae7-9f72-3d3aab6ea6e9/image.png" alt=""></p> <p><br></p> <h3 id="43-analysis">4.3 Analysis</h3> <p><strong>Decoder output slot analysis</strong><br> <img src="https://velog.velcdn.com/images/lowzxx/post/d803c3ca-0b4d-4c95-9429-d3e88d42758e/image.png" alt=""></p> <p><strong>Generalization to unseen numbers of instances.</strong><br> <img src="https://velog.velcdn.com/images/lowzxx/post/93165fc6-513a-402c-a00c-a073bdaf0bc2/image.png" alt=""></p> <p><br></p> <h3 id="44-detr-for-panoptic-segmentation">4.4 DETR for panoptic segmentation</h3> <p>DETR의 decoder outputs단에 adding mask를 통하여 panoptic segmentation task를 수행하는 것을 도식화로 보여준다.<br> <img src="https://velog.velcdn.com/images/lowzxx/post/5e41b16e-8f99-41ff-a50c-12f533099bb4/image.png" alt=""></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/39131794-4594-4db3-ab47-d38ac8354878/image.png" alt=""></p> <hr> <blockquote> <h2 id="5-conclusion">5. Conclusion</h2> </blockquote> <p>DETR(Detection Transformer)은 새로운 객체 탐지 접근 방식으로, Transformer를 활용하여 객체 탐지 문제를 End-to-End로 해결하는 모델이다. <strong>전통적인 Detection과 달리 anchor와 비최대 억제(non-maximum suppression) 같은 후처리 과정을 필요로 하지 않으며</strong>, transformer 구조를 이용하여 detection task를 성공적으로 수행한다.</p> <hr> <h3 id="reference">Reference.</h3> <p><a href="https://arxiv.org/abs/2005.12872" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2005.12872</a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/papers_img2img_turbo/">[Papers] One-Step Image Translation with Text-to-Image Models (Preprint)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/papers_GAGAvatars/">[Papers] Generalizable and Animatable Gaussian Head Avatar (NeurIPS 2024)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/papers_FaceLift/">[Papers] FaceLift: Learning Generalizable Single Image 3D Face Reconstruction from Synthetic Heads (ICCV 2025)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/papers_CAP4D/">[Papers] CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models (CVPR 2025 Oral)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/papers_GaussianAvatars/">[Papers] GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians (CVPR 2024 Highlight)</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 owei . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>