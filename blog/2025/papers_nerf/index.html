<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> [Papers] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020) | owei </title> <meta name="author" content="owei "> <meta name="description" content="Paper Review"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%96%A4&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://oweixx.github.io/blog/2025/papers_nerf/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">owei</span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">[Papers] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020)</h1> <p class="post-meta"> Created on March 31, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/paper"> <i class="fa-solid fa-hashtag fa-sm"></i> Paper</a>   ·   <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> Paper</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h3 id="paper-github-demo"> <a href="https://arxiv.org/abs/2003.08934" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://github.com/bmild/nerf" rel="external nofollow noopener" target="_blank">[Github]</a> <a href="https://www.matthewtancik.com/nerf" rel="external nofollow noopener" target="_blank">[Demo]</a> </h3> <blockquote> <p><strong>Title:</strong> NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis <br> <strong>Journal name &amp; Publication year:</strong> ECCV 2020<br> <strong>First and Last Authors:</strong> Ben Mildenhall <br> <strong>First Affiliations:</strong> UC Berkeley, Google Research, UC San Diego</p> </blockquote> <hr> <blockquote> <h2 id="abstract--introduction">Abstract &amp; Introduction</h2> </blockquote> <p>NeRF는 <strong>Novel View Synthesis(NVS)</strong> 계열의 기술이며 입력으로 들어오는 이미지들을 통해 특정 위치에서 해당 물체를 바라보는 synthetic image를 생성하는 기술이다. “for View Synthesis”라는 표현을 새로운 시점의 생성이라는 뜻으로 이해할 수 있다.</p> <p><em><strong>한마디로 지금까지 관측한 이미지들로부터 관측하지 못한 시점에서의 image를 생성하는 기술이다.</strong></em></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/6ebdd725-00bb-44b8-acb6-3f3a23cca597/image.gif" width="900"></p> <p>NeRF의 PipeLine은 크게 2단계의 과정으로 나눌 수 있다.</p> <ul> <li><strong>Neural Network(MLP)를 통한 3D 공간 특징 추출</strong></li> <li><strong>Volume Rendering을 통한 2D 이미지 생성</strong></li> </ul> <p><img src="https://velog.velcdn.com/images/lowzxx/post/83b84661-d3ad-4f6e-9cf2-12a528795e86/image.png" width="900"></p> <p>이 외에도 High-Resolution과 High-Frequency를 위한 <strong>Positional Encoding</strong>, <strong>Sampling</strong> 등에 대한 부분은 Optimizing 부분에서 확인해볼 예정이다.</p> <hr> <blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> </blockquote> <p>가장 먼저 살펴볼 부분은 <strong>밀도 및 색상을 예측 하는 MLP</strong>부분이다. 입력으로는 3D 좌표인 $x = (x, y, z)$와 시점을 나타내는 $d(θ, φ)$ 값이 들어가서 해당 좌표의 RGB 값 $c = (R,G,B)$와 density 값인 $σ$가 출력이 된다.</p> \[FΘ : (x, d) → (c, σ) \quad{} FΘ : (x,y,z,θ, φ) → (R,G,B,σ)\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/f91c69ed-38a8-4649-9bb6-6bdfc0d8ddb7/image.png" width="900"></p> <p>MLP는 아래와 같이 구성되어 있다. 검은색 화살표는 Linear + ReLU을 거치게 되고 노란색 화살표는 Linear로만 이루어져있으며, 마지막 점선 화살표는 Linear + Sigmoid로 이루어져 있다.중간중간의 <strong>+</strong> 는 Concatenate를 의미한다.</p> <p>처음으로 들어오는 입력값 position x가 3차원이 아닌 60차원으로 들어오게 된다. 이는 Positional Encoding과정을 거치기 때문인데 이는 쉽게 말해서 <strong>3차원 값으로는 표현하지 못하는 영역을 60차원으로 표현하여 디테일을 높여주기 위함</strong>이다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/2b0b8e15-cb5f-436f-9f0f-6214b1f89e96/image.png" width="900"></p> <blockquote> <p><strong>Positional Encoding의 목적</strong></p> <p><strong>원인</strong> : 일반적으로, NeRF의 MLP는 10개의 층과 뉴런으로 이루어진 단순한 구조이다. 이는 저주파(low-frequency) 정보를 학습하는 데 적합하며, 고주파(high-frequency) 정보를 학습하는 데 한계가 있다.</p> <ul> <li> <strong>저주파 정보</strong>: 부드럽고 점진적인 변화 (배경 색상)</li> <li> <strong>고주파 정보</strong>: 날카로운 경계나 세부적인 구조 (물체의 윤곽, 텍스쳐 등)</li> </ul> <p>따라서, <strong>단순히 3D 좌표를 입력하면 고주파 정보를 제대로 학습할 수 없고, 결과적으로 부드럽고 디테일이 부족한 장면을 생성하게 된다.</strong></p> <p><strong>그럼 왜 60차원인가?</strong> 3D 좌표의 각 차원을 2L개의 주파수 성분으로 확장(L=10)하며, 이는 총 $3 \times 2L = 60$이 된다.</p> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/812a8098-88a4-460b-b19e-8d5f36a9ab95/image.png" width="900"></p> <p>그리고 중간 5번째 layer에서 입력으로 들어온 60차원의 좌표값과 똑같은 값이 concatenate되는 부분은 일종의 <strong>skip connection</strong>의 역할로 모델 학습의 안정성과 효율성을 높이기 위함이다.</p> <p>8번째 레이어에서 <strong>밀도(density)</strong> 값이 출력되는데, 이는 해당 좌표를 바라보는 direction 값과는 무관하다는 것을 알 수 있다. <strong>밀도란 개념은 바라보는 시점(viewpoint)에 따라 달라지는 값이 아니라, 특정 좌표 그 자체에서 고정되는 값</strong>이기 때문이다. 따라서 NeRF의 MLP 내부에서 밀도 값은 <strong>Positional Encoding을 통해 확장된 좌표 값</strong>만을 입력으로 받아 계산된다.</p> <p>이후, direction 값 $d$가 concatenate되어 MLP는 해당 좌표와 방향 정보를 조합해 픽셀의 $(R,G,B)$ 값을 출력하게 된다. 이는 색상 값이 시점에 따라 달라지는 <strong>view-dependent</strong> 특성을 학습하기 위해 설계된 과정이다. 예를 들어 빛 반사나 굴절은 시점에 따라 달라질 수 있기 때문이다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/2b0b8e15-cb5f-436f-9f0f-6214b1f89e96/image.png" width="900"></p> <blockquote> <p><strong>밀도(density)와 direction의 관계</strong></p> <ul> <li>밀도는 특정 좌표가 얼마나 “물질”이 있는지 나타내는 값으로, 시점(view)에 독립적이다. 이는 density field가 3D 공간의 고유한 물리적 특성을 나타낸다고 할 수 있다.</li> <li>반면, 색상 정보는 바라보는 방향에 따라 빛의 반사나 굴절이 달라 질 수 있기 때문에 view-dependent한 특성을 가지게 된다.</li> </ul> <p><strong>오케이 알겠는데 그럼 왜 이렇게 설계 했을까 ?</strong></p> <ul> <li>이는 view-independent한 정보와 view-dependent한 정보를 분리하여 모델이 더 효율적으로 학습할 수 있도록 설계했기 때문이다. 즉, 밀도와 색상 예측 과정을 하나의 MLP내부에서 분리하여 보다 정교한 3D 표현을 학습할 수 있게 설계 한 것이다.</li> </ul> <p><strong>Lambertian effects</strong></p> <ul> <li>논문에서 나오는 표현으로 람베르트 반사라는 용어이다. 이는 관찰자가 바라보는 각도와 관계없이 같은 겉보기 밝기를 갖는 성질을 의미한다.</li> <li>하지만 <strong>NeRF는 direction값을 input으로 사용하기 때문에 각도에 따라 휘도가 달라지는 non-Labertian effects성질을 갖게 되는 것</strong>이다.</li> </ul> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/987984b5-e931-4cdf-93f9-d0f3f7e6b1ad/image.png" width="900"></p> <hr> <blockquote> <h2 id="volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</h2> </blockquote> <p>5D를 통해 나온 $c$(color)와 $\sigma$(density)를 통해서 <strong>2D Image를 생성하기 위해 Computer Graphics의 고전적인 방법론인 volume rendering을 사용</strong>한다.</p> <p>수식을 간단하게 살펴보자면 결과 값인 $C(\mathbf{r})$은 <strong>하나의 ray(pixel)에서 기대할 수 있는 Color값(expected color)</strong>을 의미하게 된다.</p> <blockquote> <ul> <li> <strong>$t$:</strong> t는 ray의 깊이(depth)를 의미하는 parameter로, <strong>카메라에서 시작된 광선이 3D 공간에서 특정 지점에 도달하기까지의 거리(depth)를 의미</strong>한다. $t_n$은 광선이 시작되는 지점, $t_f$는 광선이 끝나는 지점을 의미한다.</li> <li> <strong>$\sigma(\mathbf{r}(t))$:</strong> 해당 시점에서의 density값으로 볼 수 있으며 값이 커질수록 Weight가 커지게 된다.</li> <li> <strong>$T(t)$:</strong> <strong>Transmittance(빛의 투과도)</strong>를 의미하며 수식적으로 보았을 때 <strong>density값이 커질 수록 작아진다</strong>는 것을 알 수 있다. 이를 해석해보자면 우리가 보려고 하는 물체 앞에 밀도를 가지는 물체가 있을 때 <strong>우리가 보고자 하는 물체가 가려지게 되는 것</strong>을 수식적으로 표현했다고 볼 수 있다. <strong>pixel은 해당 값이 클 수록 투명하고 작을수록 불투명하게 된다.</strong> </li> <li> <strong>$c(\mathbf r(t),d)$:</strong> 해당 ray와 시점에 대한 물체의 색을 나타내는 부분이다.</li> </ul> </blockquote> \[C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) c(\mathbf{r}(t), \mathbf{d}) \, dt, \quad \text{where} \quad T(t) = \exp \left( - \int_{t_n}^t \sigma(\mathbf{r}(s)) \, ds \right).\] <p>정리하면, 한 픽셀의 색상은 광선(ray)의 모든 지점에서 <strong>(전달된 투과도) × (밀도) × (색상)</strong> 을 누적하여 적분한 값과 같다. 이 적분은 광선 상의 작은 간격($dt$)에 대해 수행된다.</p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/e795fdd8-aed6-47cb-94e0-067ec9c8791e/image.png" width="900"></p> <p><strong>continuous(연속적인) 한 적분식을 실제로 프로그래밍에 사용할 수 있게 하기 위해 discrete(이산적인)하게 변환해야 한다.</strong> 그래서 수치적 방법으로 아래와 같이 근사하게 된다. 여기서 사용되는게 <strong>Stratified sampling</strong>으로, 고정된 간격의 샘플링을 하는 것이 아니라 <strong>각 구간에 대해서 무작위 샘플링을 하게 되어 적분의 정확성을 향상</strong> 시키게 되었다고 설명한다. <strong>결론적으로 무작위 샘플링을 통해 적분을 근사하여 연속적인 장면을 표현</strong>할 수 있는 것이다.</p> \[t_i \sim \mathcal{U} \left[ t_n + \frac{i-1}{N}(t_f - t_n), \, t_n + \frac{i}{N}(t_f - t_n) \right]\] <p>그렇게 위에서 샘플링된 $c_i$와 $\sigma_i$값들을 기반으로 $\hat{C}$를 계산하게 된다.</p> <blockquote> <ul> <li>$T_i$: 남아있는 빛의 양 (투과도)</li> <li>$(1-\exp(\sigma_i\delta_i))$: 해당 지점에서 흡수된 빛의 양 (불투명도)</li> <li>$c_i$: 해당 지점의 색상</li> <li>$\hat{C}$: 각 샘플링 지점의 색상 값을 가중합한 결과로 최종 픽셀 색상</li> </ul> </blockquote> \[\hat{C}(\mathbf{r}) = \sum_{i=1}^N T_i \left(1 - \exp(-\sigma_i \delta_i)\right) c_i, \quad \text{where} \quad T_i = \exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)\] <hr> <blockquote> <h2 id="optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</h2> </blockquote> <h3 id="positional-encoding">Positional encoding</h3> <p>이전에 MLP 부분에서 3차원 좌표를 60차원 입력으로 변환할 때 사용되는 Positional Encoding 기법에 대한 설명이다. 다시 한번 복기하자면 더 높은 고차원으로 표현을 하여 고주파 정보 즉, 물체의 윤곽과 텍스쳐에 대한 detail 정보들을 출력할 수 있게 된다.</p> <blockquote> <ul> <li> <strong>저주파 정보</strong>: 부드럽고 점진적인 변화 (배경 색상)</li> <li> <strong>고주파 정보</strong>: 날카로운 경계나 세부적인 구조 (물체의 윤곽, 텍스쳐 등)</li> </ul> </blockquote> \[\gamma(p) = \left( \sin(2^0 \pi p), \cos(2^0 \pi p), \cdots, \sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p) \right).\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/88878216-bfa4-4ca6-bbc9-b5ed9147b5a7/image.png" width="900"></p> <h3 id="hierarchical-volume-sampling">Hierarchical volume sampling</h3> <p>기존의 방식들은 빈 공간(free space) 이나 가려진 영역(occluded regions)과 같이 <strong>렌더링에 기여하지 않는 부분도 반복적으로 샘플링하였기 때문에 매우 비효율적</strong> 이었다.</p> <p>NeRF의 Hierarchical Sampling은 장면의 중요 영역에 샘플링을 집중하여 렌더링 효율과 품질을 높이는 전략이다. Coarse Network와 Fine Network를 동시에 최적화 하게 된다. <strong>Coarse Network에서는 전체적인 이미지</strong>, <strong>Fine Network에서는 중요한 영역</strong>에 대해 집중하게 된다.</p> <blockquote> <ol> <li> <strong>Coarse Sampling:</strong> Stratified Sampling을 사용해 고르게 샘플링.</li> <li> <strong>PDF 생성:</strong> Coarse Network의 출력을 바탕으로 확률 밀도 함수(PDF)를 생성.</li> <li> <strong>Fine Sampling:</strong> PDF를 기반으로 Inverse Transform Samplingdmf 사용하여 중요한 영역에서 추가 샘플링.</li> <li> <strong>최종 렌더링:</strong> Coarse와 Fine 샘플을 결합하여 최종 이미지를 생성.</li> </ol> <p>이 과정은 <strong>샘플링 효율성을 극대화</strong>하고, <strong>빈 공간에 낭비되는 계산을 줄이는 동시에 중요한 영역의 디테일을 더 잘 포착</strong>하도록 설계하였다</p> </blockquote> \[\hat{C}_c(\mathbf{r}) = \sum_{i=1}^{N_c} w_i c_i, \quad w_i = T_i \left(1 - \exp(-\sigma_i \delta_i) \right).\] <h3 id="loss">Loss</h3> <p>그렇게 Coarse Network와 Fine Network를 통해 나온 output을 통해 실제 Ground Truth와 L2 Norm을 이용하여 Loss를 간단하게 구성된다.</p> \[L = \sum_{r \in R} \left[ \| \hat{C}_c(r) - C(r) \|_2^2 + \| \hat{C}_f(r) - C(r) \|_2^2 \right]\] <p><img src="https://velog.velcdn.com/images/lowzxx/post/01a206e4-e484-4041-a57b-89906c50886b/image.png" width="900"></p> <hr> <blockquote> <h2 id="results">Results</h2> </blockquote> <p><img src="https://velog.velcdn.com/images/lowzxx/post/0e4d7181-446e-4833-aeb6-b1a9d7cca4d9/image.png" width="900"></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/1e680de8-5ba0-45b1-906a-0ff78b5530ef/image.png" width="900"></p> <p><img src="https://velog.velcdn.com/images/lowzxx/post/6e158864-22bb-4d00-93fa-5152800da902/image.png" width="900"></p> <hr> <h3 id="reference">Reference.</h3> <p><a href="https://arxiv.org/abs/2003.08934" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2003.08934</a><br> <a href="https://github.com/bmild/nerf" rel="external nofollow noopener" target="_blank">https://github.com/bmild/nerf</a><br> <a href="https://csm-kr.tistory.com/64" rel="external nofollow noopener" target="_blank">https://csm-kr.tistory.com/64</a> <br> <a href="https://jaehoon-daddy.tistory.com/26" rel="external nofollow noopener" target="_blank">https://jaehoon-daddy.tistory.com/26</a><br> <a href="https://www.youtube.com/watch?v=Mk0y1L8TvKE" rel="external nofollow noopener" target="_blank">https://www.youtube.com/watch?v=Mk0y1L8TvKE</a> <br> <a href="https://an067.pages.mi.hdm-stuttgart.de/or-jupyterbook/05_NeRF_improvements/05_NeRF_improvements" rel="external nofollow noopener" target="_blank">https://an067.pages.mi.hdm-stuttgart.de/or-jupyterbook/05_NeRF_improvements/05_NeRF_improvements</a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/Research_decision/">[Research] CVPR 2026 Final Decision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/Research_rebuttal/">[Research] CVPR 2026 Rebuttal</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/papers_img2img_turbo/">[Papers] One-Step Image Translation with Text-to-Image Models (Preprint)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/papers_GAGAvatars/">[Papers] Generalizable and Animatable Gaussian Head Avatar (NeurIPS 2024)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/papers_FaceLift/">[Papers] FaceLift: Learning Generalizable Single Image 3D Face Reconstruction from Synthetic Heads (ICCV 2025)</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 owei . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>